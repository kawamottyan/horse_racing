{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ca9841-adc6-4612-b82f-f2f94a32faa7",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d127aa-802a-4ec0-9fc9-e2671c2dba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#モデル\n",
    "import lightgbm as lgb\n",
    "\n",
    "#パラメータ探索\n",
    "import optuna\n",
    "\n",
    "#クロスバリデーション\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "#エヴァリュエーション\n",
    "from sklearn.metrics import precision_score, recall_score, mean_squared_error\n",
    "\n",
    "#可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#保存\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11351b30-8083-423f-a50c-46bfde76436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを読み込む\n",
    "n_uma_race_df = pd.read_pickle('../datasets/traindata/n_uma_race.pkl')\n",
    "n_race_df = pd.read_pickle('../datasets/traindata/n_race.pkl')\n",
    "n_payout_df = pd.read_pickle('../datasets/traindata/n_payout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c664d196-e826-4eba-9cf8-851ea81544cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新しいグループを作成\n",
    "n_uma_race_df['group'] = n_uma_race_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_uma_race_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_uma_race_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_uma_race_df['race_bango'].astype(int).astype(str)\n",
    "n_race_df['group'] = n_race_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_race_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_race_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_race_df['race_bango'].astype(int).astype(str)\n",
    "n_payout_df['group'] = n_payout_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_payout_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_payout_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_payout_df['race_bango'].astype(int).astype(str)\n",
    "\n",
    "n_race_df = n_race_df.drop(['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'kaisai_kai', 'kaisai_nichime', 'race_bango'],axis=1)\n",
    "n_payout_df = n_payout_df.drop(['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'kaisai_kai', 'kaisai_nichime', 'race_bango', 'toroku_tosu', 'shusso_tosu'],axis=1)\n",
    "\n",
    "merged_df = pd.merge(n_uma_race_df, n_race_df, on='group', how='left')\n",
    "merged_df = pd.merge(merged_df, n_payout_df, on='group', how='left')\n",
    "merged_df['group'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68c59eb-a4db-40af-b9cf-c5aba98aa72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-916-46-1', '2023-916-46-2', '2023-916-46-3', ...,\n",
       "       '2023-907-50-10', '2023-907-50-11', '2023-907-50-12'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ff2e39-36ac-42c3-b039-0a870a1dc2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kaisai_nen</th>\n",
       "      <th>kaisai_tsukihi</th>\n",
       "      <th>keibajo_code</th>\n",
       "      <th>kaisai_kai</th>\n",
       "      <th>kaisai_nichime</th>\n",
       "      <th>race_bango</th>\n",
       "      <th>wakuban</th>\n",
       "      <th>umaban</th>\n",
       "      <th>ketto_toroku_bango</th>\n",
       "      <th>bamei</th>\n",
       "      <th>umakigo_code</th>\n",
       "      <th>seibetsu_code</th>\n",
       "      <th>hinshu_code</th>\n",
       "      <th>moshoku_code</th>\n",
       "      <th>barei</th>\n",
       "      <th>tozai_shozoku_code</th>\n",
       "      <th>chokyoshi_code</th>\n",
       "      <th>banushi_code</th>\n",
       "      <th>banushimei</th>\n",
       "      <th>futan_juryo</th>\n",
       "      <th>blinker_shiyo_kubun</th>\n",
       "      <th>kishu_code</th>\n",
       "      <th>kishumei_ryakusho</th>\n",
       "      <th>kishu_minarai_code</th>\n",
       "      <th>bataiju</th>\n",
       "      <th>zogen_fugo</th>\n",
       "      <th>zogen_sa</th>\n",
       "      <th>ijo_kubun_code</th>\n",
       "      <th>nyusen_juni</th>\n",
       "      <th>kakutei_chakujun</th>\n",
       "      <th>dochaku_kubun</th>\n",
       "      <th>dochaku_tosu</th>\n",
       "      <th>soha_time</th>\n",
       "      <th>chakusa_code_1</th>\n",
       "      <th>chakusa_code_2</th>\n",
       "      <th>chakusa_code_3</th>\n",
       "      <th>corner_1</th>\n",
       "      <th>corner_2</th>\n",
       "      <th>corner_3</th>\n",
       "      <th>corner_4</th>\n",
       "      <th>tansho_odds</th>\n",
       "      <th>tansho_ninkijun</th>\n",
       "      <th>kakutoku_honshokin</th>\n",
       "      <th>kakutoku_fukashokin</th>\n",
       "      <th>kohan_4f</th>\n",
       "      <th>kohan_3f</th>\n",
       "      <th>aiteuma_joho_1</th>\n",
       "      <th>aiteuma_joho_2</th>\n",
       "      <th>aiteuma_joho_3</th>\n",
       "      <th>time_sa</th>\n",
       "      <th>record_koshin_kubun</th>\n",
       "      <th>kyakushitsu_hantei</th>\n",
       "      <th>group</th>\n",
       "      <th>yobi_code</th>\n",
       "      <th>jusho_kaiji</th>\n",
       "      <th>grade_code</th>\n",
       "      <th>kyoso_shubetsu_code</th>\n",
       "      <th>kyoso_kigo_code</th>\n",
       "      <th>juryo_shubetsu_code</th>\n",
       "      <th>kyoso_joken_code_2sai</th>\n",
       "      <th>kyoso_joken_code_3sai</th>\n",
       "      <th>kyoso_joken_code_4sai</th>\n",
       "      <th>kyoso_joken_code_5sai_ijo</th>\n",
       "      <th>kyoso_joken_code</th>\n",
       "      <th>kyori</th>\n",
       "      <th>track_code</th>\n",
       "      <th>course_kubun</th>\n",
       "      <th>honshokin</th>\n",
       "      <th>fukashokin</th>\n",
       "      <th>hasso_jikoku</th>\n",
       "      <th>toroku_tosu</th>\n",
       "      <th>shusso_tosu</th>\n",
       "      <th>nyusen_tosu</th>\n",
       "      <th>tenko_code</th>\n",
       "      <th>babajotai_code_shiba</th>\n",
       "      <th>babajotai_code_dirt</th>\n",
       "      <th>fuseiritsu_flag_sanrenpuku</th>\n",
       "      <th>tokubarai_flag_sanrenpuku</th>\n",
       "      <th>henkan_flag_sanrenpuku</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1a</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1b</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>916</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020106227</td>\n",
       "      <td>スターサファイア</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5319</td>\n",
       "      <td>875800</td>\n",
       "      <td>ＪＰＮ技研</td>\n",
       "      <td>560.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5300</td>\n",
       "      <td>葛山晃平</td>\n",
       "      <td>0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1359</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>899</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>2020105068フェイマスグローリ</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-916-46-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1400</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000e+51</td>\n",
       "      <td>0</td>\n",
       "      <td>1140</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60809.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kaisai_nen  kaisai_tsukihi keibajo_code  kaisai_kai  kaisai_nichime  \\\n",
       "0        2023             916           46          13               1   \n",
       "\n",
       "   race_bango  wakuban  umaban  ketto_toroku_bango               bamei  \\\n",
       "0           1        1       1          2020106227  スターサファイア　　　　　　　　　　   \n",
       "\n",
       "   umakigo_code  seibetsu_code  hinshu_code  moshoku_code  barei  \\\n",
       "0             0              1          1.0             3      3   \n",
       "\n",
       "   tozai_shozoku_code  chokyoshi_code  banushi_code  \\\n",
       "0                   3            5319        875800   \n",
       "\n",
       "                         banushimei  futan_juryo  blinker_shiyo_kubun  \\\n",
       "0  ＪＰＮ技研　　　　　　　　　　　　　　　　　　　　　　　　　　　        560.0                    0   \n",
       "\n",
       "   kishu_code kishumei_ryakusho  kishu_minarai_code  bataiju  zogen_fugo  \\\n",
       "0        5300              葛山晃平                   0    466.0           1   \n",
       "\n",
       "   zogen_sa  ijo_kubun_code  nyusen_juni  kakutei_chakujun  dochaku_kubun  \\\n",
       "0       0.0               0            8                 8              0   \n",
       "\n",
       "   dochaku_tosu  soha_time  chakusa_code_1  chakusa_code_2  chakusa_code_3  \\\n",
       "0             0       1359             4.0             NaN             NaN   \n",
       "\n",
       "   corner_1  corner_2  corner_3  corner_4  tansho_odds  tansho_ninkijun  \\\n",
       "0         9         9         9         9          899                8   \n",
       "\n",
       "   kakutoku_honshokin  kakutoku_fukashokin  kohan_4f  kohan_3f  \\\n",
       "0                   0                    0         0       411   \n",
       "\n",
       "                 aiteuma_joho_1                aiteuma_joho_2  \\\n",
       "0  2020105068フェイマスグローリ　　　　　　　　　  0000000000　　　　　　　　　　　　　　　　　　   \n",
       "\n",
       "                 aiteuma_joho_3  time_sa  record_koshin_kubun  \\\n",
       "0  0000000000　　　　　　　　　　　　　　　　　　       50                    0   \n",
       "\n",
       "   kyakushitsu_hantei          group  yobi_code  jusho_kaiji  grade_code  \\\n",
       "0                   0  2023-916-46-1          1            0           0   \n",
       "\n",
       "   kyoso_shubetsu_code  kyoso_kigo_code  juryo_shubetsu_code  \\\n",
       "0                   49                0                    4   \n",
       "\n",
       "   kyoso_joken_code_2sai  kyoso_joken_code_3sai  kyoso_joken_code_4sai  \\\n",
       "0                      0                      0                      0   \n",
       "\n",
       "   kyoso_joken_code_5sai_ijo  kyoso_joken_code  kyori  track_code  \\\n",
       "0                          0                 0   1400          24   \n",
       "\n",
       "   course_kubun     honshokin  fukashokin  hasso_jikoku  toroku_tosu  \\\n",
       "0             0  4.000000e+51           0          1140            9   \n",
       "\n",
       "   shusso_tosu  nyusen_tosu  tenko_code  babajotai_code_shiba  \\\n",
       "0            9            9           1                     0   \n",
       "\n",
       "   babajotai_code_dirt  fuseiritsu_flag_sanrenpuku  tokubarai_flag_sanrenpuku  \\\n",
       "0                  1.0                         0.0                        0.0   \n",
       "\n",
       "   henkan_flag_sanrenpuku  haraimodoshi_sanrenpuku_1a  \\\n",
       "0                     0.0                     60809.0   \n",
       "\n",
       "   haraimodoshi_sanrenpuku_1b  haraimodoshi_sanrenpuku_1c  \n",
       "0                       210.0                         1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1919f-72fe-444c-8021-dac0032d6065",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43e44e6-e588-4837-9eab-2c1542f91680",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['hutan_wariai'] = merged_df['futan_juryo'] / merged_df['bataiju']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3207c5cf-150b-40c0-9b0f-278f94680871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sign_and_diff(row):\n",
    "    if row['zogen_fugo'] == 2:\n",
    "        return row['zogen_sa']\n",
    "    elif row['zogen_fugo'] == 0:\n",
    "        return -row['zogen_sa']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "merged_df['zogen_ryou'] = merged_df.apply(combine_sign_and_diff, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9622447d-bc27-4919-8ec1-39e9a3d2bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['ijo_kubun_code'] == 0]\n",
    "# 1\t出走取消\t取消\tSCRATCHED\tS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2b6db6-3968-4a63-ab23-5b7a73ecc35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyori のデータ型: int32\n",
      "grade_code のデータ型: int32\n",
      "seibetsu_code のデータ型: int32\n",
      "moshoku_code のデータ型: int32\n",
      "barei のデータ型: int32\n",
      "chokyoshi_code のデータ型: int32\n",
      "banushi_code のデータ型: int32\n",
      "kishu_code のデータ型: int32\n",
      "kishu_minarai_code のデータ型: int32\n",
      "kyoso_shubetsu_code のデータ型: int32\n",
      "juryo_shubetsu_code のデータ型: int32\n",
      "shusso_tosu のデータ型: int32\n",
      "tenko_code のデータ型: int32\n",
      "babajotai_code_dirt のデータ型: int32\n",
      "hutan_wariai のデータ型: float64\n",
      "zogen_ryou のデータ型: int32\n",
      "track_code のデータ型: int32\n",
      "keibajo_code のデータ型: int32\n"
     ]
    }
   ],
   "source": [
    "columns_to_convert = [\n",
    "                    'kyori',\n",
    "                    'grade_code',\n",
    "                    'seibetsu_code',\n",
    "                    'moshoku_code',\n",
    "                    'barei',\n",
    "                    'chokyoshi_code',\n",
    "                    'banushi_code',\n",
    "                    'kishu_code',\n",
    "                    'kishu_minarai_code',\n",
    "                    'kyoso_shubetsu_code',\n",
    "                    'juryo_shubetsu_code',\n",
    "                    'shusso_tosu',\n",
    "                    'tenko_code',\n",
    "                    'babajotai_code_dirt',\n",
    "                    'hutan_wariai',\n",
    "                    'zogen_ryou',\n",
    "                    'track_code',\n",
    "                    'keibajo_code'\n",
    "                    ]\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    merged_df[column].fillna(0, inplace=True)\n",
    "    try:\n",
    "        if merged_df[column].astype(float).apply(lambda x: x.is_integer()).all():\n",
    "            merged_df[column] = merged_df[column].astype(int)\n",
    "        else:\n",
    "            merged_df[column] = merged_df[column].astype(float)\n",
    "    except ValueError:\n",
    "        merged_df[column] = merged_df[column].astype(float)\n",
    "\n",
    "    print(f\"{column} のデータ型: {merged_df[column].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a07b2c5-f805-499d-849d-d7cd88bf1f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 2017402 entries, 0 to 2042540\n",
      "Series name: keibajo_code\n",
      "Non-Null Count    Dtype\n",
      "--------------    -----\n",
      "2017402 non-null  int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 23.1 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df[column].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bf1c2-3a54-4a11-9309-d830239210d8",
   "metadata": {},
   "source": [
    "# lightgbm.LGBMRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "165fa6a7-87f9-4cf1-8cdc-3421e1c4a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010年から2019年のデータを学習データとして取得\n",
    "train_data = merged_df[merged_df['kaisai_nen'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9740ae80-b106-4159-a01e-765f87246544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ランキング学習のために必要な特徴量とターゲットを設定\n",
    "features = [\n",
    "            # 'kyori',\n",
    "            # 'grade_code',\n",
    "            'seibetsu_code',\n",
    "            'moshoku_code',\n",
    "            'barei',\n",
    "            'chokyoshi_code',\n",
    "            'banushi_code',\n",
    "            'kishu_code',\n",
    "            'kishu_minarai_code',\n",
    "            'kyoso_shubetsu_code',\n",
    "            'juryo_shubetsu_code',\n",
    "            # 'shusso_tosu',\n",
    "            # 'tenko_code',\n",
    "            # 'babajotai_code_dirt',\n",
    "            'hutan_wariai',\n",
    "            'zogen_ryou',\n",
    "            # 'track_code',\n",
    "            # 'keibajo_code'\n",
    "            ]\n",
    "\n",
    "target = 'kakutei_chakujun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "228abcf6-d782-4a41-8fa3-41594a5b3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_score(y_true, y_score, k=5):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    \n",
    "    gain = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "def mean_ndcg_score(y_true, y_score, groups, k=5):\n",
    "    ndcgs = []\n",
    "    idx_start = 0\n",
    "    for group in groups:\n",
    "        ndcgs.append(ndcg_score(y_true[idx_start:idx_start+group], y_score[idx_start:idx_start+group], k))\n",
    "        idx_start += group\n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4147b41-467a-4946-b929-6eb017229b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def lambda_rank_gradient(y_true, y_pred):\n",
    "    pred_diff = y_pred[:, None] - y_pred[None, :]\n",
    "    true_diff = y_true[:, None] - y_true[None, :]\n",
    "    \n",
    "    S_ij = np.where(true_diff > 0, 1, np.where(true_diff < 0, -1, 0))\n",
    "    \n",
    "    lambda_ij = 0.5 * (1 - S_ij) - sigmoid(-pred_diff)\n",
    "    \n",
    "    grad = np.sum(lambda_ij, axis=1)\n",
    "    \n",
    "    hess = np.sum(sigmoid(pred_diff) * (1 - sigmoid(pred_diff)), axis=1)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef21f3-910b-4c2c-a759-b3444fdd5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "def custom_objective(y_true, y_pred):\n",
    "    grad, hess = lambda_rank_gradient(y_true, y_pred)\n",
    "    top5_indices = np.argsort(y_pred)[-5:]\n",
    "    if np.sum(y_true[top5_indices] == 1) == 0:\n",
    "        grad += penalty_grad\n",
    "        hess += penalty_hess\n",
    "    return grad, hess\n",
    "\n",
    "# サンプリング\n",
    "high_weight_value = 2.0\n",
    "sample_weights = np.where(train_data[target] == 1, high_weight_value, 1)\n",
    "\n",
    "# 再ランキング\n",
    "def rerank_predictions(y_pred):\n",
    "    top5_indices = np.argsort(y_pred)[-5:]\n",
    "    if np.sum(y_true[top5_indices] == 1) < 3:\n",
    "        one_indices = np.where(y_true == 1)[0]\n",
    "        for idx in one_indices:\n",
    "            if idx not in top5_indices:\n",
    "                top5_indices[-1] = idx\n",
    "                break\n",
    "    return top5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729dcb43-ba2d-426a-80f8-0e956074a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:08:42,625] A new study created in memory with name: no-name-eefe6320-99ad-48ee-9edc-b873ddf6533b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.543698\n",
      "[20]\tvalid_0's ndcg@5: 0.549463\n",
      "[30]\tvalid_0's ndcg@5: 0.551518\n",
      "[40]\tvalid_0's ndcg@5: 0.553714\n",
      "[50]\tvalid_0's ndcg@5: 0.555994\n",
      "[60]\tvalid_0's ndcg@5: 0.556377\n",
      "[70]\tvalid_0's ndcg@5: 0.556792\n",
      "[80]\tvalid_0's ndcg@5: 0.557311\n",
      "[90]\tvalid_0's ndcg@5: 0.557682\n",
      "[100]\tvalid_0's ndcg@5: 0.558691\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.558691\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.544059\n",
      "[20]\tvalid_0's ndcg@5: 0.549243\n",
      "[30]\tvalid_0's ndcg@5: 0.551177\n",
      "[40]\tvalid_0's ndcg@5: 0.553985\n",
      "[50]\tvalid_0's ndcg@5: 0.556147\n",
      "[60]\tvalid_0's ndcg@5: 0.556271\n",
      "[70]\tvalid_0's ndcg@5: 0.557442\n",
      "[80]\tvalid_0's ndcg@5: 0.557725\n",
      "[90]\tvalid_0's ndcg@5: 0.558451\n",
      "[100]\tvalid_0's ndcg@5: 0.559514\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.559702\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.544542\n",
      "[20]\tvalid_0's ndcg@5: 0.550462\n",
      "[30]\tvalid_0's ndcg@5: 0.552262\n",
      "[40]\tvalid_0's ndcg@5: 0.554336\n",
      "[50]\tvalid_0's ndcg@5: 0.557205\n",
      "[60]\tvalid_0's ndcg@5: 0.558077\n",
      "[70]\tvalid_0's ndcg@5: 0.558792\n",
      "[80]\tvalid_0's ndcg@5: 0.559503\n",
      "[90]\tvalid_0's ndcg@5: 0.560374\n",
      "[100]\tvalid_0's ndcg@5: 0.561213\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.561213\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.544584\n",
      "[20]\tvalid_0's ndcg@5: 0.5501\n",
      "[30]\tvalid_0's ndcg@5: 0.552987\n",
      "[40]\tvalid_0's ndcg@5: 0.555068\n",
      "[50]\tvalid_0's ndcg@5: 0.55751\n",
      "[60]\tvalid_0's ndcg@5: 0.557373\n",
      "[70]\tvalid_0's ndcg@5: 0.559158\n",
      "[80]\tvalid_0's ndcg@5: 0.559183\n",
      "[90]\tvalid_0's ndcg@5: 0.55942\n",
      "[100]\tvalid_0's ndcg@5: 0.560576\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.560753\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41296618585397377, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41296618585397377\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00023341931108562857, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00023341931108562857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0026043366713917, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0026043366713917\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.544643\n",
      "[20]\tvalid_0's ndcg@5: 0.549408\n",
      "[30]\tvalid_0's ndcg@5: 0.550921\n",
      "[40]\tvalid_0's ndcg@5: 0.553672\n",
      "[50]\tvalid_0's ndcg@5: 0.555972\n",
      "[60]\tvalid_0's ndcg@5: 0.556326\n",
      "[70]\tvalid_0's ndcg@5: 0.557127\n",
      "[80]\tvalid_0's ndcg@5: 0.557729\n",
      "[90]\tvalid_0's ndcg@5: 0.558378\n",
      "[100]\tvalid_0's ndcg@5: 0.559052\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.559131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:09:38,727] Trial 0 finished with value: 2213.7401692178437 and parameters: {'num_leaves': 211, 'learning_rate': 0.012284809695107224, 'feature_fraction': 0.41296618585397377, 'bagging_freq': 6, 'verbose': 0, 'lambda_l1': 0.00023341931108562857, 'lambda_l2': 0.0026043366713917}. Best is trial 0 with value: 2213.7401692178437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.502868\n",
      "[20]\tvalid_0's ndcg@5: 0.503286\n",
      "[30]\tvalid_0's ndcg@5: 0.504638\n",
      "[40]\tvalid_0's ndcg@5: 0.504876\n",
      "[50]\tvalid_0's ndcg@5: 0.506262\n",
      "[60]\tvalid_0's ndcg@5: 0.507297\n",
      "[70]\tvalid_0's ndcg@5: 0.507004\n",
      "[80]\tvalid_0's ndcg@5: 0.507272\n",
      "[90]\tvalid_0's ndcg@5: 0.507899\n",
      "[100]\tvalid_0's ndcg@5: 0.508513\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.508513\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.505825\n",
      "[20]\tvalid_0's ndcg@5: 0.505824\n",
      "[30]\tvalid_0's ndcg@5: 0.505271\n",
      "[40]\tvalid_0's ndcg@5: 0.505782\n",
      "[50]\tvalid_0's ndcg@5: 0.50651\n",
      "[60]\tvalid_0's ndcg@5: 0.506571\n",
      "[70]\tvalid_0's ndcg@5: 0.506787\n",
      "[80]\tvalid_0's ndcg@5: 0.507107\n",
      "[90]\tvalid_0's ndcg@5: 0.507213\n",
      "[100]\tvalid_0's ndcg@5: 0.507351\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.507351\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.505591\n",
      "[20]\tvalid_0's ndcg@5: 0.505984\n",
      "[30]\tvalid_0's ndcg@5: 0.506821\n",
      "[40]\tvalid_0's ndcg@5: 0.507417\n",
      "[50]\tvalid_0's ndcg@5: 0.507343\n",
      "[60]\tvalid_0's ndcg@5: 0.507964\n",
      "[70]\tvalid_0's ndcg@5: 0.508132\n",
      "[80]\tvalid_0's ndcg@5: 0.508527\n",
      "[90]\tvalid_0's ndcg@5: 0.508245\n",
      "[100]\tvalid_0's ndcg@5: 0.508615\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.508667\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.505073\n",
      "[20]\tvalid_0's ndcg@5: 0.505408\n",
      "[30]\tvalid_0's ndcg@5: 0.505847\n",
      "[40]\tvalid_0's ndcg@5: 0.507032\n",
      "[50]\tvalid_0's ndcg@5: 0.507376\n",
      "[60]\tvalid_0's ndcg@5: 0.507791\n",
      "[70]\tvalid_0's ndcg@5: 0.508084\n",
      "[80]\tvalid_0's ndcg@5: 0.508254\n",
      "[90]\tvalid_0's ndcg@5: 0.508532\n",
      "[100]\tvalid_0's ndcg@5: 0.50862\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.508728\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9566698326045779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9566698326045779\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.009585192190699813, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009585192190699813\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.468624582539678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.468624582539678e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.505174\n",
      "[20]\tvalid_0's ndcg@5: 0.505121\n",
      "[30]\tvalid_0's ndcg@5: 0.504844\n",
      "[40]\tvalid_0's ndcg@5: 0.505426\n",
      "[50]\tvalid_0's ndcg@5: 0.505444\n",
      "[60]\tvalid_0's ndcg@5: 0.506597\n",
      "[70]\tvalid_0's ndcg@5: 0.507619\n",
      "[80]\tvalid_0's ndcg@5: 0.507663\n",
      "[90]\tvalid_0's ndcg@5: 0.508124\n",
      "[100]\tvalid_0's ndcg@5: 0.508261\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.508374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:10:18,780] Trial 1 finished with value: 2006.6096223019454 and parameters: {'num_leaves': 40, 'learning_rate': 0.000286058156574033, 'feature_fraction': 0.9566698326045779, 'bagging_freq': 4, 'verbose': 0, 'lambda_l1': 0.009585192190699813, 'lambda_l2': 1.468624582539678e-05}. Best is trial 0 with value: 2213.7401692178437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.501415\n",
      "[20]\tvalid_0's ndcg@5: 0.503464\n",
      "[30]\tvalid_0's ndcg@5: 0.505456\n",
      "[40]\tvalid_0's ndcg@5: 0.507044\n",
      "[50]\tvalid_0's ndcg@5: 0.507601\n",
      "[60]\tvalid_0's ndcg@5: 0.508234\n",
      "[70]\tvalid_0's ndcg@5: 0.50891\n",
      "[80]\tvalid_0's ndcg@5: 0.509689\n",
      "[90]\tvalid_0's ndcg@5: 0.510105\n",
      "[100]\tvalid_0's ndcg@5: 0.510146\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\tvalid_0's ndcg@5: 0.510229\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.497905\n",
      "[20]\tvalid_0's ndcg@5: 0.499862\n",
      "[30]\tvalid_0's ndcg@5: 0.501979\n",
      "[40]\tvalid_0's ndcg@5: 0.502808\n",
      "[50]\tvalid_0's ndcg@5: 0.504093\n",
      "[60]\tvalid_0's ndcg@5: 0.50492\n",
      "[70]\tvalid_0's ndcg@5: 0.505297\n",
      "[80]\tvalid_0's ndcg@5: 0.505506\n",
      "[90]\tvalid_0's ndcg@5: 0.506105\n",
      "[100]\tvalid_0's ndcg@5: 0.506672\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.506674\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.501209\n",
      "[20]\tvalid_0's ndcg@5: 0.503049\n",
      "[30]\tvalid_0's ndcg@5: 0.503898\n",
      "[40]\tvalid_0's ndcg@5: 0.504418\n",
      "[50]\tvalid_0's ndcg@5: 0.505363\n",
      "[60]\tvalid_0's ndcg@5: 0.50607\n",
      "[70]\tvalid_0's ndcg@5: 0.506446\n",
      "[80]\tvalid_0's ndcg@5: 0.506672\n",
      "[90]\tvalid_0's ndcg@5: 0.507409\n",
      "[100]\tvalid_0's ndcg@5: 0.507485\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\tvalid_0's ndcg@5: 0.507667\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.500372\n",
      "[20]\tvalid_0's ndcg@5: 0.503339\n",
      "[30]\tvalid_0's ndcg@5: 0.504063\n",
      "[40]\tvalid_0's ndcg@5: 0.505255\n",
      "[50]\tvalid_0's ndcg@5: 0.506894\n",
      "[60]\tvalid_0's ndcg@5: 0.507257\n",
      "[70]\tvalid_0's ndcg@5: 0.507651\n",
      "[80]\tvalid_0's ndcg@5: 0.507816\n",
      "[90]\tvalid_0's ndcg@5: 0.507813\n",
      "[100]\tvalid_0's ndcg@5: 0.50825\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.508349\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7903781494172669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7903781494172669\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16581262608009306, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16581262608009306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12949601861892696, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12949601861892696\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.499116\n",
      "[20]\tvalid_0's ndcg@5: 0.500773\n",
      "[30]\tvalid_0's ndcg@5: 0.502441\n",
      "[40]\tvalid_0's ndcg@5: 0.504186\n",
      "[50]\tvalid_0's ndcg@5: 0.505774\n",
      "[60]\tvalid_0's ndcg@5: 0.506094\n",
      "[70]\tvalid_0's ndcg@5: 0.506501\n",
      "[80]\tvalid_0's ndcg@5: 0.506883\n",
      "[90]\tvalid_0's ndcg@5: 0.507387\n",
      "[100]\tvalid_0's ndcg@5: 0.50768\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.50768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:10:49,019] Trial 2 finished with value: 1982.6326783550926 and parameters: {'num_leaves': 8, 'learning_rate': 0.007545797065035713, 'feature_fraction': 0.7903781494172669, 'bagging_freq': 4, 'verbose': 1, 'lambda_l1': 0.16581262608009306, 'lambda_l2': 0.12949601861892696}. Best is trial 0 with value: 2213.7401692178437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.531513\n",
      "[20]\tvalid_0's ndcg@5: 0.538001\n",
      "[30]\tvalid_0's ndcg@5: 0.539709\n",
      "[40]\tvalid_0's ndcg@5: 0.540843\n",
      "[50]\tvalid_0's ndcg@5: 0.541741\n",
      "[60]\tvalid_0's ndcg@5: 0.541885\n",
      "[70]\tvalid_0's ndcg@5: 0.542012\n",
      "[80]\tvalid_0's ndcg@5: 0.541797\n",
      "[90]\tvalid_0's ndcg@5: 0.542907\n",
      "[100]\tvalid_0's ndcg@5: 0.543178\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.543286\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.53171\n",
      "[20]\tvalid_0's ndcg@5: 0.536129\n",
      "[30]\tvalid_0's ndcg@5: 0.538526\n",
      "[40]\tvalid_0's ndcg@5: 0.539375\n",
      "[50]\tvalid_0's ndcg@5: 0.540198\n",
      "[60]\tvalid_0's ndcg@5: 0.54093\n",
      "[70]\tvalid_0's ndcg@5: 0.542068\n",
      "[80]\tvalid_0's ndcg@5: 0.541884\n",
      "[90]\tvalid_0's ndcg@5: 0.542533\n",
      "[100]\tvalid_0's ndcg@5: 0.543071\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.543212\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.531502\n",
      "[20]\tvalid_0's ndcg@5: 0.538383\n",
      "[30]\tvalid_0's ndcg@5: 0.539929\n",
      "[40]\tvalid_0's ndcg@5: 0.54127\n",
      "[50]\tvalid_0's ndcg@5: 0.542203\n",
      "[60]\tvalid_0's ndcg@5: 0.542702\n",
      "[70]\tvalid_0's ndcg@5: 0.542875\n",
      "[80]\tvalid_0's ndcg@5: 0.543034\n",
      "[90]\tvalid_0's ndcg@5: 0.544105\n",
      "[100]\tvalid_0's ndcg@5: 0.544776\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.544776\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.535018\n",
      "[20]\tvalid_0's ndcg@5: 0.539621\n",
      "[30]\tvalid_0's ndcg@5: 0.541202\n",
      "[40]\tvalid_0's ndcg@5: 0.54302\n",
      "[50]\tvalid_0's ndcg@5: 0.544057\n",
      "[60]\tvalid_0's ndcg@5: 0.544316\n",
      "[70]\tvalid_0's ndcg@5: 0.544832\n",
      "[80]\tvalid_0's ndcg@5: 0.545128\n",
      "[90]\tvalid_0's ndcg@5: 0.54533\n",
      "[100]\tvalid_0's ndcg@5: 0.545949\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.546128\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5761463481824126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5761463481824126\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15786598398001384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15786598398001384\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00011457966013982632, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011457966013982632\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.53185\n",
      "[20]\tvalid_0's ndcg@5: 0.536643\n",
      "[30]\tvalid_0's ndcg@5: 0.539252\n",
      "[40]\tvalid_0's ndcg@5: 0.54014\n",
      "[50]\tvalid_0's ndcg@5: 0.541626\n",
      "[60]\tvalid_0's ndcg@5: 0.54241\n",
      "[70]\tvalid_0's ndcg@5: 0.54228\n",
      "[80]\tvalid_0's ndcg@5: 0.542541\n",
      "[90]\tvalid_0's ndcg@5: 0.543151\n",
      "[100]\tvalid_0's ndcg@5: 0.543711\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.543973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:11:56,476] Trial 3 finished with value: 2140.614564582825 and parameters: {'num_leaves': 119, 'learning_rate': 0.002189054111479007, 'feature_fraction': 0.5761463481824126, 'bagging_freq': 2, 'verbose': 1, 'lambda_l1': 0.15786598398001384, 'lambda_l2': 0.00011457966013982632}. Best is trial 0 with value: 2213.7401692178437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.526448\n",
      "[20]\tvalid_0's ndcg@5: 0.528904\n",
      "[30]\tvalid_0's ndcg@5: 0.530032\n",
      "[40]\tvalid_0's ndcg@5: 0.532071\n",
      "[50]\tvalid_0's ndcg@5: 0.533811\n",
      "[60]\tvalid_0's ndcg@5: 0.534337\n",
      "[70]\tvalid_0's ndcg@5: 0.534909\n",
      "[80]\tvalid_0's ndcg@5: 0.534981\n",
      "[90]\tvalid_0's ndcg@5: 0.535255\n",
      "[100]\tvalid_0's ndcg@5: 0.535674\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.535892\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.528446\n",
      "[20]\tvalid_0's ndcg@5: 0.53065\n",
      "[30]\tvalid_0's ndcg@5: 0.530751\n",
      "[40]\tvalid_0's ndcg@5: 0.532293\n",
      "[50]\tvalid_0's ndcg@5: 0.533863\n",
      "[60]\tvalid_0's ndcg@5: 0.53408\n",
      "[70]\tvalid_0's ndcg@5: 0.534738\n",
      "[80]\tvalid_0's ndcg@5: 0.53467\n",
      "[90]\tvalid_0's ndcg@5: 0.535514\n",
      "[100]\tvalid_0's ndcg@5: 0.535822\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.53605\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.527029\n",
      "[20]\tvalid_0's ndcg@5: 0.529702\n",
      "[30]\tvalid_0's ndcg@5: 0.531434\n",
      "[40]\tvalid_0's ndcg@5: 0.532903\n",
      "[50]\tvalid_0's ndcg@5: 0.533399\n",
      "[60]\tvalid_0's ndcg@5: 0.534506\n",
      "[70]\tvalid_0's ndcg@5: 0.535189\n",
      "[80]\tvalid_0's ndcg@5: 0.535144\n",
      "[90]\tvalid_0's ndcg@5: 0.535944\n",
      "[100]\tvalid_0's ndcg@5: 0.536219\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.536219\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.529887\n",
      "[20]\tvalid_0's ndcg@5: 0.532306\n",
      "[30]\tvalid_0's ndcg@5: 0.533665\n",
      "[40]\tvalid_0's ndcg@5: 0.534417\n",
      "[50]\tvalid_0's ndcg@5: 0.536681\n",
      "[60]\tvalid_0's ndcg@5: 0.536819\n",
      "[70]\tvalid_0's ndcg@5: 0.537155\n",
      "[80]\tvalid_0's ndcg@5: 0.537437\n",
      "[90]\tvalid_0's ndcg@5: 0.537992\n",
      "[100]\tvalid_0's ndcg@5: 0.538356\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\tvalid_0's ndcg@5: 0.538415\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6055329168377314, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6055329168377314\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0004346928510730371, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004346928510730371\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6551831033006971, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6551831033006971\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.526592\n",
      "[20]\tvalid_0's ndcg@5: 0.529175\n",
      "[30]\tvalid_0's ndcg@5: 0.530712\n",
      "[40]\tvalid_0's ndcg@5: 0.531982\n",
      "[50]\tvalid_0's ndcg@5: 0.533756\n",
      "[60]\tvalid_0's ndcg@5: 0.534366\n",
      "[70]\tvalid_0's ndcg@5: 0.53494\n",
      "[80]\tvalid_0's ndcg@5: 0.534882\n",
      "[90]\tvalid_0's ndcg@5: 0.535562\n",
      "[100]\tvalid_0's ndcg@5: 0.536091\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's ndcg@5: 0.536215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:12:44,857] Trial 4 finished with value: 2104.137292587807 and parameters: {'num_leaves': 113, 'learning_rate': 0.0005220803322449845, 'feature_fraction': 0.6055329168377314, 'bagging_freq': 2, 'verbose': 0, 'lambda_l1': 0.0004346928510730371, 'lambda_l2': 0.6551831033006971}. Best is trial 0 with value: 2213.7401692178437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.531403\n",
      "[20]\tvalid_0's ndcg@5: 0.536068\n",
      "[30]\tvalid_0's ndcg@5: 0.537715\n",
      "[40]\tvalid_0's ndcg@5: 0.539638\n",
      "[50]\tvalid_0's ndcg@5: 0.541136\n",
      "[60]\tvalid_0's ndcg@5: 0.541742\n",
      "[70]\tvalid_0's ndcg@5: 0.542654\n",
      "[80]\tvalid_0's ndcg@5: 0.542418\n",
      "[90]\tvalid_0's ndcg@5: 0.542837\n",
      "[100]\tvalid_0's ndcg@5: 0.543389\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.543453\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.533635\n",
      "[20]\tvalid_0's ndcg@5: 0.536149\n",
      "[30]\tvalid_0's ndcg@5: 0.537011\n",
      "[40]\tvalid_0's ndcg@5: 0.538934\n",
      "[50]\tvalid_0's ndcg@5: 0.540245\n",
      "[60]\tvalid_0's ndcg@5: 0.540396\n",
      "[70]\tvalid_0's ndcg@5: 0.540922\n",
      "[80]\tvalid_0's ndcg@5: 0.541435\n",
      "[90]\tvalid_0's ndcg@5: 0.541921\n",
      "[100]\tvalid_0's ndcg@5: 0.542664\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.542803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.534536\n",
      "[20]\tvalid_0's ndcg@5: 0.537344\n",
      "[30]\tvalid_0's ndcg@5: 0.53908\n",
      "[40]\tvalid_0's ndcg@5: 0.541147\n",
      "[50]\tvalid_0's ndcg@5: 0.542057\n",
      "[60]\tvalid_0's ndcg@5: 0.54288\n",
      "[70]\tvalid_0's ndcg@5: 0.543452\n",
      "[80]\tvalid_0's ndcg@5: 0.543874\n",
      "[90]\tvalid_0's ndcg@5: 0.544791\n",
      "[100]\tvalid_0's ndcg@5: 0.545429\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.545493\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.535921\n",
      "[20]\tvalid_0's ndcg@5: 0.540507\n",
      "[30]\tvalid_0's ndcg@5: 0.541258\n",
      "[40]\tvalid_0's ndcg@5: 0.542956\n",
      "[50]\tvalid_0's ndcg@5: 0.543869\n",
      "[60]\tvalid_0's ndcg@5: 0.544144\n",
      "[70]\tvalid_0's ndcg@5: 0.544728\n",
      "[80]\tvalid_0's ndcg@5: 0.54491\n",
      "[90]\tvalid_0's ndcg@5: 0.545615\n",
      "[100]\tvalid_0's ndcg@5: 0.545783\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\tvalid_0's ndcg@5: 0.545914\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7107964754000895, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7107964754000895\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.217500756435614, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.217500756435614\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5640659135155717e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5640659135155717e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.533551\n",
      "[20]\tvalid_0's ndcg@5: 0.536947\n",
      "[30]\tvalid_0's ndcg@5: 0.538923\n",
      "[40]\tvalid_0's ndcg@5: 0.540768\n",
      "[50]\tvalid_0's ndcg@5: 0.54162\n",
      "[60]\tvalid_0's ndcg@5: 0.542395\n",
      "[70]\tvalid_0's ndcg@5: 0.543162\n",
      "[80]\tvalid_0's ndcg@5: 0.543291\n",
      "[90]\tvalid_0's ndcg@5: 0.543697\n",
      "[100]\tvalid_0's ndcg@5: 0.544686\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.544686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:13:35,947] Trial 5 finished with value: 2143.650564322151 and parameters: {'num_leaves': 182, 'learning_rate': 0.002492410641651964, 'feature_fraction': 0.7107964754000895, 'bagging_freq': 4, 'verbose': 1, 'lambda_l1': 4.217500756435614, 'lambda_l2': 1.5640659135155717e-05}. Best is trial 0 with value: 2213.7401692178437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.517237\n",
      "[20]\tvalid_0's ndcg@5: 0.518179\n",
      "[30]\tvalid_0's ndcg@5: 0.518252\n",
      "[40]\tvalid_0's ndcg@5: 0.519101\n",
      "[50]\tvalid_0's ndcg@5: 0.519944\n",
      "[60]\tvalid_0's ndcg@5: 0.520039\n",
      "[70]\tvalid_0's ndcg@5: 0.520339\n",
      "[80]\tvalid_0's ndcg@5: 0.520961\n",
      "[90]\tvalid_0's ndcg@5: 0.52139\n",
      "[100]\tvalid_0's ndcg@5: 0.521325\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\tvalid_0's ndcg@5: 0.521735\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518328\n",
      "[20]\tvalid_0's ndcg@5: 0.519323\n",
      "[30]\tvalid_0's ndcg@5: 0.518825\n",
      "[40]\tvalid_0's ndcg@5: 0.519312\n",
      "[50]\tvalid_0's ndcg@5: 0.519774\n",
      "[60]\tvalid_0's ndcg@5: 0.520202\n",
      "[70]\tvalid_0's ndcg@5: 0.520833\n",
      "[80]\tvalid_0's ndcg@5: 0.520747\n",
      "[90]\tvalid_0's ndcg@5: 0.521704\n",
      "[100]\tvalid_0's ndcg@5: 0.521446\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's ndcg@5: 0.521704\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518541\n",
      "[20]\tvalid_0's ndcg@5: 0.519308\n",
      "[30]\tvalid_0's ndcg@5: 0.519649\n",
      "[40]\tvalid_0's ndcg@5: 0.520707\n",
      "[50]\tvalid_0's ndcg@5: 0.521216\n",
      "[60]\tvalid_0's ndcg@5: 0.521028\n",
      "[70]\tvalid_0's ndcg@5: 0.521669\n",
      "[80]\tvalid_0's ndcg@5: 0.521314\n",
      "[90]\tvalid_0's ndcg@5: 0.521781\n",
      "[100]\tvalid_0's ndcg@5: 0.521977\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.521977\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.521812\n",
      "[20]\tvalid_0's ndcg@5: 0.522735\n",
      "[30]\tvalid_0's ndcg@5: 0.522764\n",
      "[40]\tvalid_0's ndcg@5: 0.523238\n",
      "[50]\tvalid_0's ndcg@5: 0.523729\n",
      "[60]\tvalid_0's ndcg@5: 0.523858\n",
      "[70]\tvalid_0's ndcg@5: 0.524315\n",
      "[80]\tvalid_0's ndcg@5: 0.524606\n",
      "[90]\tvalid_0's ndcg@5: 0.525169\n",
      "[100]\tvalid_0's ndcg@5: 0.525343\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's ndcg@5: 0.525514\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490411639179948, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490411639179948\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0013802083941009094, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013802083941009094\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.22222308460677365, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22222308460677365\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.517652\n",
      "[20]\tvalid_0's ndcg@5: 0.519379\n",
      "[30]\tvalid_0's ndcg@5: 0.519294\n",
      "[40]\tvalid_0's ndcg@5: 0.520323\n",
      "[50]\tvalid_0's ndcg@5: 0.521021\n",
      "[60]\tvalid_0's ndcg@5: 0.520493\n",
      "[70]\tvalid_0's ndcg@5: 0.521424\n",
      "[80]\tvalid_0's ndcg@5: 0.521474\n",
      "[90]\tvalid_0's ndcg@5: 0.521773\n",
      "[100]\tvalid_0's ndcg@5: 0.522108\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's ndcg@5: 0.522357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:14:19,348] Trial 6 finished with value: 2049.150050157909 and parameters: {'num_leaves': 59, 'learning_rate': 0.0001008598495816652, 'feature_fraction': 0.7490411639179948, 'bagging_freq': 9, 'verbose': 1, 'lambda_l1': 0.0013802083941009094, 'lambda_l2': 0.22222308460677365}. Best is trial 0 with value: 2213.7401692178437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.544397\n",
      "[20]\tvalid_0's ndcg@5: 0.549656\n",
      "[30]\tvalid_0's ndcg@5: 0.552516\n",
      "[40]\tvalid_0's ndcg@5: 0.554837\n",
      "[50]\tvalid_0's ndcg@5: 0.556809\n",
      "[60]\tvalid_0's ndcg@5: 0.557594\n",
      "[70]\tvalid_0's ndcg@5: 0.558871\n",
      "[80]\tvalid_0's ndcg@5: 0.559806\n",
      "[90]\tvalid_0's ndcg@5: 0.560705\n",
      "[100]\tvalid_0's ndcg@5: 0.561344\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.561344\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.544168\n",
      "[20]\tvalid_0's ndcg@5: 0.548576\n",
      "[30]\tvalid_0's ndcg@5: 0.552131\n",
      "[40]\tvalid_0's ndcg@5: 0.555097\n",
      "[50]\tvalid_0's ndcg@5: 0.557562\n",
      "[60]\tvalid_0's ndcg@5: 0.558295\n",
      "[70]\tvalid_0's ndcg@5: 0.560354\n",
      "[80]\tvalid_0's ndcg@5: 0.561053\n",
      "[90]\tvalid_0's ndcg@5: 0.562064\n",
      "[100]\tvalid_0's ndcg@5: 0.562977\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.562977\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.54315\n",
      "[20]\tvalid_0's ndcg@5: 0.550361\n",
      "[30]\tvalid_0's ndcg@5: 0.553077\n",
      "[40]\tvalid_0's ndcg@5: 0.555334\n",
      "[50]\tvalid_0's ndcg@5: 0.55814\n",
      "[60]\tvalid_0's ndcg@5: 0.559093\n",
      "[70]\tvalid_0's ndcg@5: 0.560438\n",
      "[80]\tvalid_0's ndcg@5: 0.560462\n",
      "[90]\tvalid_0's ndcg@5: 0.561549\n",
      "[100]\tvalid_0's ndcg@5: 0.56334\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.56334\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.545679\n",
      "[20]\tvalid_0's ndcg@5: 0.551207\n",
      "[30]\tvalid_0's ndcg@5: 0.554596\n",
      "[40]\tvalid_0's ndcg@5: 0.557128\n",
      "[50]\tvalid_0's ndcg@5: 0.559085\n",
      "[60]\tvalid_0's ndcg@5: 0.560084\n",
      "[70]\tvalid_0's ndcg@5: 0.56157\n",
      "[80]\tvalid_0's ndcg@5: 0.561754\n",
      "[90]\tvalid_0's ndcg@5: 0.562567\n",
      "[100]\tvalid_0's ndcg@5: 0.563726\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.563726\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.544733\n",
      "[20]\tvalid_0's ndcg@5: 0.549674\n",
      "[30]\tvalid_0's ndcg@5: 0.551506\n",
      "[40]\tvalid_0's ndcg@5: 0.553988\n",
      "[50]\tvalid_0's ndcg@5: 0.556241\n",
      "[60]\tvalid_0's ndcg@5: 0.557238\n",
      "[70]\tvalid_0's ndcg@5: 0.558583\n",
      "[80]\tvalid_0's ndcg@5: 0.559338\n",
      "[90]\tvalid_0's ndcg@5: 0.560471\n",
      "[100]\tvalid_0's ndcg@5: 0.561569\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.561569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:15:06,533] Trial 7 finished with value: 2229.968287063537 and parameters: {'num_leaves': 181, 'learning_rate': 0.028034464498512067, 'feature_fraction': 0.4411214866189548, 'bagging_freq': 5, 'verbose': 1, 'lambda_l1': 0.13593822637145558, 'lambda_l2': 5.060285363292936}. Best is trial 7 with value: 2229.968287063537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.524359\n",
      "[20]\tvalid_0's ndcg@5: 0.528368\n",
      "[30]\tvalid_0's ndcg@5: 0.529887\n",
      "[40]\tvalid_0's ndcg@5: 0.531887\n",
      "[50]\tvalid_0's ndcg@5: 0.532475\n",
      "[60]\tvalid_0's ndcg@5: 0.532635\n",
      "[70]\tvalid_0's ndcg@5: 0.533621\n",
      "[80]\tvalid_0's ndcg@5: 0.533628\n",
      "[90]\tvalid_0's ndcg@5: 0.534463\n",
      "[100]\tvalid_0's ndcg@5: 0.534869\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.535059\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.525007\n",
      "[20]\tvalid_0's ndcg@5: 0.528106\n",
      "[30]\tvalid_0's ndcg@5: 0.52917\n",
      "[40]\tvalid_0's ndcg@5: 0.530165\n",
      "[50]\tvalid_0's ndcg@5: 0.531174\n",
      "[60]\tvalid_0's ndcg@5: 0.532137\n",
      "[70]\tvalid_0's ndcg@5: 0.532564\n",
      "[80]\tvalid_0's ndcg@5: 0.532787\n",
      "[90]\tvalid_0's ndcg@5: 0.533575\n",
      "[100]\tvalid_0's ndcg@5: 0.53381\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.53391\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.526812\n",
      "[20]\tvalid_0's ndcg@5: 0.529753\n",
      "[30]\tvalid_0's ndcg@5: 0.531098\n",
      "[40]\tvalid_0's ndcg@5: 0.532956\n",
      "[50]\tvalid_0's ndcg@5: 0.533616\n",
      "[60]\tvalid_0's ndcg@5: 0.533733\n",
      "[70]\tvalid_0's ndcg@5: 0.534362\n",
      "[80]\tvalid_0's ndcg@5: 0.534966\n",
      "[90]\tvalid_0's ndcg@5: 0.53548\n",
      "[100]\tvalid_0's ndcg@5: 0.536151\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.536151\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.528541\n",
      "[20]\tvalid_0's ndcg@5: 0.531246\n",
      "[30]\tvalid_0's ndcg@5: 0.532438\n",
      "[40]\tvalid_0's ndcg@5: 0.534024\n",
      "[50]\tvalid_0's ndcg@5: 0.535631\n",
      "[60]\tvalid_0's ndcg@5: 0.535825\n",
      "[70]\tvalid_0's ndcg@5: 0.536477\n",
      "[80]\tvalid_0's ndcg@5: 0.536641\n",
      "[90]\tvalid_0's ndcg@5: 0.536991\n",
      "[100]\tvalid_0's ndcg@5: 0.53763\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.537637\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902311851086219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902311851086219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35212195814639785, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35212195814639785\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005134182382828798, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005134182382828798\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.526452\n",
      "[20]\tvalid_0's ndcg@5: 0.530127\n",
      "[30]\tvalid_0's ndcg@5: 0.531229\n",
      "[40]\tvalid_0's ndcg@5: 0.532322\n",
      "[50]\tvalid_0's ndcg@5: 0.533643\n",
      "[60]\tvalid_0's ndcg@5: 0.533732\n",
      "[70]\tvalid_0's ndcg@5: 0.534397\n",
      "[80]\tvalid_0's ndcg@5: 0.534615\n",
      "[90]\tvalid_0's ndcg@5: 0.535036\n",
      "[100]\tvalid_0's ndcg@5: 0.535344\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.535621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:15:54,586] Trial 8 finished with value: 2104.9164790678624 and parameters: {'num_leaves': 128, 'learning_rate': 0.0018525112556392307, 'feature_fraction': 0.7902311851086219, 'bagging_freq': 2, 'verbose': 1, 'lambda_l1': 0.35212195814639785, 'lambda_l2': 0.0005134182382828798}. Best is trial 7 with value: 2229.968287063537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518655\n",
      "[20]\tvalid_0's ndcg@5: 0.522154\n",
      "[30]\tvalid_0's ndcg@5: 0.523545\n",
      "[40]\tvalid_0's ndcg@5: 0.524541\n",
      "[50]\tvalid_0's ndcg@5: 0.525206\n",
      "[60]\tvalid_0's ndcg@5: 0.525936\n",
      "[70]\tvalid_0's ndcg@5: 0.526543\n",
      "[80]\tvalid_0's ndcg@5: 0.526341\n",
      "[90]\tvalid_0's ndcg@5: 0.527106\n",
      "[100]\tvalid_0's ndcg@5: 0.527477\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's ndcg@5: 0.527568\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518951\n",
      "[20]\tvalid_0's ndcg@5: 0.520951\n",
      "[30]\tvalid_0's ndcg@5: 0.521898\n",
      "[40]\tvalid_0's ndcg@5: 0.522999\n",
      "[50]\tvalid_0's ndcg@5: 0.523578\n",
      "[60]\tvalid_0's ndcg@5: 0.524685\n",
      "[70]\tvalid_0's ndcg@5: 0.525115\n",
      "[80]\tvalid_0's ndcg@5: 0.525149\n",
      "[90]\tvalid_0's ndcg@5: 0.525493\n",
      "[100]\tvalid_0's ndcg@5: 0.526007\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.526278\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.520604\n",
      "[20]\tvalid_0's ndcg@5: 0.52379\n",
      "[30]\tvalid_0's ndcg@5: 0.522907\n",
      "[40]\tvalid_0's ndcg@5: 0.524023\n",
      "[50]\tvalid_0's ndcg@5: 0.52514\n",
      "[60]\tvalid_0's ndcg@5: 0.52561\n",
      "[70]\tvalid_0's ndcg@5: 0.525764\n",
      "[80]\tvalid_0's ndcg@5: 0.525919\n",
      "[90]\tvalid_0's ndcg@5: 0.526639\n",
      "[100]\tvalid_0's ndcg@5: 0.526862\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.527006\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.520983\n",
      "[20]\tvalid_0's ndcg@5: 0.525177\n",
      "[30]\tvalid_0's ndcg@5: 0.52588\n",
      "[40]\tvalid_0's ndcg@5: 0.527499\n",
      "[50]\tvalid_0's ndcg@5: 0.528023\n",
      "[60]\tvalid_0's ndcg@5: 0.528464\n",
      "[70]\tvalid_0's ndcg@5: 0.529127\n",
      "[80]\tvalid_0's ndcg@5: 0.5292\n",
      "[90]\tvalid_0's ndcg@5: 0.529382\n",
      "[100]\tvalid_0's ndcg@5: 0.529949\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.529949\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8312158203879805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8312158203879805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04415184392357372, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04415184392357372\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.617036167482278e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.617036167482278e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.520398\n",
      "[20]\tvalid_0's ndcg@5: 0.52338\n",
      "[30]\tvalid_0's ndcg@5: 0.523678\n",
      "[40]\tvalid_0's ndcg@5: 0.52535\n",
      "[50]\tvalid_0's ndcg@5: 0.526269\n",
      "[60]\tvalid_0's ndcg@5: 0.526618\n",
      "[70]\tvalid_0's ndcg@5: 0.527367\n",
      "[80]\tvalid_0's ndcg@5: 0.52683\n",
      "[90]\tvalid_0's ndcg@5: 0.527294\n",
      "[100]\tvalid_0's ndcg@5: 0.527372\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-10 14:16:37,998] Trial 9 finished with value: 2072.1018117852645 and parameters: {'num_leaves': 79, 'learning_rate': 0.0009624133078358207, 'feature_fraction': 0.8312158203879805, 'bagging_freq': 6, 'verbose': 1, 'lambda_l1': 0.04415184392357372, 'lambda_l2': 2.617036167482278e-05}. Best is trial 7 with value: 2229.968287063537.\n"
     ]
    }
   ],
   "source": [
    "group_sizes = train_data.groupby('group').size()\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna parameters\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_at': 5,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': trial.suggest_int('verbose', 0, 1),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-5, 10.0, log=True),  # L1正則化\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-5, 10.0, log=True)  # L2正則化\n",
    "    }\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    ndcgs = []\n",
    "    \n",
    "    for train_index, valid_index in gkf.split(train_data, groups=train_data['group']):\n",
    "        train_fold_data = train_data.iloc[train_index]\n",
    "        valid_fold_data = train_data.iloc[valid_index]\n",
    "\n",
    "        train_fold_group_sizes = train_fold_data.groupby('group').size().tolist()\n",
    "        valid_fold_group_sizes = valid_fold_data.groupby('group').size().tolist()\n",
    "\n",
    "        ranker = lgb.LGBMRanker(**params)\n",
    "        ranker.fit(train_fold_data[features], train_fold_data[target], \n",
    "           group=train_fold_group_sizes, \n",
    "           eval_set=[(valid_fold_data[features], valid_fold_data[target])], \n",
    "           eval_group=[valid_fold_group_sizes],\n",
    "           eval_at=5, early_stopping_rounds=100, verbose=10)\n",
    "           # fobj=custom_objective,  # 損失関数\n",
    "           # sample_weight=sample_weights[train_index])  # サンプル重み\n",
    "        \n",
    "        y_pred = ranker.predict(valid_fold_data[features])\n",
    "        # top5_indices = rerank_predictions(y_pred)  # 再ランキング\n",
    "        ndcg_value = mean_ndcg_score(valid_fold_data[target].values, y_pred, valid_fold_group_sizes)\n",
    "        ndcgs.append(ndcg_value)\n",
    "    \n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54620a51-010d-4544-ad64-6cb8c3033b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1172\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.898807\n",
      "[20]\tvalid_0's ndcg@5: 0.900304\n",
      "[30]\tvalid_0's ndcg@5: 0.900627\n",
      "[40]\tvalid_0's ndcg@5: 0.901217\n",
      "[50]\tvalid_0's ndcg@5: 0.901675\n",
      "[60]\tvalid_0's ndcg@5: 0.901826\n",
      "[70]\tvalid_0's ndcg@5: 0.902145\n",
      "[80]\tvalid_0's ndcg@5: 0.902204\n",
      "[90]\tvalid_0's ndcg@5: 0.902375\n",
      "[100]\tvalid_0's ndcg@5: 0.902621\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902621\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.898704\n",
      "[20]\tvalid_0's ndcg@5: 0.899744\n",
      "[30]\tvalid_0's ndcg@5: 0.900704\n",
      "[40]\tvalid_0's ndcg@5: 0.901127\n",
      "[50]\tvalid_0's ndcg@5: 0.90174\n",
      "[60]\tvalid_0's ndcg@5: 0.901802\n",
      "[70]\tvalid_0's ndcg@5: 0.901958\n",
      "[80]\tvalid_0's ndcg@5: 0.90211\n",
      "[90]\tvalid_0's ndcg@5: 0.902271\n",
      "[100]\tvalid_0's ndcg@5: 0.902417\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902417\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.898797\n",
      "[20]\tvalid_0's ndcg@5: 0.899916\n",
      "[30]\tvalid_0's ndcg@5: 0.900713\n",
      "[40]\tvalid_0's ndcg@5: 0.901146\n",
      "[50]\tvalid_0's ndcg@5: 0.901528\n",
      "[60]\tvalid_0's ndcg@5: 0.901866\n",
      "[70]\tvalid_0's ndcg@5: 0.901983\n",
      "[80]\tvalid_0's ndcg@5: 0.902116\n",
      "[90]\tvalid_0's ndcg@5: 0.902132\n",
      "[100]\tvalid_0's ndcg@5: 0.902258\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902258\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.898941\n",
      "[20]\tvalid_0's ndcg@5: 0.900168\n",
      "[30]\tvalid_0's ndcg@5: 0.900749\n",
      "[40]\tvalid_0's ndcg@5: 0.90123\n",
      "[50]\tvalid_0's ndcg@5: 0.901864\n",
      "[60]\tvalid_0's ndcg@5: 0.90185\n",
      "[70]\tvalid_0's ndcg@5: 0.902093\n",
      "[80]\tvalid_0's ndcg@5: 0.902277\n",
      "[90]\tvalid_0's ndcg@5: 0.902517\n",
      "[100]\tvalid_0's ndcg@5: 0.902712\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.902755\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4411214866189548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4411214866189548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.13593822637145558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13593822637145558\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.060285363292936, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.060285363292936\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.89838\n",
      "[20]\tvalid_0's ndcg@5: 0.899492\n",
      "[30]\tvalid_0's ndcg@5: 0.899998\n",
      "[40]\tvalid_0's ndcg@5: 0.900577\n",
      "[50]\tvalid_0's ndcg@5: 0.901243\n",
      "[60]\tvalid_0's ndcg@5: 0.901409\n",
      "[70]\tvalid_0's ndcg@5: 0.901468\n",
      "[80]\tvalid_0's ndcg@5: 0.901751\n",
      "[90]\tvalid_0's ndcg@5: 0.901862\n",
      "[100]\tvalid_0's ndcg@5: 0.901904\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.901928\n"
     ]
    }
   ],
   "source": [
    "# 最適なパラメータ\n",
    "best_params = study.best_params\n",
    "\n",
    "# KFoldでのモデル訓練\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = []\n",
    "\n",
    "for train_index, valid_index in kf.split(train_data):\n",
    "    train_fold_data = train_data.iloc[train_index]\n",
    "    valid_fold_data = train_data.iloc[valid_index]\n",
    "    \n",
    "    train_fold_group_sizes = train_fold_data.groupby('group').size().tolist()\n",
    "    valid_fold_group_sizes = valid_fold_data.groupby('group').size().tolist()\n",
    "    \n",
    "    ranker = lgb.LGBMRanker(**best_params)\n",
    "    ranker.fit(\n",
    "        train_fold_data[features], train_fold_data[target], \n",
    "        group=train_fold_group_sizes, \n",
    "        eval_set=[(valid_fold_data[features], valid_fold_data[target])], \n",
    "        eval_group=[valid_fold_group_sizes], \n",
    "        eval_at=5, early_stopping_rounds=20, verbose=10\n",
    "    )\n",
    "    models.append(ranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50618bb5-4841-4cb9-b4cb-cc029629358c",
   "metadata": {},
   "source": [
    "### テストデータで予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59de8ac9-a8f5-45fc-a650-2f6d47ffc9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>predicted_rank</th>\n",
       "      <th>kakutei_chakujun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1635601</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635602</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635603</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635604</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635605</th>\n",
       "      <td>2021-101-45-1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785604</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785605</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785606</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785607</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785608</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147974 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   group  predicted_rank  kakutei_chakujun\n",
       "1635601    2021-222-83-2             8.0                 6\n",
       "1635602    2021-222-83-2             2.0                 7\n",
       "1635603    2021-222-83-2            10.0                 1\n",
       "1635604    2021-222-83-2             1.0                 2\n",
       "1635605    2021-101-45-1             5.0                 1\n",
       "...                  ...             ...               ...\n",
       "1785604  2021-1231-54-11            11.0                 3\n",
       "1785605  2021-1231-54-11             3.0                 1\n",
       "1785606  2021-1231-54-11             5.0                 2\n",
       "1785607  2021-1231-54-11             4.0                11\n",
       "1785608  2021-1231-54-11             7.0                 5\n",
       "\n",
       "[147974 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2020年から2022年のテストデータを取得\n",
    "test_data_2020 = merged_df[merged_df['kaisai_nen'] == 2020].copy()\n",
    "test_data_2021 = merged_df[merged_df['kaisai_nen'] == 2021].copy()\n",
    "test_data_2022 = merged_df[merged_df['kaisai_nen'] == 2022].copy()\n",
    "\n",
    "# 2021年と2022年のデータに対して予測を行い、平均を取る\n",
    "test_data_2020.loc[:, 'y_pred'] = sum([model.predict(test_data_2020[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "test_data_2021.loc[:, 'y_pred'] = sum([model.predict(test_data_2021[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "test_data_2022.loc[:, 'y_pred'] = sum([model.predict(test_data_2022[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "\n",
    "# 予測されたランクをグループごとに計算\n",
    "test_data_2020.loc[:, 'predicted_rank'] = test_data_2020.groupby('group')['y_pred'].rank(method='min')\n",
    "test_data_2021.loc[:, 'predicted_rank'] = test_data_2021.groupby('group')['y_pred'].rank(method='min')\n",
    "test_data_2022.loc[:, 'predicted_rank'] = test_data_2022.groupby('group')['y_pred'].rank(method='min')\n",
    "\n",
    "# 結果を表示\n",
    "test_data_2021[['group', 'predicted_rank', 'kakutei_chakujun']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb294d-cc15-4ece-8051-7e9a1f1ca38c",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3c7ea0-effe-4b02-a1db-9f38b16e8542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 RMSE: 365.291%\n",
      "2021 RMSE: 371.245%\n",
      "2022 RMSE: 370.603%\n",
      "Mean RMSE: 369.046%\n"
     ]
    }
   ],
   "source": [
    "# RMSEを計算\n",
    "rmse_2020 = np.sqrt(mean_squared_error(test_data_2020['predicted_rank'], test_data_2020['kakutei_chakujun']))\n",
    "print(f\"2021 RMSE: {rmse_2020:.3%}\")\n",
    "rmse_2021 = np.sqrt(mean_squared_error(test_data_2021['predicted_rank'], test_data_2021['kakutei_chakujun']))\n",
    "print(f\"2021 RMSE: {rmse_2021:.3%}\")\n",
    "rmse_2022 = np.sqrt(mean_squared_error(test_data_2022['predicted_rank'], test_data_2022['kakutei_chakujun']))\n",
    "print(f\"2022 RMSE: {rmse_2022:.3%}\")\n",
    "rmse_mean = np.mean([rmse_2020, rmse_2021, rmse_2022])\n",
    "print(f\"Mean RMSE: {rmse_mean:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67d558ef-cfb1-42ed-aebf-8a7d6e851def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_5(predictions, actual):\n",
    "    top_5_predictions = predictions.argsort()[-5:][::-1]  # 予測の上位5頭のインデックス\n",
    "    top_3_actual = actual.argsort()[-3:][::-1]  # 実際の上位3頭のインデックス\n",
    "    common_elements = np.intersect1d(top_5_predictions, top_3_actual)  # 共通の要素を抽出\n",
    "    precision = len(common_elements) / 5\n",
    "    return precision\n",
    "\n",
    "def recall_at_5(predictions, actual):\n",
    "    top_5_predictions = predictions.argsort()[-5:][::-1]  # 予測の上位5頭のインデックス\n",
    "    top_3_actual = actual.argsort()[-3:][::-1]  # 実際の上位3頭のインデックス\n",
    "    common_elements = np.intersect1d(top_5_predictions, top_3_actual)  # 共通の要素を抽出\n",
    "    recall = len(common_elements) / 3\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19fcf8c0-d484-4492-9ca0-2e4b65bb7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_top3_in_top5_predictions(group):\n",
    "    predicted_top5 = group.nsmallest(5, 'y_pred').index.tolist()\n",
    "    actual_top3 = group.nsmallest(3, 'kakutei_chakujun').index.tolist()\n",
    "    return all([horse in predicted_top5 for horse in actual_top3])\n",
    "\n",
    "def calculate_group_profit(group):\n",
    "    if check_top3_in_top5_predictions(group):\n",
    "        payout_value = group['haraimodoshi_sanrenpuku_1b'].iloc[0]\n",
    "        return payout_value - 1000\n",
    "    else:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b30d6ff-8e24-4188-9d8f-f569b483b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test_data in [test_data_2020, test_data_2021, test_data_2022]:\n",
    "#     merged_data = test_data\n",
    "#     profits = merged_data.groupby('group').apply(calculate_group_profit).reset_index()\n",
    "#     profits.columns = ['group', 'profit']\n",
    "#     test_data = pd.merge(test_data, profits, on='group', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c260fed-21fe-4db0-9bbb-e378b2443d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 Precision@5: 38.702%\n",
      "2020 Recall@5: 64.503%\n",
      "2021 Precision@5: 38.337%\n",
      "2021 Recall@5: 63.895%\n",
      "2022 Precision@5: 38.277%\n",
      "2022 Recall@5: 63.795%\n",
      "Mean Precision@5: 38.438%\n",
      "Mean Recall@5: 64.064%\n",
      "2020 Average Net Profit: 554.560 yen\n",
      "2021 Average Net Profit: 512.140 yen\n",
      "2022 Average Net Profit: 553.179 yen\n",
      "Mean Average Net Profit: 539.960 yen\n"
     ]
    }
   ],
   "source": [
    "group_ids_2020 = test_data_2020['group'].unique()\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for group_id in group_ids_2020:\n",
    "    test_data = test_data_2020[test_data_2020['group'] == group_id]\n",
    "    p = precision_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    r = recall_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "\n",
    "precision_5_2020 = np.mean(precisions)\n",
    "recall_5_2020 = np.mean(recalls)\n",
    "\n",
    "print(f\"2020 Precision@5: {precision_5_2020:.3%}\")\n",
    "print(f\"2020 Recall@5: {recall_5_2020:.3%}\")\n",
    "\n",
    "group_ids_2021 = test_data_2021['group'].unique()\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for group_id in group_ids_2021:\n",
    "    test_data = test_data_2021[test_data_2021['group'] == group_id]\n",
    "    p = precision_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    r = recall_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "\n",
    "precision_5_2021 = np.mean(precisions)\n",
    "recall_5_2021 = np.mean(recalls)\n",
    "\n",
    "print(f\"2021 Precision@5: {precision_5_2021:.3%}\")\n",
    "print(f\"2021 Recall@5: {recall_5_2021:.3%}\")\n",
    "\n",
    "\n",
    "roup_ids_2022 = test_data_2022['group'].unique()\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for group_id in roup_ids_2022:\n",
    "    test_data = test_data_2022[test_data_2022['group'] == group_id]\n",
    "    p = precision_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    r = recall_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "\n",
    "precision_5_2022 = np.mean(precisions)\n",
    "recall_5_2022 = np.mean(recalls)\n",
    "\n",
    "print(f\"2022 Precision@5: {precision_5_2022:.3%}\")\n",
    "print(f\"2022 Recall@5: {recall_5_2022:.3%}\")\n",
    "\n",
    "\n",
    "precision_5_mean = np.mean([precision_5_2020, precision_5_2021, precision_5_2022])\n",
    "recall_5_mean = np.mean([recall_5_2020, recall_5_2021, recall_5_2022])\n",
    "\n",
    "print(f\"Mean Precision@5: {precision_5_mean:.3%}\")\n",
    "print(f\"Mean Recall@5: {recall_5_mean:.3%}\")\n",
    "\n",
    "# 各レースで上記の関数を適用\n",
    "results_2020 = test_data_2020.groupby('group').apply(check_top3_in_top5_predictions)\n",
    "results_2021 = test_data_2021.groupby('group').apply(check_top3_in_top5_predictions)\n",
    "results_2022 = test_data_2022.groupby('group').apply(check_top3_in_top5_predictions)\n",
    "\n",
    "# 予測モデルが成功したレースのインデックスを取得する\n",
    "successful_groups_2020 = results_2020[results_2020].index\n",
    "successful_groups_2021 = results_2021[results_2021].index\n",
    "successful_groups_2022 = results_2022[results_2022].index\n",
    "\n",
    "# そのインデックスを使用して、harai_dfから対応する支払倍率を取得する\n",
    "successful_payout_2020 = merged_df[merged_df['group'].isin(successful_groups_2020)]\n",
    "successful_payout_2021 = merged_df[merged_df['group'].isin(successful_groups_2021)]\n",
    "successful_payout_2022 = merged_df[merged_df['group'].isin(successful_groups_2022)]\n",
    "\n",
    "payouts_2020 = successful_payout_2020['haraimodoshi_sanrenpuku_1b']\n",
    "payouts_2021 = successful_payout_2021['haraimodoshi_sanrenpuku_1b']\n",
    "payouts_2022 = successful_payout_2022['haraimodoshi_sanrenpuku_1b']\n",
    "\n",
    "# すべての成功したレースに対して、100円をかけた場合の支払いを計算する\n",
    "total_payout_2020 = (payouts_2020).sum()\n",
    "total_payout_2021 = (payouts_2021).sum()\n",
    "total_payout_2022 = (payouts_2022).sum()\n",
    "\n",
    "# 合計の支払いから、すべてのレースにかけた合計金額を引くことで、純利益を計算する\n",
    "total_investment_2020 = test_data_2020['group'].nunique() * 1000\n",
    "total_investment_2021 = test_data_2021['group'].nunique() * 1000\n",
    "total_investment_2022 = test_data_2021['group'].nunique() * 1000\n",
    "\n",
    "net_profit_2020 = total_payout_2020 - total_investment_2020\n",
    "net_profit_2021 = total_payout_2021 - total_investment_2021\n",
    "net_profit_2022 = total_payout_2022 - total_investment_2022\n",
    "\n",
    "average_net_profit_2020 = net_profit_2020/len(test_data_2020)\n",
    "average_net_profit_2021 = net_profit_2021/len(test_data_2021)\n",
    "average_net_profit_2022 = net_profit_2020/len(test_data_2022)\n",
    "\n",
    "print(f\"2020 Average Net Profit: {average_net_profit_2020:.3f} yen\")\n",
    "print(f\"2021 Average Net Profit: {average_net_profit_2021:.3f} yen\")\n",
    "print(f\"2022 Average Net Profit: {average_net_profit_2022:.3f} yen\")\n",
    "\n",
    "mean_average_net_profit = np.mean([average_net_profit_2020, average_net_profit_2021, average_net_profit_2022])\n",
    "print(f\"Mean Average Net Profit: {mean_average_net_profit:.3f} yen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cba3cc9d-fed6-4870-8be0-6eafd8219f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# models[0]'s feature importances\n",
    "feature_importance = models[0].feature_importances_\n",
    "\n",
    "# Convert feature names and importances to DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b012e5-3fdb-4152-966e-1b76dd8e46bc",
   "metadata": {},
   "source": [
    "# モデル保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f56bbe4-89fe-4ca0-8ee4-3c4a6daed162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1172\n",
      "[LightGBM] [Info] Number of data points in the train set: 2017402, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 5.554518\n",
      "Model saved to ./bestmodels/lightgbmranker.pkl\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "full_train_dataset = lgb.Dataset(merged_df[features], label=merged_df[target])\n",
    "full_model = lgb.train(best_params, full_train_dataset, num_boost_round=500, verbose_eval=100)  # ここではvalid_setsやearly_stopping_roundsは使用しない\n",
    "\n",
    "# モデルを保存\n",
    "# model_save_path = '../app/models/model.pkl'\n",
    "model_save_path = './bestmodels/lightgbmranker.pkl'\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(full_model, f)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1347bfe-200d-49c4-8f13-6bf97f2b6f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "2020-1001-30-1     False\n",
       "2020-1001-30-10    False\n",
       "2020-1001-30-11    False\n",
       "2020-1001-30-12    False\n",
       "2020-1001-30-2      True\n",
       "                   ...  \n",
       "2020-930-50-5       True\n",
       "2020-930-50-6      False\n",
       "2020-930-50-7      False\n",
       "2020-930-50-8       True\n",
       "2020-930-50-9      False\n",
       "Length: 15067, dtype: bool"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fb97dbd-e5c0-4f2b-8b9f-af80989301c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2020-1001-30-2', '2020-1001-30-4', '2020-1001-30-7', '2020-1001-30-9',\n",
       "       '2020-1001-43-2', '2020-1001-43-5', '2020-1001-48-10', '2020-1001-48-3',\n",
       "       '2020-1001-48-5', '2020-1001-48-7',\n",
       "       ...\n",
       "       '2020-930-43-4', '2020-930-43-7', '2020-930-43-8', '2020-930-48-1',\n",
       "       '2020-930-48-11', '2020-930-48-12', '2020-930-50-1', '2020-930-50-4',\n",
       "       '2020-930-50-5', '2020-930-50-8'],\n",
       "      dtype='object', name='group', length=4048)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successful_groups_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19acc4-0eb7-4ac7-8e31-146549b97530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
