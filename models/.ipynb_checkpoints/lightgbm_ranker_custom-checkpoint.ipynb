{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ca9841-adc6-4612-b82f-f2f94a32faa7",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d127aa-802a-4ec0-9fc9-e2671c2dba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#モデル\n",
    "import lightgbm as lgb\n",
    "\n",
    "#パラメータ探索\n",
    "import optuna\n",
    "\n",
    "#クロスバリデーション\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# 評価指標\n",
    "import sys\n",
    "sys.path.append('./evaluation')\n",
    "from rmse import rmse\n",
    "from recall5 import recall5\n",
    "from profit import profit\n",
    "\n",
    "#可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#保存\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11351b30-8083-423f-a50c-46bfde76436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを読み込む\n",
    "n_uma_race_df = pd.read_pickle('../datasets/traindata/n_uma_race.pkl')\n",
    "n_race_df = pd.read_pickle('../datasets/traindata/n_race.pkl')\n",
    "n_payout_df = pd.read_pickle('../datasets/traindata/n_payout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c664d196-e826-4eba-9cf8-851ea81544cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新しいグループを作成\n",
    "n_uma_race_df['group'] = n_uma_race_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_uma_race_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_uma_race_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_uma_race_df['race_bango'].astype(int).astype(str)\n",
    "n_race_df['group'] = n_race_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_race_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_race_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_race_df['race_bango'].astype(int).astype(str)\n",
    "n_payout_df['group'] = n_payout_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_payout_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_payout_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_payout_df['race_bango'].astype(int).astype(str)\n",
    "\n",
    "n_race_df = n_race_df.drop(['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'kaisai_kai', 'kaisai_nichime', 'race_bango'],axis=1)\n",
    "n_payout_df = n_payout_df.drop(['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'kaisai_kai', 'kaisai_nichime', 'race_bango', 'toroku_tosu', 'shusso_tosu'],axis=1)\n",
    "\n",
    "merged_df = pd.merge(n_uma_race_df, n_race_df, on='group', how='left')\n",
    "merged_df = pd.merge(merged_df, n_payout_df, on='group', how='left')\n",
    "merged_df['group'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68c59eb-a4db-40af-b9cf-c5aba98aa72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-916-46-1', '2023-916-46-2', '2023-916-46-3', ...,\n",
       "       '2023-907-50-10', '2023-907-50-11', '2023-907-50-12'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ff2e39-36ac-42c3-b039-0a870a1dc2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kaisai_nen</th>\n",
       "      <th>kaisai_tsukihi</th>\n",
       "      <th>keibajo_code</th>\n",
       "      <th>kaisai_kai</th>\n",
       "      <th>kaisai_nichime</th>\n",
       "      <th>race_bango</th>\n",
       "      <th>wakuban</th>\n",
       "      <th>umaban</th>\n",
       "      <th>ketto_toroku_bango</th>\n",
       "      <th>bamei</th>\n",
       "      <th>umakigo_code</th>\n",
       "      <th>seibetsu_code</th>\n",
       "      <th>hinshu_code</th>\n",
       "      <th>moshoku_code</th>\n",
       "      <th>barei</th>\n",
       "      <th>tozai_shozoku_code</th>\n",
       "      <th>chokyoshi_code</th>\n",
       "      <th>banushi_code</th>\n",
       "      <th>banushimei</th>\n",
       "      <th>futan_juryo</th>\n",
       "      <th>blinker_shiyo_kubun</th>\n",
       "      <th>kishu_code</th>\n",
       "      <th>kishumei_ryakusho</th>\n",
       "      <th>kishu_minarai_code</th>\n",
       "      <th>bataiju</th>\n",
       "      <th>zogen_fugo</th>\n",
       "      <th>zogen_sa</th>\n",
       "      <th>ijo_kubun_code</th>\n",
       "      <th>nyusen_juni</th>\n",
       "      <th>kakutei_chakujun</th>\n",
       "      <th>dochaku_kubun</th>\n",
       "      <th>dochaku_tosu</th>\n",
       "      <th>soha_time</th>\n",
       "      <th>chakusa_code_1</th>\n",
       "      <th>chakusa_code_2</th>\n",
       "      <th>chakusa_code_3</th>\n",
       "      <th>corner_1</th>\n",
       "      <th>corner_2</th>\n",
       "      <th>corner_3</th>\n",
       "      <th>corner_4</th>\n",
       "      <th>tansho_odds</th>\n",
       "      <th>tansho_ninkijun</th>\n",
       "      <th>kakutoku_honshokin</th>\n",
       "      <th>kakutoku_fukashokin</th>\n",
       "      <th>kohan_4f</th>\n",
       "      <th>kohan_3f</th>\n",
       "      <th>aiteuma_joho_1</th>\n",
       "      <th>aiteuma_joho_2</th>\n",
       "      <th>aiteuma_joho_3</th>\n",
       "      <th>time_sa</th>\n",
       "      <th>record_koshin_kubun</th>\n",
       "      <th>kyakushitsu_hantei</th>\n",
       "      <th>group</th>\n",
       "      <th>yobi_code</th>\n",
       "      <th>jusho_kaiji</th>\n",
       "      <th>grade_code</th>\n",
       "      <th>kyoso_shubetsu_code</th>\n",
       "      <th>kyoso_kigo_code</th>\n",
       "      <th>juryo_shubetsu_code</th>\n",
       "      <th>kyoso_joken_code_2sai</th>\n",
       "      <th>kyoso_joken_code_3sai</th>\n",
       "      <th>kyoso_joken_code_4sai</th>\n",
       "      <th>kyoso_joken_code_5sai_ijo</th>\n",
       "      <th>kyoso_joken_code</th>\n",
       "      <th>kyori</th>\n",
       "      <th>track_code</th>\n",
       "      <th>course_kubun</th>\n",
       "      <th>honshokin</th>\n",
       "      <th>fukashokin</th>\n",
       "      <th>hasso_jikoku</th>\n",
       "      <th>toroku_tosu</th>\n",
       "      <th>shusso_tosu</th>\n",
       "      <th>nyusen_tosu</th>\n",
       "      <th>tenko_code</th>\n",
       "      <th>babajotai_code_shiba</th>\n",
       "      <th>babajotai_code_dirt</th>\n",
       "      <th>fuseiritsu_flag_sanrenpuku</th>\n",
       "      <th>tokubarai_flag_sanrenpuku</th>\n",
       "      <th>henkan_flag_sanrenpuku</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1a</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1b</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>916</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020106227</td>\n",
       "      <td>スターサファイア</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5319</td>\n",
       "      <td>875800</td>\n",
       "      <td>ＪＰＮ技研</td>\n",
       "      <td>560.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5300</td>\n",
       "      <td>葛山晃平</td>\n",
       "      <td>0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1359</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>899</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>2020105068フェイマスグローリ</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-916-46-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1400</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000e+51</td>\n",
       "      <td>0</td>\n",
       "      <td>1140</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60809.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kaisai_nen  kaisai_tsukihi keibajo_code  kaisai_kai  kaisai_nichime  \\\n",
       "0        2023             916           46          13               1   \n",
       "\n",
       "   race_bango  wakuban  umaban  ketto_toroku_bango               bamei  \\\n",
       "0           1        1       1          2020106227  スターサファイア　　　　　　　　　　   \n",
       "\n",
       "   umakigo_code  seibetsu_code  hinshu_code  moshoku_code  barei  \\\n",
       "0             0              1          1.0             3      3   \n",
       "\n",
       "   tozai_shozoku_code  chokyoshi_code  banushi_code  \\\n",
       "0                   3            5319        875800   \n",
       "\n",
       "                         banushimei  futan_juryo  blinker_shiyo_kubun  \\\n",
       "0  ＪＰＮ技研　　　　　　　　　　　　　　　　　　　　　　　　　　　        560.0                    0   \n",
       "\n",
       "   kishu_code kishumei_ryakusho  kishu_minarai_code  bataiju  zogen_fugo  \\\n",
       "0        5300              葛山晃平                   0    466.0           1   \n",
       "\n",
       "   zogen_sa  ijo_kubun_code  nyusen_juni  kakutei_chakujun  dochaku_kubun  \\\n",
       "0       0.0               0            8                 8              0   \n",
       "\n",
       "   dochaku_tosu  soha_time  chakusa_code_1  chakusa_code_2  chakusa_code_3  \\\n",
       "0             0       1359             4.0             NaN             NaN   \n",
       "\n",
       "   corner_1  corner_2  corner_3  corner_4  tansho_odds  tansho_ninkijun  \\\n",
       "0         9         9         9         9          899                8   \n",
       "\n",
       "   kakutoku_honshokin  kakutoku_fukashokin  kohan_4f  kohan_3f  \\\n",
       "0                   0                    0         0       411   \n",
       "\n",
       "                 aiteuma_joho_1                aiteuma_joho_2  \\\n",
       "0  2020105068フェイマスグローリ　　　　　　　　　  0000000000　　　　　　　　　　　　　　　　　　   \n",
       "\n",
       "                 aiteuma_joho_3  time_sa  record_koshin_kubun  \\\n",
       "0  0000000000　　　　　　　　　　　　　　　　　　       50                    0   \n",
       "\n",
       "   kyakushitsu_hantei          group  yobi_code  jusho_kaiji  grade_code  \\\n",
       "0                   0  2023-916-46-1          1            0           0   \n",
       "\n",
       "   kyoso_shubetsu_code  kyoso_kigo_code  juryo_shubetsu_code  \\\n",
       "0                   49                0                    4   \n",
       "\n",
       "   kyoso_joken_code_2sai  kyoso_joken_code_3sai  kyoso_joken_code_4sai  \\\n",
       "0                      0                      0                      0   \n",
       "\n",
       "   kyoso_joken_code_5sai_ijo  kyoso_joken_code  kyori  track_code  \\\n",
       "0                          0                 0   1400          24   \n",
       "\n",
       "   course_kubun     honshokin  fukashokin  hasso_jikoku  toroku_tosu  \\\n",
       "0             0  4.000000e+51           0          1140            9   \n",
       "\n",
       "   shusso_tosu  nyusen_tosu  tenko_code  babajotai_code_shiba  \\\n",
       "0            9            9           1                     0   \n",
       "\n",
       "   babajotai_code_dirt  fuseiritsu_flag_sanrenpuku  tokubarai_flag_sanrenpuku  \\\n",
       "0                  1.0                         0.0                        0.0   \n",
       "\n",
       "   henkan_flag_sanrenpuku  haraimodoshi_sanrenpuku_1a  \\\n",
       "0                     0.0                     60809.0   \n",
       "\n",
       "   haraimodoshi_sanrenpuku_1b  haraimodoshi_sanrenpuku_1c  \n",
       "0                       210.0                         1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1919f-72fe-444c-8021-dac0032d6065",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43e44e6-e588-4837-9eab-2c1542f91680",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['hutan_wariai'] = merged_df['futan_juryo'] / merged_df['bataiju']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3207c5cf-150b-40c0-9b0f-278f94680871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sign_and_diff(row):\n",
    "    if row['zogen_fugo'] == 2:\n",
    "        return row['zogen_sa']\n",
    "    elif row['zogen_fugo'] == 0:\n",
    "        return -row['zogen_sa']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "merged_df['zogen_ryou'] = merged_df.apply(combine_sign_and_diff, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9622447d-bc27-4919-8ec1-39e9a3d2bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['ijo_kubun_code'] == 0]\n",
    "# 1\t出走取消\t取消\tSCRATCHED\tS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2b6db6-3968-4a63-ab23-5b7a73ecc35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyori のデータ型: int32\n",
      "grade_code のデータ型: int32\n",
      "seibetsu_code のデータ型: int32\n",
      "moshoku_code のデータ型: int32\n",
      "barei のデータ型: int32\n",
      "chokyoshi_code のデータ型: int32\n",
      "banushi_code のデータ型: int32\n",
      "kishu_code のデータ型: int32\n",
      "kishu_minarai_code のデータ型: int32\n",
      "kyoso_shubetsu_code のデータ型: int32\n",
      "juryo_shubetsu_code のデータ型: int32\n",
      "shusso_tosu のデータ型: int32\n",
      "tenko_code のデータ型: int32\n",
      "babajotai_code_dirt のデータ型: int32\n",
      "hutan_wariai のデータ型: float64\n",
      "zogen_ryou のデータ型: int32\n",
      "track_code のデータ型: int32\n",
      "keibajo_code のデータ型: int32\n"
     ]
    }
   ],
   "source": [
    "columns_to_convert = [\n",
    "                    'kyori',\n",
    "                    'grade_code',\n",
    "                    'seibetsu_code',\n",
    "                    'moshoku_code',\n",
    "                    'barei',\n",
    "                    'chokyoshi_code',\n",
    "                    'banushi_code',\n",
    "                    'kishu_code',\n",
    "                    'kishu_minarai_code',\n",
    "                    'kyoso_shubetsu_code',\n",
    "                    'juryo_shubetsu_code',\n",
    "                    'shusso_tosu',\n",
    "                    'tenko_code',\n",
    "                    'babajotai_code_dirt',\n",
    "                    'hutan_wariai',\n",
    "                    'zogen_ryou',\n",
    "                    'track_code',\n",
    "                    'keibajo_code'\n",
    "                    ]\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    merged_df[column].fillna(0, inplace=True)\n",
    "    try:\n",
    "        if merged_df[column].astype(float).apply(lambda x: x.is_integer()).all():\n",
    "            merged_df[column] = merged_df[column].astype(int)\n",
    "        else:\n",
    "            merged_df[column] = merged_df[column].astype(float)\n",
    "    except ValueError:\n",
    "        merged_df[column] = merged_df[column].astype(float)\n",
    "\n",
    "    print(f\"{column} のデータ型: {merged_df[column].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a07b2c5-f805-499d-849d-d7cd88bf1f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 2017402 entries, 0 to 2042540\n",
      "Series name: keibajo_code\n",
      "Non-Null Count    Dtype\n",
      "--------------    -----\n",
      "2017402 non-null  int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 23.1 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df[column].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bf1c2-3a54-4a11-9309-d830239210d8",
   "metadata": {},
   "source": [
    "# lightgbm.LGBMRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "165fa6a7-87f9-4cf1-8cdc-3421e1c4a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010年から2019年のデータを学習データとして取得\n",
    "train_data = merged_df[merged_df['kaisai_nen'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9740ae80-b106-4159-a01e-765f87246544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ランキング学習のために必要な特徴量とターゲットを設定\n",
    "features = [\n",
    "            # 'kyori',\n",
    "            # 'grade_code',\n",
    "            'seibetsu_code',\n",
    "            'moshoku_code',\n",
    "            'barei',\n",
    "            'chokyoshi_code',\n",
    "            'banushi_code',\n",
    "            'kishu_code',\n",
    "            'kishu_minarai_code',\n",
    "            'kyoso_shubetsu_code',\n",
    "            'juryo_shubetsu_code',\n",
    "            # 'shusso_tosu',\n",
    "            # 'tenko_code',\n",
    "            # 'babajotai_code_dirt',\n",
    "            'hutan_wariai',\n",
    "            'zogen_ryou',\n",
    "            # 'track_code',\n",
    "            # 'keibajo_code'\n",
    "            ]\n",
    "\n",
    "target = 'kakutei_chakujun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "228abcf6-d782-4a41-8fa3-41594a5b3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_score(y_true, y_score, k=5):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    \n",
    "    gain = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "def mean_ndcg_score(y_true, y_score, groups, k=5):\n",
    "    ndcgs = []\n",
    "    idx_start = 0\n",
    "    for group in groups:\n",
    "        ndcgs.append(ndcg_score(y_true[idx_start:idx_start+group], y_score[idx_start:idx_start+group], k))\n",
    "        idx_start += group\n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4147b41-467a-4946-b929-6eb017229b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def lambda_rank_gradient(y_true, y_pred):\n",
    "#     pred_diff = y_pred[:, None] - y_pred[None, :]\n",
    "#     true_diff = y_true[:, None] - y_true[None, :]\n",
    "    \n",
    "#     S_ij = np.where(true_diff > 0, 1, np.where(true_diff < 0, -1, 0))\n",
    "    \n",
    "#     lambda_ij = 0.5 * (1 - S_ij) - sigmoid(-pred_diff)\n",
    "    \n",
    "#     grad = np.sum(lambda_ij, axis=1)\n",
    "    \n",
    "#     hess = np.sum(sigmoid(pred_diff) * (1 - sigmoid(pred_diff)), axis=1)\n",
    "#     return grad, hess\n",
    "\n",
    "# def lambda_rank_gradient_efficient(y_true, y_pred, query_group):\n",
    "#     grad = np.zeros_like(y_pred)\n",
    "#     hess = np.zeros_like(y_pred)\n",
    "    \n",
    "#     start_idx = 0\n",
    "#     for group_size in query_group:\n",
    "#         group_true = y_true[start_idx:start_idx+group_size]\n",
    "#         group_pred = y_pred[start_idx:start_idx+group_size]\n",
    "        \n",
    "#         for i in range(group_size):\n",
    "#             for j in range(group_size):\n",
    "#                 if group_true[i] == group_true[j]:\n",
    "#                     continue\n",
    "#                 S_ij = 1 if group_true[i] > group_true[j] else -1\n",
    "#                 pred_diff = group_pred[i] - group_pred[j]\n",
    "#                 logistic = 1 / (1 + np.exp(S_ij * pred_diff))\n",
    "#                 grad[i] += S_ij * logistic\n",
    "#                 hess[i] += logistic * (1 - logistic)\n",
    "        \n",
    "#         start_idx += group_size\n",
    "    \n",
    "#     return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eef21f3-910b-4c2c-a759-b3444fdd5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 損失関数\n",
    "# def custom_objective(y_true, y_pred, dataset):\n",
    "#     if isinstance(y_pred, lgb.basic.Dataset):\n",
    "#         y_pred = y_pred.get_label()\n",
    "\n",
    "#     query_group = dataset.get_group()  # クエリのグループサイズを取得\n",
    "#     grad, hess = lambda_rank_gradient_efficient(np.array(y_true), np.array(y_pred), query_group)\n",
    "\n",
    "#     top5_indices = np.argsort(y_pred)[-5:]\n",
    "#     if np.sum(y_true[top5_indices] == 1) == 0:\n",
    "#         grad[top5_indices] += 1.0 / len(top5_indices)\n",
    "#     return grad, hess\n",
    "\n",
    "\n",
    "# # サンプリング\n",
    "# high_weight_value = 2.0\n",
    "# sample_weights = np.where(train_data[target] == 1, high_weight_value, 1)\n",
    "\n",
    "# # 再ランキング\n",
    "# def rerank_predictions(y_true, y_pred):\n",
    "#     top5_indices = np.argsort(y_pred)[-5:]\n",
    "#     if np.sum(y_true[top5_indices] == 1) < 3:\n",
    "#         one_indices = np.where(y_true == 1)[0]\n",
    "#         for idx in one_indices:\n",
    "#             if idx not in top5_indices:\n",
    "#                 top5_indices[-1] = idx\n",
    "#                 break\n",
    "#     return top5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98ac1239-ae15-4896-9b3b-f0fb1cec81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(ranking_data):\n",
    "    ranking_data['label'] = 0\n",
    "    ranking_data.loc[ranking_data['kakutei_chakujun'] <= 3, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb900af-ebea-4c34-8896-dbd67b71c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_labels(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0544c591-fb45-4a96-9bc2-383bc352415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. カスタム損失関数\n",
    "PENALTY_TERM = 1\n",
    "\n",
    "def custom_loss(y_pred, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    grad = 2 * (y_pred - y_true)\n",
    "    hess = 2 * np.ones_like(y_true)\n",
    "    \n",
    "    top5_indices = np.argsort(y_pred)[-5:]\n",
    "    if np.sum(y_true[top5_indices] == 1) == 0:\n",
    "        grad[top5_indices] += PENALTY_TERM\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7494d0-f5b1-4b7b-8a8a-d51cc6b0da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 再ランキング戦略\n",
    "def rerank_predictions(y_true, y_pred):\n",
    "    top5_indices = np.argsort(y_pred)[-5:]\n",
    "    if np.sum(y_true[top5_indices] == 1) < 3:\n",
    "        one_indices = np.where(y_true == 1)[0]\n",
    "        for idx in one_indices:\n",
    "            if idx not in top5_indices:\n",
    "                top5_indices[-1] = idx\n",
    "                break\n",
    "    return top5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9351c0e4-4ccf-4287-a6b8-d575f81d8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. サンプリング戦略\n",
    "y_true = merged_df['label'].values\n",
    "sample_weights = np.where(y_true == 1, 2.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd0a210d-4083-45f2-8b25-12b2506b06e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assign labels to the items based on their ranks\n",
    "# def assign_labels(ranking_data):\n",
    "#     ranking_data['label'] = 0\n",
    "#     ranking_data.loc[ranking_data['rank'] <= 3, 'label'] = 1\n",
    "\n",
    "# # Custom loss function with penalty term\n",
    "# def custom_loss(y_pred, train_data):\n",
    "#     y_true = train_data.get_label()\n",
    "#     grad = 2 * (y_pred - y_true)  # Gradient calculation\n",
    "#     hess = 2 * np.ones_like(y_true)  # Hessian calculation\n",
    "    \n",
    "#     # Get top 5 indices\n",
    "#     top5_indices = np.argsort(y_pred)[-5:]\n",
    "    \n",
    "#     # If none of the top 5 predictions are correct, apply penalty\n",
    "#     if np.sum(y_true[top5_indices] == 1) == 0:\n",
    "#         grad[top5_indices] += PENALTY_TERM\n",
    "    \n",
    "#     return grad, hess\n",
    "\n",
    "# HIGH_WEIGHT = 2.0\n",
    "# LOW_WEIGHT = 1.0\n",
    "\n",
    "# y_true = train_data[target].values\n",
    "# sample_weights = np.where(y_true == 1, HIGH_WEIGHT, LOW_WEIGHT)\n",
    "\n",
    "# # custom_loss function\n",
    "# PENALTY_TERM = 1  # adjust this based on your needs\n",
    "\n",
    "# # Re-ranking strategy\n",
    "# def rerank_predictions(y_true, y_pred):\n",
    "#     top5_indices = np.argsort(y_pred)[-5:]\n",
    "#     if np.sum(y_true[top5_indices] == 1) < 3:\n",
    "#         one_indices = np.where(y_true == 1)[0]\n",
    "#         for idx in one_indices:\n",
    "#             if idx not in top5_indices:\n",
    "#                 top5_indices[-1] = idx\n",
    "#                 break\n",
    "#     return top5_indices\n",
    "\n",
    "# # You can then incorporate these functions into your model training and evaluation code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "729dcb43-ba2d-426a-80f8-0e956074a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 02:24:39,042] A new study created in memory with name: no-name-62d48221-a6f9-4bc0-aceb-4748a604ea15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.537555\n",
      "[20]\tvalid_0's ndcg@5: 0.54038\n",
      "[30]\tvalid_0's ndcg@5: 0.541076\n",
      "[40]\tvalid_0's ndcg@5: 0.542151\n",
      "[50]\tvalid_0's ndcg@5: 0.542453\n",
      "[60]\tvalid_0's ndcg@5: 0.542552\n",
      "[70]\tvalid_0's ndcg@5: 0.542513\n",
      "[80]\tvalid_0's ndcg@5: 0.54281\n",
      "[90]\tvalid_0's ndcg@5: 0.543141\n",
      "[100]\tvalid_0's ndcg@5: 0.542876\n",
      "[110]\tvalid_0's ndcg@5: 0.542624\n",
      "[120]\tvalid_0's ndcg@5: 0.543219\n",
      "[130]\tvalid_0's ndcg@5: 0.54316\n",
      "[140]\tvalid_0's ndcg@5: 0.543053\n",
      "[150]\tvalid_0's ndcg@5: 0.543428\n",
      "[160]\tvalid_0's ndcg@5: 0.543686\n",
      "[170]\tvalid_0's ndcg@5: 0.543821\n",
      "[180]\tvalid_0's ndcg@5: 0.544234\n",
      "[190]\tvalid_0's ndcg@5: 0.544249\n",
      "[200]\tvalid_0's ndcg@5: 0.544223\n",
      "[210]\tvalid_0's ndcg@5: 0.544334\n",
      "[220]\tvalid_0's ndcg@5: 0.544468\n",
      "[230]\tvalid_0's ndcg@5: 0.544574\n",
      "[240]\tvalid_0's ndcg@5: 0.544885\n",
      "[250]\tvalid_0's ndcg@5: 0.545251\n",
      "[260]\tvalid_0's ndcg@5: 0.545264\n",
      "[270]\tvalid_0's ndcg@5: 0.545327\n",
      "[280]\tvalid_0's ndcg@5: 0.545796\n",
      "[290]\tvalid_0's ndcg@5: 0.545816\n",
      "[300]\tvalid_0's ndcg@5: 0.545956\n",
      "[310]\tvalid_0's ndcg@5: 0.546086\n",
      "[320]\tvalid_0's ndcg@5: 0.546159\n",
      "[330]\tvalid_0's ndcg@5: 0.546455\n",
      "[340]\tvalid_0's ndcg@5: 0.546425\n",
      "[350]\tvalid_0's ndcg@5: 0.54686\n",
      "[360]\tvalid_0's ndcg@5: 0.546932\n",
      "[370]\tvalid_0's ndcg@5: 0.546955\n",
      "[380]\tvalid_0's ndcg@5: 0.546955\n",
      "[390]\tvalid_0's ndcg@5: 0.546996\n",
      "[400]\tvalid_0's ndcg@5: 0.54718\n",
      "[410]\tvalid_0's ndcg@5: 0.547337\n",
      "[420]\tvalid_0's ndcg@5: 0.547356\n",
      "[430]\tvalid_0's ndcg@5: 0.547469\n",
      "[440]\tvalid_0's ndcg@5: 0.547712\n",
      "[450]\tvalid_0's ndcg@5: 0.547719\n",
      "[460]\tvalid_0's ndcg@5: 0.547812\n",
      "[470]\tvalid_0's ndcg@5: 0.547856\n",
      "[480]\tvalid_0's ndcg@5: 0.547927\n",
      "[490]\tvalid_0's ndcg@5: 0.548146\n",
      "[500]\tvalid_0's ndcg@5: 0.548103\n",
      "[510]\tvalid_0's ndcg@5: 0.54817\n",
      "[520]\tvalid_0's ndcg@5: 0.548283\n",
      "[530]\tvalid_0's ndcg@5: 0.548342\n",
      "[540]\tvalid_0's ndcg@5: 0.548422\n",
      "[550]\tvalid_0's ndcg@5: 0.548528\n",
      "[560]\tvalid_0's ndcg@5: 0.548594\n",
      "[570]\tvalid_0's ndcg@5: 0.548609\n",
      "[580]\tvalid_0's ndcg@5: 0.54852\n",
      "[590]\tvalid_0's ndcg@5: 0.548669\n",
      "[600]\tvalid_0's ndcg@5: 0.548661\n",
      "[610]\tvalid_0's ndcg@5: 0.548723\n",
      "[620]\tvalid_0's ndcg@5: 0.548706\n",
      "[630]\tvalid_0's ndcg@5: 0.548955\n",
      "[640]\tvalid_0's ndcg@5: 0.548931\n",
      "[650]\tvalid_0's ndcg@5: 0.549072\n",
      "[660]\tvalid_0's ndcg@5: 0.549104\n",
      "[670]\tvalid_0's ndcg@5: 0.549286\n",
      "[680]\tvalid_0's ndcg@5: 0.549594\n",
      "[690]\tvalid_0's ndcg@5: 0.549681\n",
      "[700]\tvalid_0's ndcg@5: 0.5498\n",
      "[710]\tvalid_0's ndcg@5: 0.549785\n",
      "[720]\tvalid_0's ndcg@5: 0.549891\n",
      "[730]\tvalid_0's ndcg@5: 0.550067\n",
      "[740]\tvalid_0's ndcg@5: 0.5499\n",
      "[750]\tvalid_0's ndcg@5: 0.549924\n",
      "[760]\tvalid_0's ndcg@5: 0.55003\n",
      "[770]\tvalid_0's ndcg@5: 0.550094\n",
      "[780]\tvalid_0's ndcg@5: 0.550052\n",
      "[790]\tvalid_0's ndcg@5: 0.550208\n",
      "[800]\tvalid_0's ndcg@5: 0.550324\n",
      "[810]\tvalid_0's ndcg@5: 0.550417\n",
      "[820]\tvalid_0's ndcg@5: 0.550492\n",
      "[830]\tvalid_0's ndcg@5: 0.550539\n",
      "[840]\tvalid_0's ndcg@5: 0.5507\n",
      "[850]\tvalid_0's ndcg@5: 0.550823\n",
      "[860]\tvalid_0's ndcg@5: 0.550995\n",
      "[870]\tvalid_0's ndcg@5: 0.55103\n",
      "[880]\tvalid_0's ndcg@5: 0.551211\n",
      "[890]\tvalid_0's ndcg@5: 0.551391\n",
      "[900]\tvalid_0's ndcg@5: 0.551446\n",
      "[910]\tvalid_0's ndcg@5: 0.55155\n",
      "[920]\tvalid_0's ndcg@5: 0.55174\n",
      "[930]\tvalid_0's ndcg@5: 0.551696\n",
      "[940]\tvalid_0's ndcg@5: 0.551741\n",
      "[950]\tvalid_0's ndcg@5: 0.551897\n",
      "[960]\tvalid_0's ndcg@5: 0.55202\n",
      "[970]\tvalid_0's ndcg@5: 0.552023\n",
      "[980]\tvalid_0's ndcg@5: 0.55208\n",
      "[990]\tvalid_0's ndcg@5: 0.552146\n",
      "[1000]\tvalid_0's ndcg@5: 0.552109\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[995]\tvalid_0's ndcg@5: 0.552179\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.540357\n",
      "[20]\tvalid_0's ndcg@5: 0.54215\n",
      "[30]\tvalid_0's ndcg@5: 0.542775\n",
      "[40]\tvalid_0's ndcg@5: 0.543604\n",
      "[50]\tvalid_0's ndcg@5: 0.543722\n",
      "[60]\tvalid_0's ndcg@5: 0.543641\n",
      "[70]\tvalid_0's ndcg@5: 0.543643\n",
      "[80]\tvalid_0's ndcg@5: 0.544257\n",
      "[90]\tvalid_0's ndcg@5: 0.544806\n",
      "[100]\tvalid_0's ndcg@5: 0.544476\n",
      "[110]\tvalid_0's ndcg@5: 0.544297\n",
      "[120]\tvalid_0's ndcg@5: 0.544768\n",
      "[130]\tvalid_0's ndcg@5: 0.544786\n",
      "[140]\tvalid_0's ndcg@5: 0.544818\n",
      "[150]\tvalid_0's ndcg@5: 0.544753\n",
      "[160]\tvalid_0's ndcg@5: 0.544806\n",
      "[170]\tvalid_0's ndcg@5: 0.545171\n",
      "[180]\tvalid_0's ndcg@5: 0.545174\n",
      "[190]\tvalid_0's ndcg@5: 0.545388\n",
      "[200]\tvalid_0's ndcg@5: 0.545521\n",
      "[210]\tvalid_0's ndcg@5: 0.54562\n",
      "[220]\tvalid_0's ndcg@5: 0.545655\n",
      "[230]\tvalid_0's ndcg@5: 0.546092\n",
      "[240]\tvalid_0's ndcg@5: 0.546166\n",
      "[250]\tvalid_0's ndcg@5: 0.546393\n",
      "[260]\tvalid_0's ndcg@5: 0.54661\n",
      "[270]\tvalid_0's ndcg@5: 0.546855\n",
      "[280]\tvalid_0's ndcg@5: 0.547031\n",
      "[290]\tvalid_0's ndcg@5: 0.547202\n",
      "[300]\tvalid_0's ndcg@5: 0.547327\n",
      "[310]\tvalid_0's ndcg@5: 0.547549\n",
      "[320]\tvalid_0's ndcg@5: 0.547831\n",
      "[330]\tvalid_0's ndcg@5: 0.547936\n",
      "[340]\tvalid_0's ndcg@5: 0.548172\n",
      "[350]\tvalid_0's ndcg@5: 0.548321\n",
      "[360]\tvalid_0's ndcg@5: 0.548383\n",
      "[370]\tvalid_0's ndcg@5: 0.548457\n",
      "[380]\tvalid_0's ndcg@5: 0.54834\n",
      "[390]\tvalid_0's ndcg@5: 0.548428\n",
      "[400]\tvalid_0's ndcg@5: 0.548427\n",
      "[410]\tvalid_0's ndcg@5: 0.548538\n",
      "[420]\tvalid_0's ndcg@5: 0.5487\n",
      "[430]\tvalid_0's ndcg@5: 0.548841\n",
      "[440]\tvalid_0's ndcg@5: 0.54897\n",
      "[450]\tvalid_0's ndcg@5: 0.549039\n",
      "[460]\tvalid_0's ndcg@5: 0.548941\n",
      "[470]\tvalid_0's ndcg@5: 0.549025\n",
      "[480]\tvalid_0's ndcg@5: 0.549102\n",
      "[490]\tvalid_0's ndcg@5: 0.549169\n",
      "[500]\tvalid_0's ndcg@5: 0.549235\n",
      "[510]\tvalid_0's ndcg@5: 0.549397\n",
      "[520]\tvalid_0's ndcg@5: 0.549411\n",
      "[530]\tvalid_0's ndcg@5: 0.5495\n",
      "[540]\tvalid_0's ndcg@5: 0.549743\n",
      "[550]\tvalid_0's ndcg@5: 0.549816\n",
      "[560]\tvalid_0's ndcg@5: 0.549819\n",
      "[570]\tvalid_0's ndcg@5: 0.549939\n",
      "[580]\tvalid_0's ndcg@5: 0.550091\n",
      "[590]\tvalid_0's ndcg@5: 0.550097\n",
      "[600]\tvalid_0's ndcg@5: 0.5501\n",
      "[610]\tvalid_0's ndcg@5: 0.550307\n",
      "[620]\tvalid_0's ndcg@5: 0.550281\n",
      "[630]\tvalid_0's ndcg@5: 0.550285\n",
      "[640]\tvalid_0's ndcg@5: 0.55033\n",
      "[650]\tvalid_0's ndcg@5: 0.550542\n",
      "[660]\tvalid_0's ndcg@5: 0.550623\n",
      "[670]\tvalid_0's ndcg@5: 0.550631\n",
      "[680]\tvalid_0's ndcg@5: 0.550731\n",
      "[690]\tvalid_0's ndcg@5: 0.550807\n",
      "[700]\tvalid_0's ndcg@5: 0.55084\n",
      "[710]\tvalid_0's ndcg@5: 0.550938\n",
      "[720]\tvalid_0's ndcg@5: 0.55109\n",
      "[730]\tvalid_0's ndcg@5: 0.551305\n",
      "[740]\tvalid_0's ndcg@5: 0.551319\n",
      "[750]\tvalid_0's ndcg@5: 0.551419\n",
      "[760]\tvalid_0's ndcg@5: 0.551551\n",
      "[770]\tvalid_0's ndcg@5: 0.551608\n",
      "[780]\tvalid_0's ndcg@5: 0.551592\n",
      "[790]\tvalid_0's ndcg@5: 0.551617\n",
      "[800]\tvalid_0's ndcg@5: 0.551763\n",
      "[810]\tvalid_0's ndcg@5: 0.551823\n",
      "[820]\tvalid_0's ndcg@5: 0.551862\n",
      "[830]\tvalid_0's ndcg@5: 0.551808\n",
      "[840]\tvalid_0's ndcg@5: 0.551971\n",
      "[850]\tvalid_0's ndcg@5: 0.552049\n",
      "[860]\tvalid_0's ndcg@5: 0.552226\n",
      "[870]\tvalid_0's ndcg@5: 0.552312\n",
      "[880]\tvalid_0's ndcg@5: 0.552449\n",
      "[890]\tvalid_0's ndcg@5: 0.552671\n",
      "[900]\tvalid_0's ndcg@5: 0.552772\n",
      "[910]\tvalid_0's ndcg@5: 0.55287\n",
      "[920]\tvalid_0's ndcg@5: 0.552972\n",
      "[930]\tvalid_0's ndcg@5: 0.553075\n",
      "[940]\tvalid_0's ndcg@5: 0.553108\n",
      "[950]\tvalid_0's ndcg@5: 0.553128\n",
      "[960]\tvalid_0's ndcg@5: 0.553328\n",
      "[970]\tvalid_0's ndcg@5: 0.553418\n",
      "[980]\tvalid_0's ndcg@5: 0.553648\n",
      "[990]\tvalid_0's ndcg@5: 0.553536\n",
      "[1000]\tvalid_0's ndcg@5: 0.553591\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[981]\tvalid_0's ndcg@5: 0.553657\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.540643\n",
      "[20]\tvalid_0's ndcg@5: 0.543275\n",
      "[30]\tvalid_0's ndcg@5: 0.542835\n",
      "[40]\tvalid_0's ndcg@5: 0.54334\n",
      "[50]\tvalid_0's ndcg@5: 0.54396\n",
      "[60]\tvalid_0's ndcg@5: 0.543709\n",
      "[70]\tvalid_0's ndcg@5: 0.54401\n",
      "[80]\tvalid_0's ndcg@5: 0.544312\n",
      "[90]\tvalid_0's ndcg@5: 0.544628\n",
      "[100]\tvalid_0's ndcg@5: 0.544427\n",
      "[110]\tvalid_0's ndcg@5: 0.544228\n",
      "[120]\tvalid_0's ndcg@5: 0.544723\n",
      "[130]\tvalid_0's ndcg@5: 0.544558\n",
      "[140]\tvalid_0's ndcg@5: 0.544576\n",
      "[150]\tvalid_0's ndcg@5: 0.545162\n",
      "[160]\tvalid_0's ndcg@5: 0.545529\n",
      "[170]\tvalid_0's ndcg@5: 0.545527\n",
      "[180]\tvalid_0's ndcg@5: 0.546117\n",
      "[190]\tvalid_0's ndcg@5: 0.545878\n",
      "[200]\tvalid_0's ndcg@5: 0.546152\n",
      "[210]\tvalid_0's ndcg@5: 0.546018\n",
      "[220]\tvalid_0's ndcg@5: 0.546295\n",
      "[230]\tvalid_0's ndcg@5: 0.546542\n",
      "[240]\tvalid_0's ndcg@5: 0.546891\n",
      "[250]\tvalid_0's ndcg@5: 0.547246\n",
      "[260]\tvalid_0's ndcg@5: 0.547241\n",
      "[270]\tvalid_0's ndcg@5: 0.547595\n",
      "[280]\tvalid_0's ndcg@5: 0.547795\n",
      "[290]\tvalid_0's ndcg@5: 0.547791\n",
      "[300]\tvalid_0's ndcg@5: 0.547995\n",
      "[310]\tvalid_0's ndcg@5: 0.547986\n",
      "[320]\tvalid_0's ndcg@5: 0.548178\n",
      "[330]\tvalid_0's ndcg@5: 0.548373\n",
      "[340]\tvalid_0's ndcg@5: 0.548505\n",
      "[350]\tvalid_0's ndcg@5: 0.548642\n",
      "[360]\tvalid_0's ndcg@5: 0.548618\n",
      "[370]\tvalid_0's ndcg@5: 0.548633\n",
      "[380]\tvalid_0's ndcg@5: 0.5486\n",
      "[390]\tvalid_0's ndcg@5: 0.548858\n",
      "[400]\tvalid_0's ndcg@5: 0.548722\n",
      "[410]\tvalid_0's ndcg@5: 0.548894\n",
      "[420]\tvalid_0's ndcg@5: 0.548991\n",
      "[430]\tvalid_0's ndcg@5: 0.54907\n",
      "[440]\tvalid_0's ndcg@5: 0.549115\n",
      "[450]\tvalid_0's ndcg@5: 0.54915\n",
      "[460]\tvalid_0's ndcg@5: 0.549134\n",
      "[470]\tvalid_0's ndcg@5: 0.549318\n",
      "[480]\tvalid_0's ndcg@5: 0.549351\n",
      "[490]\tvalid_0's ndcg@5: 0.549479\n",
      "[500]\tvalid_0's ndcg@5: 0.549517\n",
      "[510]\tvalid_0's ndcg@5: 0.549666\n",
      "[520]\tvalid_0's ndcg@5: 0.54967\n",
      "[530]\tvalid_0's ndcg@5: 0.549855\n",
      "[540]\tvalid_0's ndcg@5: 0.549961\n",
      "[550]\tvalid_0's ndcg@5: 0.550085\n",
      "[560]\tvalid_0's ndcg@5: 0.550175\n",
      "[570]\tvalid_0's ndcg@5: 0.550274\n",
      "[580]\tvalid_0's ndcg@5: 0.550509\n",
      "[590]\tvalid_0's ndcg@5: 0.550731\n",
      "[600]\tvalid_0's ndcg@5: 0.550719\n",
      "[610]\tvalid_0's ndcg@5: 0.550857\n",
      "[620]\tvalid_0's ndcg@5: 0.550888\n",
      "[630]\tvalid_0's ndcg@5: 0.551002\n",
      "[640]\tvalid_0's ndcg@5: 0.551215\n",
      "[650]\tvalid_0's ndcg@5: 0.551356\n",
      "[660]\tvalid_0's ndcg@5: 0.551575\n",
      "[670]\tvalid_0's ndcg@5: 0.551744\n",
      "[680]\tvalid_0's ndcg@5: 0.55191\n",
      "[690]\tvalid_0's ndcg@5: 0.55195\n",
      "[700]\tvalid_0's ndcg@5: 0.551934\n",
      "[710]\tvalid_0's ndcg@5: 0.552054\n",
      "[720]\tvalid_0's ndcg@5: 0.552178\n",
      "[730]\tvalid_0's ndcg@5: 0.55224\n",
      "[740]\tvalid_0's ndcg@5: 0.5523\n",
      "[750]\tvalid_0's ndcg@5: 0.552449\n",
      "[760]\tvalid_0's ndcg@5: 0.552622\n",
      "[770]\tvalid_0's ndcg@5: 0.552585\n",
      "[780]\tvalid_0's ndcg@5: 0.552817\n",
      "[790]\tvalid_0's ndcg@5: 0.553\n",
      "[800]\tvalid_0's ndcg@5: 0.553064\n",
      "[810]\tvalid_0's ndcg@5: 0.553204\n",
      "[820]\tvalid_0's ndcg@5: 0.553295\n",
      "[830]\tvalid_0's ndcg@5: 0.553277\n",
      "[840]\tvalid_0's ndcg@5: 0.553407\n",
      "[850]\tvalid_0's ndcg@5: 0.55346\n",
      "[860]\tvalid_0's ndcg@5: 0.553514\n",
      "[870]\tvalid_0's ndcg@5: 0.553656\n",
      "[880]\tvalid_0's ndcg@5: 0.553809\n",
      "[890]\tvalid_0's ndcg@5: 0.553824\n",
      "[900]\tvalid_0's ndcg@5: 0.55394\n",
      "[910]\tvalid_0's ndcg@5: 0.554097\n",
      "[920]\tvalid_0's ndcg@5: 0.554216\n",
      "[930]\tvalid_0's ndcg@5: 0.554236\n",
      "[940]\tvalid_0's ndcg@5: 0.554155\n",
      "[950]\tvalid_0's ndcg@5: 0.554257\n",
      "[960]\tvalid_0's ndcg@5: 0.55439\n",
      "[970]\tvalid_0's ndcg@5: 0.554492\n",
      "[980]\tvalid_0's ndcg@5: 0.554604\n",
      "[990]\tvalid_0's ndcg@5: 0.554761\n",
      "[1000]\tvalid_0's ndcg@5: 0.554689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[991]\tvalid_0's ndcg@5: 0.554785\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.540979\n",
      "[20]\tvalid_0's ndcg@5: 0.542653\n",
      "[30]\tvalid_0's ndcg@5: 0.542605\n",
      "[40]\tvalid_0's ndcg@5: 0.54354\n",
      "[50]\tvalid_0's ndcg@5: 0.543586\n",
      "[60]\tvalid_0's ndcg@5: 0.543629\n",
      "[70]\tvalid_0's ndcg@5: 0.543469\n",
      "[80]\tvalid_0's ndcg@5: 0.544012\n",
      "[90]\tvalid_0's ndcg@5: 0.544551\n",
      "[100]\tvalid_0's ndcg@5: 0.544206\n",
      "[110]\tvalid_0's ndcg@5: 0.544161\n",
      "[120]\tvalid_0's ndcg@5: 0.544908\n",
      "[130]\tvalid_0's ndcg@5: 0.544752\n",
      "[140]\tvalid_0's ndcg@5: 0.544952\n",
      "[150]\tvalid_0's ndcg@5: 0.545143\n",
      "[160]\tvalid_0's ndcg@5: 0.545373\n",
      "[170]\tvalid_0's ndcg@5: 0.545571\n",
      "[180]\tvalid_0's ndcg@5: 0.545863\n",
      "[190]\tvalid_0's ndcg@5: 0.545795\n",
      "[200]\tvalid_0's ndcg@5: 0.545942\n",
      "[210]\tvalid_0's ndcg@5: 0.546094\n",
      "[220]\tvalid_0's ndcg@5: 0.545914\n",
      "[230]\tvalid_0's ndcg@5: 0.546254\n",
      "[240]\tvalid_0's ndcg@5: 0.546838\n",
      "[250]\tvalid_0's ndcg@5: 0.547206\n",
      "[260]\tvalid_0's ndcg@5: 0.547163\n",
      "[270]\tvalid_0's ndcg@5: 0.547102\n",
      "[280]\tvalid_0's ndcg@5: 0.547379\n",
      "[290]\tvalid_0's ndcg@5: 0.547523\n",
      "[300]\tvalid_0's ndcg@5: 0.547651\n",
      "[310]\tvalid_0's ndcg@5: 0.547644\n",
      "[320]\tvalid_0's ndcg@5: 0.54759\n",
      "[330]\tvalid_0's ndcg@5: 0.547896\n",
      "[340]\tvalid_0's ndcg@5: 0.548235\n",
      "[350]\tvalid_0's ndcg@5: 0.548388\n",
      "[360]\tvalid_0's ndcg@5: 0.548592\n",
      "[370]\tvalid_0's ndcg@5: 0.548522\n",
      "[380]\tvalid_0's ndcg@5: 0.548589\n",
      "[390]\tvalid_0's ndcg@5: 0.548589\n",
      "[400]\tvalid_0's ndcg@5: 0.548586\n",
      "[410]\tvalid_0's ndcg@5: 0.548656\n",
      "[420]\tvalid_0's ndcg@5: 0.548697\n",
      "[430]\tvalid_0's ndcg@5: 0.548672\n",
      "[440]\tvalid_0's ndcg@5: 0.549053\n",
      "[450]\tvalid_0's ndcg@5: 0.549058\n",
      "[460]\tvalid_0's ndcg@5: 0.549061\n",
      "[470]\tvalid_0's ndcg@5: 0.549234\n",
      "[480]\tvalid_0's ndcg@5: 0.54918\n",
      "[490]\tvalid_0's ndcg@5: 0.549321\n",
      "[500]\tvalid_0's ndcg@5: 0.549238\n",
      "[510]\tvalid_0's ndcg@5: 0.549318\n",
      "[520]\tvalid_0's ndcg@5: 0.549344\n",
      "[530]\tvalid_0's ndcg@5: 0.549572\n",
      "[540]\tvalid_0's ndcg@5: 0.549716\n",
      "[550]\tvalid_0's ndcg@5: 0.549913\n",
      "[560]\tvalid_0's ndcg@5: 0.550073\n",
      "[570]\tvalid_0's ndcg@5: 0.550222\n",
      "[580]\tvalid_0's ndcg@5: 0.550341\n",
      "[590]\tvalid_0's ndcg@5: 0.550526\n",
      "[600]\tvalid_0's ndcg@5: 0.550553\n",
      "[610]\tvalid_0's ndcg@5: 0.550779\n",
      "[620]\tvalid_0's ndcg@5: 0.550843\n",
      "[630]\tvalid_0's ndcg@5: 0.55094\n",
      "[640]\tvalid_0's ndcg@5: 0.550993\n",
      "[650]\tvalid_0's ndcg@5: 0.551181\n",
      "[660]\tvalid_0's ndcg@5: 0.551102\n",
      "[670]\tvalid_0's ndcg@5: 0.551162\n",
      "[680]\tvalid_0's ndcg@5: 0.551165\n",
      "[690]\tvalid_0's ndcg@5: 0.551261\n",
      "[700]\tvalid_0's ndcg@5: 0.551227\n",
      "[710]\tvalid_0's ndcg@5: 0.551501\n",
      "[720]\tvalid_0's ndcg@5: 0.55166\n",
      "[730]\tvalid_0's ndcg@5: 0.551742\n",
      "[740]\tvalid_0's ndcg@5: 0.551902\n",
      "[750]\tvalid_0's ndcg@5: 0.552076\n",
      "[760]\tvalid_0's ndcg@5: 0.552217\n",
      "[770]\tvalid_0's ndcg@5: 0.552402\n",
      "[780]\tvalid_0's ndcg@5: 0.552493\n",
      "[790]\tvalid_0's ndcg@5: 0.552505\n",
      "[800]\tvalid_0's ndcg@5: 0.552501\n",
      "[810]\tvalid_0's ndcg@5: 0.552661\n",
      "[820]\tvalid_0's ndcg@5: 0.552834\n",
      "[830]\tvalid_0's ndcg@5: 0.552861\n",
      "[840]\tvalid_0's ndcg@5: 0.552905\n",
      "[850]\tvalid_0's ndcg@5: 0.553005\n",
      "[860]\tvalid_0's ndcg@5: 0.553121\n",
      "[870]\tvalid_0's ndcg@5: 0.553173\n",
      "[880]\tvalid_0's ndcg@5: 0.553136\n",
      "[890]\tvalid_0's ndcg@5: 0.55324\n",
      "[900]\tvalid_0's ndcg@5: 0.55352\n",
      "[910]\tvalid_0's ndcg@5: 0.553777\n",
      "[920]\tvalid_0's ndcg@5: 0.553683\n",
      "[930]\tvalid_0's ndcg@5: 0.553742\n",
      "[940]\tvalid_0's ndcg@5: 0.55377\n",
      "[950]\tvalid_0's ndcg@5: 0.553862\n",
      "[960]\tvalid_0's ndcg@5: 0.553954\n",
      "[970]\tvalid_0's ndcg@5: 0.554019\n",
      "[980]\tvalid_0's ndcg@5: 0.554133\n",
      "[990]\tvalid_0's ndcg@5: 0.554358\n",
      "[1000]\tvalid_0's ndcg@5: 0.554354\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's ndcg@5: 0.554458\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.541182\n",
      "[20]\tvalid_0's ndcg@5: 0.543362\n",
      "[30]\tvalid_0's ndcg@5: 0.543542\n",
      "[40]\tvalid_0's ndcg@5: 0.544207\n",
      "[50]\tvalid_0's ndcg@5: 0.543741\n",
      "[60]\tvalid_0's ndcg@5: 0.543867\n",
      "[70]\tvalid_0's ndcg@5: 0.54423\n",
      "[80]\tvalid_0's ndcg@5: 0.5446\n",
      "[90]\tvalid_0's ndcg@5: 0.545151\n",
      "[100]\tvalid_0's ndcg@5: 0.544404\n",
      "[110]\tvalid_0's ndcg@5: 0.543804\n",
      "[120]\tvalid_0's ndcg@5: 0.54458\n",
      "[130]\tvalid_0's ndcg@5: 0.545006\n",
      "[140]\tvalid_0's ndcg@5: 0.544908\n",
      "[150]\tvalid_0's ndcg@5: 0.545153\n",
      "[160]\tvalid_0's ndcg@5: 0.545337\n",
      "[170]\tvalid_0's ndcg@5: 0.545664\n",
      "[180]\tvalid_0's ndcg@5: 0.546015\n",
      "[190]\tvalid_0's ndcg@5: 0.545931\n",
      "[200]\tvalid_0's ndcg@5: 0.546093\n",
      "[210]\tvalid_0's ndcg@5: 0.546217\n",
      "[220]\tvalid_0's ndcg@5: 0.546467\n",
      "[230]\tvalid_0's ndcg@5: 0.546644\n",
      "[240]\tvalid_0's ndcg@5: 0.546522\n",
      "[250]\tvalid_0's ndcg@5: 0.547018\n",
      "[260]\tvalid_0's ndcg@5: 0.546989\n",
      "[270]\tvalid_0's ndcg@5: 0.547062\n",
      "[280]\tvalid_0's ndcg@5: 0.547393\n",
      "[290]\tvalid_0's ndcg@5: 0.547506\n",
      "[300]\tvalid_0's ndcg@5: 0.54765\n",
      "[310]\tvalid_0's ndcg@5: 0.547764\n",
      "[320]\tvalid_0's ndcg@5: 0.547819\n",
      "[330]\tvalid_0's ndcg@5: 0.548082\n",
      "[340]\tvalid_0's ndcg@5: 0.548325\n",
      "[350]\tvalid_0's ndcg@5: 0.548471\n",
      "[360]\tvalid_0's ndcg@5: 0.548652\n",
      "[370]\tvalid_0's ndcg@5: 0.54889\n",
      "[380]\tvalid_0's ndcg@5: 0.548963\n",
      "[390]\tvalid_0's ndcg@5: 0.548865\n",
      "[400]\tvalid_0's ndcg@5: 0.548835\n",
      "[410]\tvalid_0's ndcg@5: 0.549277\n",
      "[420]\tvalid_0's ndcg@5: 0.549278\n",
      "[430]\tvalid_0's ndcg@5: 0.549229\n",
      "[440]\tvalid_0's ndcg@5: 0.549284\n",
      "[450]\tvalid_0's ndcg@5: 0.54936\n",
      "[460]\tvalid_0's ndcg@5: 0.549169\n",
      "[470]\tvalid_0's ndcg@5: 0.549126\n",
      "[480]\tvalid_0's ndcg@5: 0.54945\n",
      "[490]\tvalid_0's ndcg@5: 0.549503\n",
      "[500]\tvalid_0's ndcg@5: 0.549541\n",
      "[510]\tvalid_0's ndcg@5: 0.549682\n",
      "[520]\tvalid_0's ndcg@5: 0.549825\n",
      "[530]\tvalid_0's ndcg@5: 0.549965\n",
      "[540]\tvalid_0's ndcg@5: 0.549913\n",
      "[550]\tvalid_0's ndcg@5: 0.550099\n",
      "[560]\tvalid_0's ndcg@5: 0.550185\n",
      "[570]\tvalid_0's ndcg@5: 0.550414\n",
      "[580]\tvalid_0's ndcg@5: 0.550503\n",
      "[590]\tvalid_0's ndcg@5: 0.55062\n",
      "[600]\tvalid_0's ndcg@5: 0.550648\n",
      "[610]\tvalid_0's ndcg@5: 0.550646\n",
      "[620]\tvalid_0's ndcg@5: 0.550656\n",
      "[630]\tvalid_0's ndcg@5: 0.550785\n",
      "[640]\tvalid_0's ndcg@5: 0.55085\n",
      "[650]\tvalid_0's ndcg@5: 0.550949\n",
      "[660]\tvalid_0's ndcg@5: 0.551151\n",
      "[670]\tvalid_0's ndcg@5: 0.551295\n",
      "[680]\tvalid_0's ndcg@5: 0.551472\n",
      "[690]\tvalid_0's ndcg@5: 0.551421\n",
      "[700]\tvalid_0's ndcg@5: 0.551547\n",
      "[710]\tvalid_0's ndcg@5: 0.551696\n",
      "[720]\tvalid_0's ndcg@5: 0.551917\n",
      "[730]\tvalid_0's ndcg@5: 0.551839\n",
      "[740]\tvalid_0's ndcg@5: 0.551858\n",
      "[750]\tvalid_0's ndcg@5: 0.552013\n",
      "[760]\tvalid_0's ndcg@5: 0.552047\n",
      "[770]\tvalid_0's ndcg@5: 0.552051\n",
      "[780]\tvalid_0's ndcg@5: 0.552145\n",
      "[790]\tvalid_0's ndcg@5: 0.552277\n",
      "[800]\tvalid_0's ndcg@5: 0.552408\n",
      "[810]\tvalid_0's ndcg@5: 0.552511\n",
      "[820]\tvalid_0's ndcg@5: 0.552657\n",
      "[830]\tvalid_0's ndcg@5: 0.552662\n",
      "[840]\tvalid_0's ndcg@5: 0.552851\n",
      "[850]\tvalid_0's ndcg@5: 0.552998\n",
      "[860]\tvalid_0's ndcg@5: 0.553102\n",
      "[870]\tvalid_0's ndcg@5: 0.553344\n",
      "[880]\tvalid_0's ndcg@5: 0.553406\n",
      "[890]\tvalid_0's ndcg@5: 0.553473\n",
      "[900]\tvalid_0's ndcg@5: 0.553561\n",
      "[910]\tvalid_0's ndcg@5: 0.553648\n",
      "[920]\tvalid_0's ndcg@5: 0.553742\n",
      "[930]\tvalid_0's ndcg@5: 0.553827\n",
      "[940]\tvalid_0's ndcg@5: 0.553803\n",
      "[950]\tvalid_0's ndcg@5: 0.553795\n",
      "[960]\tvalid_0's ndcg@5: 0.553932\n",
      "[970]\tvalid_0's ndcg@5: 0.554016\n",
      "[980]\tvalid_0's ndcg@5: 0.554072\n",
      "[990]\tvalid_0's ndcg@5: 0.554332\n",
      "[1000]\tvalid_0's ndcg@5: 0.554283\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[997]\tvalid_0's ndcg@5: 0.554365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 03:06:34,098] Trial 0 finished with value: 0.34403863073148233 and parameters: {'num_leaves': 251, 'learning_rate': 0.0014813656935166576, 'feature_fraction': 0.7118258827082823, 'bagging_freq': 10, 'verbose': 0, 'lambda_l1': 1.908136586499084e-05, 'lambda_l2': 0.04482151958411491}. Best is trial 0 with value: 0.34403863073148233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.542544\n",
      "[20]\tvalid_0's ndcg@5: 0.545507\n",
      "[30]\tvalid_0's ndcg@5: 0.544796\n",
      "[40]\tvalid_0's ndcg@5: 0.544337\n",
      "[50]\tvalid_0's ndcg@5: 0.545435\n",
      "[60]\tvalid_0's ndcg@5: 0.545276\n",
      "[70]\tvalid_0's ndcg@5: 0.544817\n",
      "[80]\tvalid_0's ndcg@5: 0.545185\n",
      "[90]\tvalid_0's ndcg@5: 0.545532\n",
      "[100]\tvalid_0's ndcg@5: 0.545418\n",
      "[110]\tvalid_0's ndcg@5: 0.545617\n",
      "[120]\tvalid_0's ndcg@5: 0.545733\n",
      "[130]\tvalid_0's ndcg@5: 0.545672\n",
      "[140]\tvalid_0's ndcg@5: 0.545939\n",
      "[150]\tvalid_0's ndcg@5: 0.545742\n",
      "[160]\tvalid_0's ndcg@5: 0.545918\n",
      "[170]\tvalid_0's ndcg@5: 0.545926\n",
      "[180]\tvalid_0's ndcg@5: 0.546083\n",
      "[190]\tvalid_0's ndcg@5: 0.546142\n",
      "[200]\tvalid_0's ndcg@5: 0.546147\n",
      "[210]\tvalid_0's ndcg@5: 0.546087\n",
      "[220]\tvalid_0's ndcg@5: 0.546178\n",
      "[230]\tvalid_0's ndcg@5: 0.546327\n",
      "[240]\tvalid_0's ndcg@5: 0.546514\n",
      "[250]\tvalid_0's ndcg@5: 0.546394\n",
      "[260]\tvalid_0's ndcg@5: 0.54617\n",
      "[270]\tvalid_0's ndcg@5: 0.546465\n",
      "[280]\tvalid_0's ndcg@5: 0.546218\n",
      "[290]\tvalid_0's ndcg@5: 0.546411\n",
      "[300]\tvalid_0's ndcg@5: 0.54614\n",
      "[310]\tvalid_0's ndcg@5: 0.546056\n",
      "[320]\tvalid_0's ndcg@5: 0.546125\n",
      "[330]\tvalid_0's ndcg@5: 0.546142\n",
      "[340]\tvalid_0's ndcg@5: 0.546221\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's ndcg@5: 0.546514\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.543291\n",
      "[20]\tvalid_0's ndcg@5: 0.545192\n",
      "[30]\tvalid_0's ndcg@5: 0.544476\n",
      "[40]\tvalid_0's ndcg@5: 0.544894\n",
      "[50]\tvalid_0's ndcg@5: 0.54531\n",
      "[60]\tvalid_0's ndcg@5: 0.545344\n",
      "[70]\tvalid_0's ndcg@5: 0.545651\n",
      "[80]\tvalid_0's ndcg@5: 0.545697\n",
      "[90]\tvalid_0's ndcg@5: 0.545933\n",
      "[100]\tvalid_0's ndcg@5: 0.545872\n",
      "[110]\tvalid_0's ndcg@5: 0.546051\n",
      "[120]\tvalid_0's ndcg@5: 0.546067\n",
      "[130]\tvalid_0's ndcg@5: 0.546463\n",
      "[140]\tvalid_0's ndcg@5: 0.546129\n",
      "[150]\tvalid_0's ndcg@5: 0.546221\n",
      "[160]\tvalid_0's ndcg@5: 0.546449\n",
      "[170]\tvalid_0's ndcg@5: 0.546395\n",
      "[180]\tvalid_0's ndcg@5: 0.546753\n",
      "[190]\tvalid_0's ndcg@5: 0.546751\n",
      "[200]\tvalid_0's ndcg@5: 0.546511\n",
      "[210]\tvalid_0's ndcg@5: 0.546645\n",
      "[220]\tvalid_0's ndcg@5: 0.546681\n",
      "[230]\tvalid_0's ndcg@5: 0.546571\n",
      "[240]\tvalid_0's ndcg@5: 0.546722\n",
      "[250]\tvalid_0's ndcg@5: 0.546782\n",
      "[260]\tvalid_0's ndcg@5: 0.546956\n",
      "[270]\tvalid_0's ndcg@5: 0.546995\n",
      "[280]\tvalid_0's ndcg@5: 0.547157\n",
      "[290]\tvalid_0's ndcg@5: 0.546984\n",
      "[300]\tvalid_0's ndcg@5: 0.547007\n",
      "[310]\tvalid_0's ndcg@5: 0.546704\n",
      "[320]\tvalid_0's ndcg@5: 0.547011\n",
      "[330]\tvalid_0's ndcg@5: 0.547104\n",
      "[340]\tvalid_0's ndcg@5: 0.54722\n",
      "[350]\tvalid_0's ndcg@5: 0.547137\n",
      "[360]\tvalid_0's ndcg@5: 0.546847\n",
      "[370]\tvalid_0's ndcg@5: 0.547016\n",
      "[380]\tvalid_0's ndcg@5: 0.547232\n",
      "[390]\tvalid_0's ndcg@5: 0.547189\n",
      "[400]\tvalid_0's ndcg@5: 0.547183\n",
      "[410]\tvalid_0's ndcg@5: 0.547107\n",
      "[420]\tvalid_0's ndcg@5: 0.547224\n",
      "[430]\tvalid_0's ndcg@5: 0.547253\n",
      "[440]\tvalid_0's ndcg@5: 0.547339\n",
      "[450]\tvalid_0's ndcg@5: 0.547271\n",
      "[460]\tvalid_0's ndcg@5: 0.547479\n",
      "[470]\tvalid_0's ndcg@5: 0.547485\n",
      "[480]\tvalid_0's ndcg@5: 0.547454\n",
      "[490]\tvalid_0's ndcg@5: 0.547411\n",
      "[500]\tvalid_0's ndcg@5: 0.547421\n",
      "[510]\tvalid_0's ndcg@5: 0.547556\n",
      "[520]\tvalid_0's ndcg@5: 0.547498\n",
      "[530]\tvalid_0's ndcg@5: 0.547569\n",
      "[540]\tvalid_0's ndcg@5: 0.54766\n",
      "[550]\tvalid_0's ndcg@5: 0.547693\n",
      "[560]\tvalid_0's ndcg@5: 0.547888\n",
      "[570]\tvalid_0's ndcg@5: 0.54789\n",
      "[580]\tvalid_0's ndcg@5: 0.547844\n",
      "[590]\tvalid_0's ndcg@5: 0.547946\n",
      "[600]\tvalid_0's ndcg@5: 0.547954\n",
      "[610]\tvalid_0's ndcg@5: 0.547867\n",
      "[620]\tvalid_0's ndcg@5: 0.547874\n",
      "[630]\tvalid_0's ndcg@5: 0.547889\n",
      "[640]\tvalid_0's ndcg@5: 0.547981\n",
      "[650]\tvalid_0's ndcg@5: 0.547874\n",
      "[660]\tvalid_0's ndcg@5: 0.547824\n",
      "[670]\tvalid_0's ndcg@5: 0.547937\n",
      "[680]\tvalid_0's ndcg@5: 0.547826\n",
      "[690]\tvalid_0's ndcg@5: 0.547746\n",
      "[700]\tvalid_0's ndcg@5: 0.547832\n",
      "[710]\tvalid_0's ndcg@5: 0.547917\n",
      "[720]\tvalid_0's ndcg@5: 0.547933\n",
      "[730]\tvalid_0's ndcg@5: 0.5479\n",
      "[740]\tvalid_0's ndcg@5: 0.547888\n",
      "[750]\tvalid_0's ndcg@5: 0.547868\n",
      "[760]\tvalid_0's ndcg@5: 0.547816\n",
      "[770]\tvalid_0's ndcg@5: 0.547811\n",
      "[780]\tvalid_0's ndcg@5: 0.547845\n",
      "[790]\tvalid_0's ndcg@5: 0.547982\n",
      "[800]\tvalid_0's ndcg@5: 0.547986\n",
      "Early stopping, best iteration is:\n",
      "[704]\tvalid_0's ndcg@5: 0.548034\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.54365\n",
      "[20]\tvalid_0's ndcg@5: 0.547765\n",
      "[30]\tvalid_0's ndcg@5: 0.546258\n",
      "[40]\tvalid_0's ndcg@5: 0.545948\n",
      "[50]\tvalid_0's ndcg@5: 0.546873\n",
      "[60]\tvalid_0's ndcg@5: 0.546507\n",
      "[70]\tvalid_0's ndcg@5: 0.546423\n",
      "[80]\tvalid_0's ndcg@5: 0.546589\n",
      "[90]\tvalid_0's ndcg@5: 0.54636\n",
      "[100]\tvalid_0's ndcg@5: 0.546584\n",
      "[110]\tvalid_0's ndcg@5: 0.547206\n",
      "[120]\tvalid_0's ndcg@5: 0.547911\n",
      "[130]\tvalid_0's ndcg@5: 0.547973\n",
      "[140]\tvalid_0's ndcg@5: 0.548116\n",
      "[150]\tvalid_0's ndcg@5: 0.547877\n",
      "[160]\tvalid_0's ndcg@5: 0.547644\n",
      "[170]\tvalid_0's ndcg@5: 0.547464\n",
      "[180]\tvalid_0's ndcg@5: 0.547816\n",
      "[190]\tvalid_0's ndcg@5: 0.547812\n",
      "[200]\tvalid_0's ndcg@5: 0.547786\n",
      "[210]\tvalid_0's ndcg@5: 0.548201\n",
      "[220]\tvalid_0's ndcg@5: 0.548392\n",
      "[230]\tvalid_0's ndcg@5: 0.548485\n",
      "[240]\tvalid_0's ndcg@5: 0.548539\n",
      "[250]\tvalid_0's ndcg@5: 0.548514\n",
      "[260]\tvalid_0's ndcg@5: 0.548597\n",
      "[270]\tvalid_0's ndcg@5: 0.548581\n",
      "[280]\tvalid_0's ndcg@5: 0.548299\n",
      "[290]\tvalid_0's ndcg@5: 0.548332\n",
      "[300]\tvalid_0's ndcg@5: 0.548488\n",
      "[310]\tvalid_0's ndcg@5: 0.548465\n",
      "[320]\tvalid_0's ndcg@5: 0.548615\n",
      "[330]\tvalid_0's ndcg@5: 0.548614\n",
      "[340]\tvalid_0's ndcg@5: 0.548502\n",
      "[350]\tvalid_0's ndcg@5: 0.54859\n",
      "[360]\tvalid_0's ndcg@5: 0.548283\n",
      "[370]\tvalid_0's ndcg@5: 0.548407\n",
      "[380]\tvalid_0's ndcg@5: 0.548474\n",
      "[390]\tvalid_0's ndcg@5: 0.548499\n",
      "[400]\tvalid_0's ndcg@5: 0.548426\n",
      "[410]\tvalid_0's ndcg@5: 0.548539\n",
      "[420]\tvalid_0's ndcg@5: 0.548363\n",
      "Early stopping, best iteration is:\n",
      "[325]\tvalid_0's ndcg@5: 0.548783\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.543831\n",
      "[20]\tvalid_0's ndcg@5: 0.546585\n",
      "[30]\tvalid_0's ndcg@5: 0.545327\n",
      "[40]\tvalid_0's ndcg@5: 0.544968\n",
      "[50]\tvalid_0's ndcg@5: 0.54562\n",
      "[60]\tvalid_0's ndcg@5: 0.545296\n",
      "[70]\tvalid_0's ndcg@5: 0.544708\n",
      "[80]\tvalid_0's ndcg@5: 0.545227\n",
      "[90]\tvalid_0's ndcg@5: 0.545597\n",
      "[100]\tvalid_0's ndcg@5: 0.545542\n",
      "[110]\tvalid_0's ndcg@5: 0.545959\n",
      "[120]\tvalid_0's ndcg@5: 0.546606\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's ndcg@5: 0.546903\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.543278\n",
      "[20]\tvalid_0's ndcg@5: 0.546946\n",
      "[30]\tvalid_0's ndcg@5: 0.545654\n",
      "[40]\tvalid_0's ndcg@5: 0.544804\n",
      "[50]\tvalid_0's ndcg@5: 0.546362\n",
      "[60]\tvalid_0's ndcg@5: 0.546346\n",
      "[70]\tvalid_0's ndcg@5: 0.54605\n",
      "[80]\tvalid_0's ndcg@5: 0.546681\n",
      "[90]\tvalid_0's ndcg@5: 0.546088\n",
      "[100]\tvalid_0's ndcg@5: 0.546229\n",
      "[110]\tvalid_0's ndcg@5: 0.546784\n",
      "[120]\tvalid_0's ndcg@5: 0.547359\n",
      "[130]\tvalid_0's ndcg@5: 0.547321\n",
      "[140]\tvalid_0's ndcg@5: 0.547334\n",
      "[150]\tvalid_0's ndcg@5: 0.547215\n",
      "[160]\tvalid_0's ndcg@5: 0.54745\n",
      "[170]\tvalid_0's ndcg@5: 0.547249\n",
      "[180]\tvalid_0's ndcg@5: 0.54734\n",
      "[190]\tvalid_0's ndcg@5: 0.547147\n",
      "[200]\tvalid_0's ndcg@5: 0.547416\n",
      "[210]\tvalid_0's ndcg@5: 0.547416\n",
      "[220]\tvalid_0's ndcg@5: 0.547677\n",
      "[230]\tvalid_0's ndcg@5: 0.547634\n",
      "[240]\tvalid_0's ndcg@5: 0.547706\n",
      "[250]\tvalid_0's ndcg@5: 0.547561\n",
      "[260]\tvalid_0's ndcg@5: 0.547671\n",
      "[270]\tvalid_0's ndcg@5: 0.547826\n",
      "[280]\tvalid_0's ndcg@5: 0.547722\n",
      "[290]\tvalid_0's ndcg@5: 0.54766\n",
      "[300]\tvalid_0's ndcg@5: 0.547674\n",
      "[310]\tvalid_0's ndcg@5: 0.547675\n",
      "[320]\tvalid_0's ndcg@5: 0.547909\n",
      "[330]\tvalid_0's ndcg@5: 0.547993\n",
      "[340]\tvalid_0's ndcg@5: 0.547957\n",
      "[350]\tvalid_0's ndcg@5: 0.548039\n",
      "[360]\tvalid_0's ndcg@5: 0.548156\n",
      "[370]\tvalid_0's ndcg@5: 0.548179\n",
      "[380]\tvalid_0's ndcg@5: 0.548248\n",
      "[390]\tvalid_0's ndcg@5: 0.548116\n",
      "[400]\tvalid_0's ndcg@5: 0.548376\n",
      "[410]\tvalid_0's ndcg@5: 0.54828\n",
      "[420]\tvalid_0's ndcg@5: 0.54839\n",
      "[430]\tvalid_0's ndcg@5: 0.548366\n",
      "[440]\tvalid_0's ndcg@5: 0.548297\n",
      "[450]\tvalid_0's ndcg@5: 0.548248\n",
      "[460]\tvalid_0's ndcg@5: 0.54849\n",
      "[470]\tvalid_0's ndcg@5: 0.548475\n",
      "[480]\tvalid_0's ndcg@5: 0.548493\n",
      "[490]\tvalid_0's ndcg@5: 0.548443\n",
      "[500]\tvalid_0's ndcg@5: 0.548358\n",
      "[510]\tvalid_0's ndcg@5: 0.548342\n",
      "[520]\tvalid_0's ndcg@5: 0.54826\n",
      "[530]\tvalid_0's ndcg@5: 0.548491\n",
      "[540]\tvalid_0's ndcg@5: 0.548617\n",
      "[550]\tvalid_0's ndcg@5: 0.54862\n",
      "[560]\tvalid_0's ndcg@5: 0.548649\n",
      "[570]\tvalid_0's ndcg@5: 0.548815\n",
      "[580]\tvalid_0's ndcg@5: 0.54871\n",
      "[590]\tvalid_0's ndcg@5: 0.548695\n",
      "[600]\tvalid_0's ndcg@5: 0.548824\n",
      "[610]\tvalid_0's ndcg@5: 0.548775\n",
      "[620]\tvalid_0's ndcg@5: 0.548669\n",
      "[630]\tvalid_0's ndcg@5: 0.548691\n",
      "[640]\tvalid_0's ndcg@5: 0.54878\n",
      "[650]\tvalid_0's ndcg@5: 0.548675\n",
      "[660]\tvalid_0's ndcg@5: 0.548869\n",
      "[670]\tvalid_0's ndcg@5: 0.548877\n",
      "[680]\tvalid_0's ndcg@5: 0.548917\n",
      "[690]\tvalid_0's ndcg@5: 0.548993\n",
      "[700]\tvalid_0's ndcg@5: 0.549017\n",
      "[710]\tvalid_0's ndcg@5: 0.548866\n",
      "[720]\tvalid_0's ndcg@5: 0.548958\n",
      "[730]\tvalid_0's ndcg@5: 0.54897\n",
      "[740]\tvalid_0's ndcg@5: 0.549152\n",
      "[750]\tvalid_0's ndcg@5: 0.549169\n",
      "[760]\tvalid_0's ndcg@5: 0.549212\n",
      "[770]\tvalid_0's ndcg@5: 0.549236\n",
      "[780]\tvalid_0's ndcg@5: 0.549334\n",
      "[790]\tvalid_0's ndcg@5: 0.549343\n",
      "[800]\tvalid_0's ndcg@5: 0.549347\n",
      "[810]\tvalid_0's ndcg@5: 0.549354\n",
      "[820]\tvalid_0's ndcg@5: 0.549328\n",
      "[830]\tvalid_0's ndcg@5: 0.549206\n",
      "[840]\tvalid_0's ndcg@5: 0.549217\n",
      "[850]\tvalid_0's ndcg@5: 0.549252\n",
      "[860]\tvalid_0's ndcg@5: 0.549417\n",
      "[870]\tvalid_0's ndcg@5: 0.549373\n",
      "[880]\tvalid_0's ndcg@5: 0.549368\n",
      "[890]\tvalid_0's ndcg@5: 0.54936\n",
      "[900]\tvalid_0's ndcg@5: 0.549435\n",
      "[910]\tvalid_0's ndcg@5: 0.549354\n",
      "[920]\tvalid_0's ndcg@5: 0.549433\n",
      "[930]\tvalid_0's ndcg@5: 0.549438\n",
      "[940]\tvalid_0's ndcg@5: 0.549572\n",
      "[950]\tvalid_0's ndcg@5: 0.549639\n",
      "[960]\tvalid_0's ndcg@5: 0.549778\n",
      "[970]\tvalid_0's ndcg@5: 0.549782\n",
      "[980]\tvalid_0's ndcg@5: 0.549733\n",
      "[990]\tvalid_0's ndcg@5: 0.549669\n",
      "[1000]\tvalid_0's ndcg@5: 0.549702\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[966]\tvalid_0's ndcg@5: 0.549843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 03:17:31,993] Trial 1 finished with value: 0.4231315315949834 and parameters: {'num_leaves': 163, 'learning_rate': 0.00046338301260256146, 'feature_fraction': 0.43408671592940146, 'bagging_freq': 9, 'verbose': 1, 'lambda_l1': 1.117673976938139e-05, 'lambda_l2': 0.00045154417571197547}. Best is trial 1 with value: 0.4231315315949834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.513513\n",
      "[20]\tvalid_0's ndcg@5: 0.516904\n",
      "[30]\tvalid_0's ndcg@5: 0.520526\n",
      "[40]\tvalid_0's ndcg@5: 0.523555\n",
      "[50]\tvalid_0's ndcg@5: 0.526616\n",
      "[60]\tvalid_0's ndcg@5: 0.529468\n",
      "[70]\tvalid_0's ndcg@5: 0.531925\n",
      "[80]\tvalid_0's ndcg@5: 0.533914\n",
      "[90]\tvalid_0's ndcg@5: 0.536413\n",
      "[100]\tvalid_0's ndcg@5: 0.538555\n",
      "[110]\tvalid_0's ndcg@5: 0.540821\n",
      "[120]\tvalid_0's ndcg@5: 0.541677\n",
      "[130]\tvalid_0's ndcg@5: 0.543057\n",
      "[140]\tvalid_0's ndcg@5: 0.544221\n",
      "[150]\tvalid_0's ndcg@5: 0.545033\n",
      "[160]\tvalid_0's ndcg@5: 0.546321\n",
      "[170]\tvalid_0's ndcg@5: 0.547601\n",
      "[180]\tvalid_0's ndcg@5: 0.548158\n",
      "[190]\tvalid_0's ndcg@5: 0.549125\n",
      "[200]\tvalid_0's ndcg@5: 0.550257\n",
      "[210]\tvalid_0's ndcg@5: 0.551056\n",
      "[220]\tvalid_0's ndcg@5: 0.551913\n",
      "[230]\tvalid_0's ndcg@5: 0.552671\n",
      "[240]\tvalid_0's ndcg@5: 0.553463\n",
      "[250]\tvalid_0's ndcg@5: 0.553866\n",
      "[260]\tvalid_0's ndcg@5: 0.554576\n",
      "[270]\tvalid_0's ndcg@5: 0.555193\n",
      "[280]\tvalid_0's ndcg@5: 0.555998\n",
      "[290]\tvalid_0's ndcg@5: 0.556583\n",
      "[300]\tvalid_0's ndcg@5: 0.557326\n",
      "[310]\tvalid_0's ndcg@5: 0.557913\n",
      "[320]\tvalid_0's ndcg@5: 0.55885\n",
      "[330]\tvalid_0's ndcg@5: 0.559296\n",
      "[340]\tvalid_0's ndcg@5: 0.559903\n",
      "[350]\tvalid_0's ndcg@5: 0.560519\n",
      "[360]\tvalid_0's ndcg@5: 0.560959\n",
      "[370]\tvalid_0's ndcg@5: 0.561144\n",
      "[380]\tvalid_0's ndcg@5: 0.561636\n",
      "[390]\tvalid_0's ndcg@5: 0.561872\n",
      "[400]\tvalid_0's ndcg@5: 0.562453\n",
      "[410]\tvalid_0's ndcg@5: 0.562754\n",
      "[420]\tvalid_0's ndcg@5: 0.563277\n",
      "[430]\tvalid_0's ndcg@5: 0.563481\n",
      "[440]\tvalid_0's ndcg@5: 0.563808\n",
      "[450]\tvalid_0's ndcg@5: 0.56407\n",
      "[460]\tvalid_0's ndcg@5: 0.564464\n",
      "[470]\tvalid_0's ndcg@5: 0.564721\n",
      "[480]\tvalid_0's ndcg@5: 0.565004\n",
      "[490]\tvalid_0's ndcg@5: 0.565237\n",
      "[500]\tvalid_0's ndcg@5: 0.565374\n",
      "[510]\tvalid_0's ndcg@5: 0.565684\n",
      "[520]\tvalid_0's ndcg@5: 0.565842\n",
      "[530]\tvalid_0's ndcg@5: 0.565879\n",
      "[540]\tvalid_0's ndcg@5: 0.566234\n",
      "[550]\tvalid_0's ndcg@5: 0.566413\n",
      "[560]\tvalid_0's ndcg@5: 0.566821\n",
      "[570]\tvalid_0's ndcg@5: 0.567043\n",
      "[580]\tvalid_0's ndcg@5: 0.567417\n",
      "[590]\tvalid_0's ndcg@5: 0.567725\n",
      "[600]\tvalid_0's ndcg@5: 0.568011\n",
      "[610]\tvalid_0's ndcg@5: 0.568176\n",
      "[620]\tvalid_0's ndcg@5: 0.56839\n",
      "[630]\tvalid_0's ndcg@5: 0.568534\n",
      "[640]\tvalid_0's ndcg@5: 0.568665\n",
      "[650]\tvalid_0's ndcg@5: 0.568746\n",
      "[660]\tvalid_0's ndcg@5: 0.569021\n",
      "[670]\tvalid_0's ndcg@5: 0.569191\n",
      "[680]\tvalid_0's ndcg@5: 0.56938\n",
      "[690]\tvalid_0's ndcg@5: 0.569436\n",
      "[700]\tvalid_0's ndcg@5: 0.569705\n",
      "[710]\tvalid_0's ndcg@5: 0.570041\n",
      "[720]\tvalid_0's ndcg@5: 0.570176\n",
      "[730]\tvalid_0's ndcg@5: 0.570319\n",
      "[740]\tvalid_0's ndcg@5: 0.570335\n",
      "[750]\tvalid_0's ndcg@5: 0.570688\n",
      "[760]\tvalid_0's ndcg@5: 0.57095\n",
      "[770]\tvalid_0's ndcg@5: 0.570999\n",
      "[780]\tvalid_0's ndcg@5: 0.571091\n",
      "[790]\tvalid_0's ndcg@5: 0.571131\n",
      "[800]\tvalid_0's ndcg@5: 0.571216\n",
      "[810]\tvalid_0's ndcg@5: 0.571646\n",
      "[820]\tvalid_0's ndcg@5: 0.571803\n",
      "[830]\tvalid_0's ndcg@5: 0.572019\n",
      "[840]\tvalid_0's ndcg@5: 0.57207\n",
      "[850]\tvalid_0's ndcg@5: 0.57229\n",
      "[860]\tvalid_0's ndcg@5: 0.572608\n",
      "[870]\tvalid_0's ndcg@5: 0.572723\n",
      "[880]\tvalid_0's ndcg@5: 0.572846\n",
      "[890]\tvalid_0's ndcg@5: 0.572953\n",
      "[900]\tvalid_0's ndcg@5: 0.573191\n",
      "[910]\tvalid_0's ndcg@5: 0.573389\n",
      "[920]\tvalid_0's ndcg@5: 0.573596\n",
      "[930]\tvalid_0's ndcg@5: 0.573785\n",
      "[940]\tvalid_0's ndcg@5: 0.574023\n",
      "[950]\tvalid_0's ndcg@5: 0.574114\n",
      "[960]\tvalid_0's ndcg@5: 0.574431\n",
      "[970]\tvalid_0's ndcg@5: 0.574625\n",
      "[980]\tvalid_0's ndcg@5: 0.574817\n",
      "[990]\tvalid_0's ndcg@5: 0.575008\n",
      "[1000]\tvalid_0's ndcg@5: 0.57531\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's ndcg@5: 0.57531\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.519076\n",
      "[20]\tvalid_0's ndcg@5: 0.521985\n",
      "[30]\tvalid_0's ndcg@5: 0.524309\n",
      "[40]\tvalid_0's ndcg@5: 0.527433\n",
      "[50]\tvalid_0's ndcg@5: 0.530614\n",
      "[60]\tvalid_0's ndcg@5: 0.533346\n",
      "[70]\tvalid_0's ndcg@5: 0.535401\n",
      "[80]\tvalid_0's ndcg@5: 0.537663\n",
      "[90]\tvalid_0's ndcg@5: 0.539796\n",
      "[100]\tvalid_0's ndcg@5: 0.541281\n",
      "[110]\tvalid_0's ndcg@5: 0.542956\n",
      "[120]\tvalid_0's ndcg@5: 0.544054\n",
      "[130]\tvalid_0's ndcg@5: 0.545671\n",
      "[140]\tvalid_0's ndcg@5: 0.546604\n",
      "[150]\tvalid_0's ndcg@5: 0.547804\n",
      "[160]\tvalid_0's ndcg@5: 0.548964\n",
      "[170]\tvalid_0's ndcg@5: 0.549524\n",
      "[180]\tvalid_0's ndcg@5: 0.550765\n",
      "[190]\tvalid_0's ndcg@5: 0.551642\n",
      "[200]\tvalid_0's ndcg@5: 0.552487\n",
      "[210]\tvalid_0's ndcg@5: 0.553058\n",
      "[220]\tvalid_0's ndcg@5: 0.553951\n",
      "[230]\tvalid_0's ndcg@5: 0.554873\n",
      "[240]\tvalid_0's ndcg@5: 0.555947\n",
      "[250]\tvalid_0's ndcg@5: 0.556531\n",
      "[260]\tvalid_0's ndcg@5: 0.557324\n",
      "[270]\tvalid_0's ndcg@5: 0.557808\n",
      "[280]\tvalid_0's ndcg@5: 0.558384\n",
      "[290]\tvalid_0's ndcg@5: 0.559105\n",
      "[300]\tvalid_0's ndcg@5: 0.559838\n",
      "[310]\tvalid_0's ndcg@5: 0.560434\n",
      "[320]\tvalid_0's ndcg@5: 0.560868\n",
      "[330]\tvalid_0's ndcg@5: 0.561525\n",
      "[340]\tvalid_0's ndcg@5: 0.561905\n",
      "[350]\tvalid_0's ndcg@5: 0.562448\n",
      "[360]\tvalid_0's ndcg@5: 0.562895\n",
      "[370]\tvalid_0's ndcg@5: 0.563036\n",
      "[380]\tvalid_0's ndcg@5: 0.563532\n",
      "[390]\tvalid_0's ndcg@5: 0.563845\n",
      "[400]\tvalid_0's ndcg@5: 0.564283\n",
      "[410]\tvalid_0's ndcg@5: 0.564611\n",
      "[420]\tvalid_0's ndcg@5: 0.564846\n",
      "[430]\tvalid_0's ndcg@5: 0.565199\n",
      "[440]\tvalid_0's ndcg@5: 0.56553\n",
      "[450]\tvalid_0's ndcg@5: 0.565799\n",
      "[460]\tvalid_0's ndcg@5: 0.565964\n",
      "[470]\tvalid_0's ndcg@5: 0.566377\n",
      "[480]\tvalid_0's ndcg@5: 0.56645\n",
      "[490]\tvalid_0's ndcg@5: 0.566654\n",
      "[500]\tvalid_0's ndcg@5: 0.567042\n",
      "[510]\tvalid_0's ndcg@5: 0.567278\n",
      "[520]\tvalid_0's ndcg@5: 0.567377\n",
      "[530]\tvalid_0's ndcg@5: 0.567516\n",
      "[540]\tvalid_0's ndcg@5: 0.567948\n",
      "[550]\tvalid_0's ndcg@5: 0.568295\n",
      "[560]\tvalid_0's ndcg@5: 0.568558\n",
      "[570]\tvalid_0's ndcg@5: 0.568706\n",
      "[580]\tvalid_0's ndcg@5: 0.569168\n",
      "[590]\tvalid_0's ndcg@5: 0.569339\n",
      "[600]\tvalid_0's ndcg@5: 0.569674\n",
      "[610]\tvalid_0's ndcg@5: 0.569794\n",
      "[620]\tvalid_0's ndcg@5: 0.569938\n",
      "[630]\tvalid_0's ndcg@5: 0.57022\n",
      "[640]\tvalid_0's ndcg@5: 0.570371\n",
      "[650]\tvalid_0's ndcg@5: 0.570893\n",
      "[660]\tvalid_0's ndcg@5: 0.571128\n",
      "[670]\tvalid_0's ndcg@5: 0.571298\n",
      "[680]\tvalid_0's ndcg@5: 0.571647\n",
      "[690]\tvalid_0's ndcg@5: 0.571968\n",
      "[700]\tvalid_0's ndcg@5: 0.572255\n",
      "[710]\tvalid_0's ndcg@5: 0.572392\n",
      "[720]\tvalid_0's ndcg@5: 0.572517\n",
      "[730]\tvalid_0's ndcg@5: 0.572652\n",
      "[740]\tvalid_0's ndcg@5: 0.572842\n",
      "[750]\tvalid_0's ndcg@5: 0.573074\n",
      "[760]\tvalid_0's ndcg@5: 0.573386\n",
      "[770]\tvalid_0's ndcg@5: 0.573397\n",
      "[780]\tvalid_0's ndcg@5: 0.573595\n",
      "[790]\tvalid_0's ndcg@5: 0.573709\n",
      "[800]\tvalid_0's ndcg@5: 0.573968\n",
      "[810]\tvalid_0's ndcg@5: 0.574188\n",
      "[820]\tvalid_0's ndcg@5: 0.574434\n",
      "[830]\tvalid_0's ndcg@5: 0.574646\n",
      "[840]\tvalid_0's ndcg@5: 0.574745\n",
      "[850]\tvalid_0's ndcg@5: 0.57495\n",
      "[860]\tvalid_0's ndcg@5: 0.575279\n",
      "[870]\tvalid_0's ndcg@5: 0.575465\n",
      "[880]\tvalid_0's ndcg@5: 0.575522\n",
      "[890]\tvalid_0's ndcg@5: 0.575685\n",
      "[900]\tvalid_0's ndcg@5: 0.576047\n",
      "[910]\tvalid_0's ndcg@5: 0.576329\n",
      "[920]\tvalid_0's ndcg@5: 0.57664\n",
      "[930]\tvalid_0's ndcg@5: 0.576856\n",
      "[940]\tvalid_0's ndcg@5: 0.577124\n",
      "[950]\tvalid_0's ndcg@5: 0.577236\n",
      "[960]\tvalid_0's ndcg@5: 0.577332\n",
      "[970]\tvalid_0's ndcg@5: 0.5775\n",
      "[980]\tvalid_0's ndcg@5: 0.577777\n",
      "[990]\tvalid_0's ndcg@5: 0.577886\n",
      "[1000]\tvalid_0's ndcg@5: 0.578142\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's ndcg@5: 0.578142\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518625\n",
      "[20]\tvalid_0's ndcg@5: 0.522364\n",
      "[30]\tvalid_0's ndcg@5: 0.525576\n",
      "[40]\tvalid_0's ndcg@5: 0.528074\n",
      "[50]\tvalid_0's ndcg@5: 0.531187\n",
      "[60]\tvalid_0's ndcg@5: 0.53297\n",
      "[70]\tvalid_0's ndcg@5: 0.535481\n",
      "[80]\tvalid_0's ndcg@5: 0.537521\n",
      "[90]\tvalid_0's ndcg@5: 0.539679\n",
      "[100]\tvalid_0's ndcg@5: 0.540958\n",
      "[110]\tvalid_0's ndcg@5: 0.54237\n",
      "[120]\tvalid_0's ndcg@5: 0.543775\n",
      "[130]\tvalid_0's ndcg@5: 0.545052\n",
      "[140]\tvalid_0's ndcg@5: 0.546188\n",
      "[150]\tvalid_0's ndcg@5: 0.547715\n",
      "[160]\tvalid_0's ndcg@5: 0.549102\n",
      "[170]\tvalid_0's ndcg@5: 0.550123\n",
      "[180]\tvalid_0's ndcg@5: 0.550997\n",
      "[190]\tvalid_0's ndcg@5: 0.551685\n",
      "[200]\tvalid_0's ndcg@5: 0.552653\n",
      "[210]\tvalid_0's ndcg@5: 0.553646\n",
      "[220]\tvalid_0's ndcg@5: 0.554418\n",
      "[230]\tvalid_0's ndcg@5: 0.555512\n",
      "[240]\tvalid_0's ndcg@5: 0.556397\n",
      "[250]\tvalid_0's ndcg@5: 0.557\n",
      "[260]\tvalid_0's ndcg@5: 0.557823\n",
      "[270]\tvalid_0's ndcg@5: 0.558626\n",
      "[280]\tvalid_0's ndcg@5: 0.559338\n",
      "[290]\tvalid_0's ndcg@5: 0.559816\n",
      "[300]\tvalid_0's ndcg@5: 0.560257\n",
      "[310]\tvalid_0's ndcg@5: 0.560626\n",
      "[320]\tvalid_0's ndcg@5: 0.561355\n",
      "[330]\tvalid_0's ndcg@5: 0.561929\n",
      "[340]\tvalid_0's ndcg@5: 0.562426\n",
      "[350]\tvalid_0's ndcg@5: 0.563016\n",
      "[360]\tvalid_0's ndcg@5: 0.563209\n",
      "[370]\tvalid_0's ndcg@5: 0.563682\n",
      "[380]\tvalid_0's ndcg@5: 0.563743\n",
      "[390]\tvalid_0's ndcg@5: 0.564457\n",
      "[400]\tvalid_0's ndcg@5: 0.564739\n",
      "[410]\tvalid_0's ndcg@5: 0.565279\n",
      "[420]\tvalid_0's ndcg@5: 0.56568\n",
      "[430]\tvalid_0's ndcg@5: 0.565721\n",
      "[440]\tvalid_0's ndcg@5: 0.5659\n",
      "[450]\tvalid_0's ndcg@5: 0.566307\n",
      "[460]\tvalid_0's ndcg@5: 0.566438\n",
      "[470]\tvalid_0's ndcg@5: 0.566813\n",
      "[480]\tvalid_0's ndcg@5: 0.567157\n",
      "[490]\tvalid_0's ndcg@5: 0.567585\n",
      "[500]\tvalid_0's ndcg@5: 0.567907\n",
      "[510]\tvalid_0's ndcg@5: 0.568274\n",
      "[520]\tvalid_0's ndcg@5: 0.568539\n",
      "[530]\tvalid_0's ndcg@5: 0.56888\n",
      "[540]\tvalid_0's ndcg@5: 0.569279\n",
      "[550]\tvalid_0's ndcg@5: 0.56955\n",
      "[560]\tvalid_0's ndcg@5: 0.569982\n",
      "[570]\tvalid_0's ndcg@5: 0.570149\n",
      "[580]\tvalid_0's ndcg@5: 0.570414\n",
      "[590]\tvalid_0's ndcg@5: 0.570633\n",
      "[600]\tvalid_0's ndcg@5: 0.570873\n",
      "[610]\tvalid_0's ndcg@5: 0.571295\n",
      "[620]\tvalid_0's ndcg@5: 0.571553\n",
      "[630]\tvalid_0's ndcg@5: 0.571722\n",
      "[640]\tvalid_0's ndcg@5: 0.572079\n",
      "[650]\tvalid_0's ndcg@5: 0.572211\n",
      "[660]\tvalid_0's ndcg@5: 0.572405\n",
      "[670]\tvalid_0's ndcg@5: 0.572748\n",
      "[680]\tvalid_0's ndcg@5: 0.573023\n",
      "[690]\tvalid_0's ndcg@5: 0.573298\n",
      "[700]\tvalid_0's ndcg@5: 0.573664\n",
      "[710]\tvalid_0's ndcg@5: 0.573783\n",
      "[720]\tvalid_0's ndcg@5: 0.573975\n",
      "[730]\tvalid_0's ndcg@5: 0.574282\n",
      "[740]\tvalid_0's ndcg@5: 0.574334\n",
      "[750]\tvalid_0's ndcg@5: 0.574277\n",
      "[760]\tvalid_0's ndcg@5: 0.574541\n",
      "[770]\tvalid_0's ndcg@5: 0.574844\n",
      "[780]\tvalid_0's ndcg@5: 0.575025\n",
      "[790]\tvalid_0's ndcg@5: 0.575366\n",
      "[800]\tvalid_0's ndcg@5: 0.575372\n",
      "[810]\tvalid_0's ndcg@5: 0.575544\n",
      "[820]\tvalid_0's ndcg@5: 0.575737\n",
      "[830]\tvalid_0's ndcg@5: 0.575888\n",
      "[840]\tvalid_0's ndcg@5: 0.576084\n",
      "[850]\tvalid_0's ndcg@5: 0.576133\n",
      "[860]\tvalid_0's ndcg@5: 0.576295\n",
      "[870]\tvalid_0's ndcg@5: 0.576608\n",
      "[880]\tvalid_0's ndcg@5: 0.576581\n",
      "[890]\tvalid_0's ndcg@5: 0.576643\n",
      "[900]\tvalid_0's ndcg@5: 0.576848\n",
      "[910]\tvalid_0's ndcg@5: 0.577003\n",
      "[920]\tvalid_0's ndcg@5: 0.577256\n",
      "[930]\tvalid_0's ndcg@5: 0.577695\n",
      "[940]\tvalid_0's ndcg@5: 0.57776\n",
      "[950]\tvalid_0's ndcg@5: 0.577886\n",
      "[960]\tvalid_0's ndcg@5: 0.578083\n",
      "[970]\tvalid_0's ndcg@5: 0.57833\n",
      "[980]\tvalid_0's ndcg@5: 0.57841\n",
      "[990]\tvalid_0's ndcg@5: 0.57842\n",
      "[1000]\tvalid_0's ndcg@5: 0.578536\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's ndcg@5: 0.578536\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518013\n",
      "[20]\tvalid_0's ndcg@5: 0.523133\n",
      "[30]\tvalid_0's ndcg@5: 0.525467\n",
      "[40]\tvalid_0's ndcg@5: 0.527575\n",
      "[50]\tvalid_0's ndcg@5: 0.530502\n",
      "[60]\tvalid_0's ndcg@5: 0.533178\n",
      "[70]\tvalid_0's ndcg@5: 0.535399\n",
      "[80]\tvalid_0's ndcg@5: 0.537279\n",
      "[90]\tvalid_0's ndcg@5: 0.539559\n",
      "[100]\tvalid_0's ndcg@5: 0.54122\n",
      "[110]\tvalid_0's ndcg@5: 0.543565\n",
      "[120]\tvalid_0's ndcg@5: 0.544989\n",
      "[130]\tvalid_0's ndcg@5: 0.546577\n",
      "[140]\tvalid_0's ndcg@5: 0.547871\n",
      "[150]\tvalid_0's ndcg@5: 0.549246\n",
      "[160]\tvalid_0's ndcg@5: 0.550085\n",
      "[170]\tvalid_0's ndcg@5: 0.550996\n",
      "[180]\tvalid_0's ndcg@5: 0.551765\n",
      "[190]\tvalid_0's ndcg@5: 0.552765\n",
      "[200]\tvalid_0's ndcg@5: 0.553451\n",
      "[210]\tvalid_0's ndcg@5: 0.554271\n",
      "[220]\tvalid_0's ndcg@5: 0.55522\n",
      "[230]\tvalid_0's ndcg@5: 0.556275\n",
      "[240]\tvalid_0's ndcg@5: 0.556889\n",
      "[250]\tvalid_0's ndcg@5: 0.557462\n",
      "[260]\tvalid_0's ndcg@5: 0.558093\n",
      "[270]\tvalid_0's ndcg@5: 0.558556\n",
      "[280]\tvalid_0's ndcg@5: 0.55938\n",
      "[290]\tvalid_0's ndcg@5: 0.559849\n",
      "[300]\tvalid_0's ndcg@5: 0.560121\n",
      "[310]\tvalid_0's ndcg@5: 0.560631\n",
      "[320]\tvalid_0's ndcg@5: 0.561176\n",
      "[330]\tvalid_0's ndcg@5: 0.561578\n",
      "[340]\tvalid_0's ndcg@5: 0.562002\n",
      "[350]\tvalid_0's ndcg@5: 0.562494\n",
      "[360]\tvalid_0's ndcg@5: 0.563028\n",
      "[370]\tvalid_0's ndcg@5: 0.563346\n",
      "[380]\tvalid_0's ndcg@5: 0.564019\n",
      "[390]\tvalid_0's ndcg@5: 0.564406\n",
      "[400]\tvalid_0's ndcg@5: 0.564892\n",
      "[410]\tvalid_0's ndcg@5: 0.565096\n",
      "[420]\tvalid_0's ndcg@5: 0.565418\n",
      "[430]\tvalid_0's ndcg@5: 0.565917\n",
      "[440]\tvalid_0's ndcg@5: 0.566189\n",
      "[450]\tvalid_0's ndcg@5: 0.566614\n",
      "[460]\tvalid_0's ndcg@5: 0.56688\n",
      "[470]\tvalid_0's ndcg@5: 0.567177\n",
      "[480]\tvalid_0's ndcg@5: 0.567422\n",
      "[490]\tvalid_0's ndcg@5: 0.567745\n",
      "[500]\tvalid_0's ndcg@5: 0.56833\n",
      "[510]\tvalid_0's ndcg@5: 0.568582\n",
      "[520]\tvalid_0's ndcg@5: 0.568893\n",
      "[530]\tvalid_0's ndcg@5: 0.568916\n",
      "[540]\tvalid_0's ndcg@5: 0.569203\n",
      "[550]\tvalid_0's ndcg@5: 0.569478\n",
      "[560]\tvalid_0's ndcg@5: 0.569724\n",
      "[570]\tvalid_0's ndcg@5: 0.569815\n",
      "[580]\tvalid_0's ndcg@5: 0.569984\n",
      "[590]\tvalid_0's ndcg@5: 0.570169\n",
      "[600]\tvalid_0's ndcg@5: 0.570518\n",
      "[610]\tvalid_0's ndcg@5: 0.570798\n",
      "[620]\tvalid_0's ndcg@5: 0.570856\n",
      "[630]\tvalid_0's ndcg@5: 0.571205\n",
      "[640]\tvalid_0's ndcg@5: 0.57138\n",
      "[650]\tvalid_0's ndcg@5: 0.571474\n",
      "[660]\tvalid_0's ndcg@5: 0.57174\n",
      "[670]\tvalid_0's ndcg@5: 0.572103\n",
      "[680]\tvalid_0's ndcg@5: 0.572368\n",
      "[690]\tvalid_0's ndcg@5: 0.572573\n",
      "[700]\tvalid_0's ndcg@5: 0.57286\n",
      "[710]\tvalid_0's ndcg@5: 0.572895\n",
      "[720]\tvalid_0's ndcg@5: 0.573059\n",
      "[730]\tvalid_0's ndcg@5: 0.573318\n",
      "[740]\tvalid_0's ndcg@5: 0.57362\n",
      "[750]\tvalid_0's ndcg@5: 0.573818\n",
      "[760]\tvalid_0's ndcg@5: 0.574142\n",
      "[770]\tvalid_0's ndcg@5: 0.57416\n",
      "[780]\tvalid_0's ndcg@5: 0.57437\n",
      "[790]\tvalid_0's ndcg@5: 0.574443\n",
      "[800]\tvalid_0's ndcg@5: 0.574782\n",
      "[810]\tvalid_0's ndcg@5: 0.574934\n",
      "[820]\tvalid_0's ndcg@5: 0.57499\n",
      "[830]\tvalid_0's ndcg@5: 0.575156\n",
      "[840]\tvalid_0's ndcg@5: 0.575304\n",
      "[850]\tvalid_0's ndcg@5: 0.575427\n",
      "[860]\tvalid_0's ndcg@5: 0.575674\n",
      "[870]\tvalid_0's ndcg@5: 0.575814\n",
      "[880]\tvalid_0's ndcg@5: 0.576028\n",
      "[890]\tvalid_0's ndcg@5: 0.576073\n",
      "[900]\tvalid_0's ndcg@5: 0.576269\n",
      "[910]\tvalid_0's ndcg@5: 0.57638\n",
      "[920]\tvalid_0's ndcg@5: 0.576631\n",
      "[930]\tvalid_0's ndcg@5: 0.576599\n",
      "[940]\tvalid_0's ndcg@5: 0.576736\n",
      "[950]\tvalid_0's ndcg@5: 0.576894\n",
      "[960]\tvalid_0's ndcg@5: 0.577166\n",
      "[970]\tvalid_0's ndcg@5: 0.577308\n",
      "[980]\tvalid_0's ndcg@5: 0.577357\n",
      "[990]\tvalid_0's ndcg@5: 0.577477\n",
      "[1000]\tvalid_0's ndcg@5: 0.577735\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's ndcg@5: 0.577735\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Info] Loading query weights...\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] eval_at is set with ndcg_at=5, ndcg_eval_at=5 will be ignored. Current value: eval_at=5\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518283\n",
      "[20]\tvalid_0's ndcg@5: 0.52136\n",
      "[30]\tvalid_0's ndcg@5: 0.524334\n",
      "[40]\tvalid_0's ndcg@5: 0.527151\n",
      "[50]\tvalid_0's ndcg@5: 0.529763\n",
      "[60]\tvalid_0's ndcg@5: 0.532379\n",
      "[70]\tvalid_0's ndcg@5: 0.534531\n",
      "[80]\tvalid_0's ndcg@5: 0.536445\n",
      "[90]\tvalid_0's ndcg@5: 0.538109\n",
      "[100]\tvalid_0's ndcg@5: 0.539956\n",
      "[110]\tvalid_0's ndcg@5: 0.541381\n",
      "[120]\tvalid_0's ndcg@5: 0.543023\n",
      "[130]\tvalid_0's ndcg@5: 0.54463\n",
      "[140]\tvalid_0's ndcg@5: 0.545782\n",
      "[150]\tvalid_0's ndcg@5: 0.546941\n",
      "[160]\tvalid_0's ndcg@5: 0.548205\n",
      "[170]\tvalid_0's ndcg@5: 0.54909\n",
      "[180]\tvalid_0's ndcg@5: 0.549501\n",
      "[190]\tvalid_0's ndcg@5: 0.550571\n",
      "[200]\tvalid_0's ndcg@5: 0.551093\n",
      "[210]\tvalid_0's ndcg@5: 0.552138\n",
      "[220]\tvalid_0's ndcg@5: 0.553021\n",
      "[230]\tvalid_0's ndcg@5: 0.553952\n",
      "[240]\tvalid_0's ndcg@5: 0.554802\n",
      "[250]\tvalid_0's ndcg@5: 0.555379\n",
      "[260]\tvalid_0's ndcg@5: 0.556278\n",
      "[270]\tvalid_0's ndcg@5: 0.557434\n",
      "[280]\tvalid_0's ndcg@5: 0.558231\n",
      "[290]\tvalid_0's ndcg@5: 0.558794\n",
      "[300]\tvalid_0's ndcg@5: 0.55906\n",
      "[310]\tvalid_0's ndcg@5: 0.559832\n",
      "[320]\tvalid_0's ndcg@5: 0.560235\n",
      "[330]\tvalid_0's ndcg@5: 0.560862\n",
      "[340]\tvalid_0's ndcg@5: 0.561524\n",
      "[350]\tvalid_0's ndcg@5: 0.561802\n",
      "[360]\tvalid_0's ndcg@5: 0.562288\n",
      "[370]\tvalid_0's ndcg@5: 0.562788\n",
      "[380]\tvalid_0's ndcg@5: 0.563034\n",
      "[390]\tvalid_0's ndcg@5: 0.563424\n",
      "[400]\tvalid_0's ndcg@5: 0.563856\n",
      "[410]\tvalid_0's ndcg@5: 0.564071\n",
      "[420]\tvalid_0's ndcg@5: 0.564425\n",
      "[430]\tvalid_0's ndcg@5: 0.564778\n",
      "[440]\tvalid_0's ndcg@5: 0.565167\n",
      "[450]\tvalid_0's ndcg@5: 0.565435\n",
      "[460]\tvalid_0's ndcg@5: 0.565732\n",
      "[470]\tvalid_0's ndcg@5: 0.566042\n",
      "[480]\tvalid_0's ndcg@5: 0.566369\n",
      "[490]\tvalid_0's ndcg@5: 0.566516\n",
      "[500]\tvalid_0's ndcg@5: 0.566826\n",
      "[510]\tvalid_0's ndcg@5: 0.56702\n",
      "[520]\tvalid_0's ndcg@5: 0.5671\n",
      "[530]\tvalid_0's ndcg@5: 0.56741\n",
      "[540]\tvalid_0's ndcg@5: 0.567769\n",
      "[550]\tvalid_0's ndcg@5: 0.567886\n",
      "[560]\tvalid_0's ndcg@5: 0.56832\n",
      "[570]\tvalid_0's ndcg@5: 0.568665\n",
      "[580]\tvalid_0's ndcg@5: 0.56895\n",
      "[590]\tvalid_0's ndcg@5: 0.569344\n",
      "[600]\tvalid_0's ndcg@5: 0.56981\n",
      "[610]\tvalid_0's ndcg@5: 0.570218\n",
      "[620]\tvalid_0's ndcg@5: 0.570393\n",
      "[630]\tvalid_0's ndcg@5: 0.570605\n",
      "[640]\tvalid_0's ndcg@5: 0.570833\n",
      "[650]\tvalid_0's ndcg@5: 0.571152\n",
      "[660]\tvalid_0's ndcg@5: 0.571276\n",
      "[670]\tvalid_0's ndcg@5: 0.571614\n",
      "[680]\tvalid_0's ndcg@5: 0.571864\n",
      "[690]\tvalid_0's ndcg@5: 0.572144\n",
      "[700]\tvalid_0's ndcg@5: 0.572308\n",
      "[710]\tvalid_0's ndcg@5: 0.572443\n",
      "[720]\tvalid_0's ndcg@5: 0.572799\n",
      "[730]\tvalid_0's ndcg@5: 0.573113\n",
      "[740]\tvalid_0's ndcg@5: 0.573427\n",
      "[750]\tvalid_0's ndcg@5: 0.573588\n",
      "[760]\tvalid_0's ndcg@5: 0.573701\n",
      "[770]\tvalid_0's ndcg@5: 0.573834\n",
      "[780]\tvalid_0's ndcg@5: 0.573993\n",
      "[790]\tvalid_0's ndcg@5: 0.574303\n",
      "[800]\tvalid_0's ndcg@5: 0.574576\n",
      "[810]\tvalid_0's ndcg@5: 0.574659\n",
      "[820]\tvalid_0's ndcg@5: 0.574921\n",
      "[830]\tvalid_0's ndcg@5: 0.575202\n",
      "[840]\tvalid_0's ndcg@5: 0.575684\n",
      "[850]\tvalid_0's ndcg@5: 0.575868\n",
      "[860]\tvalid_0's ndcg@5: 0.575891\n",
      "[870]\tvalid_0's ndcg@5: 0.576227\n",
      "[880]\tvalid_0's ndcg@5: 0.576485\n",
      "[890]\tvalid_0's ndcg@5: 0.576612\n",
      "[900]\tvalid_0's ndcg@5: 0.576925\n",
      "[910]\tvalid_0's ndcg@5: 0.577149\n",
      "[920]\tvalid_0's ndcg@5: 0.577294\n",
      "[930]\tvalid_0's ndcg@5: 0.577431\n",
      "[940]\tvalid_0's ndcg@5: 0.577478\n",
      "[950]\tvalid_0's ndcg@5: 0.577662\n",
      "[960]\tvalid_0's ndcg@5: 0.577872\n",
      "[970]\tvalid_0's ndcg@5: 0.577944\n",
      "[980]\tvalid_0's ndcg@5: 0.578161\n",
      "[990]\tvalid_0's ndcg@5: 0.578434\n",
      "[1000]\tvalid_0's ndcg@5: 0.578602\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's ndcg@5: 0.578602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-19 03:36:11,676] Trial 2 finished with value: 1.340653285993159 and parameters: {'num_leaves': 143, 'learning_rate': 0.01906272701145026, 'feature_fraction': 0.9590800050447245, 'bagging_freq': 6, 'verbose': 1, 'lambda_l1': 0.0031900296156635905, 'lambda_l2': 0.001035183265991094}. Best is trial 2 with value: 1.340653285993159.\n"
     ]
    }
   ],
   "source": [
    "group_sizes = train_data.groupby('group').size()\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna parameters\n",
    "    params = {\n",
    "        'objective': None,\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_at': 5,\n",
    "        'ndcg_eval_at': 5,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': trial.suggest_int('verbose', 0, 1),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-5, 10.0, log=True),  # L1正則化\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-5, 10.0, log=True)  # L2正則化\n",
    "    }\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    ndcgs = []\n",
    "    \n",
    "    for train_index, valid_index in gkf.split(train_data, groups=train_data['group']):\n",
    "        train_fold_data = train_data.iloc[train_index]\n",
    "        valid_fold_data = train_data.iloc[valid_index]\n",
    "    \n",
    "        train_fold_group_sizes = train_fold_data.groupby('group').size().tolist()\n",
    "        valid_fold_group_sizes = valid_fold_data.groupby('group').size().tolist()\n",
    "    \n",
    "        # train_indexを使用してsample_weightsから適切なウェイトを取得\n",
    "        train_dataset = lgb.Dataset(train_fold_data[features], \n",
    "                                   label=train_fold_data[target], \n",
    "                                   group=train_fold_group_sizes, \n",
    "                                   weight=sample_weights[train_index]) \n",
    "    \n",
    "        valid_dataset = lgb.Dataset(valid_fold_data[features], \n",
    "                                   label=valid_fold_data[target], \n",
    "                                   group=valid_fold_group_sizes, \n",
    "                                   reference=train_dataset)\n",
    "\n",
    "        ranker = lgb.train(\n",
    "            params,\n",
    "            train_dataset,\n",
    "            valid_sets=[valid_dataset],\n",
    "            evals_result={},\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=10,\n",
    "            fobj=custom_loss\n",
    "        )\n",
    "        \n",
    "        y_pred = ranker.predict(valid_fold_data[features])\n",
    "        top5_indices = rerank_predictions(valid_fold_data[target].values, y_pred)  # y_trueを渡すように修正\n",
    "        ndcg_value = mean_ndcg_score(valid_fold_data[target].values[top5_indices], y_pred[top5_indices], valid_fold_group_sizes)\n",
    "        ndcgs.append(ndcg_value)\n",
    "    \n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54620a51-010d-4544-ad64-6cb8c3033b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1172\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.896196\n",
      "[20]\tvalid_0's ndcg@5: 0.897\n",
      "[30]\tvalid_0's ndcg@5: 0.897565\n",
      "[40]\tvalid_0's ndcg@5: 0.897886\n",
      "[50]\tvalid_0's ndcg@5: 0.898222\n",
      "[60]\tvalid_0's ndcg@5: 0.898551\n",
      "[70]\tvalid_0's ndcg@5: 0.898854\n",
      "[80]\tvalid_0's ndcg@5: 0.899286\n",
      "[90]\tvalid_0's ndcg@5: 0.899578\n",
      "[100]\tvalid_0's ndcg@5: 0.899779\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's ndcg@5: 0.899788\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.895601\n",
      "[20]\tvalid_0's ndcg@5: 0.896466\n",
      "[30]\tvalid_0's ndcg@5: 0.896993\n",
      "[40]\tvalid_0's ndcg@5: 0.897426\n",
      "[50]\tvalid_0's ndcg@5: 0.897862\n",
      "[60]\tvalid_0's ndcg@5: 0.898236\n",
      "[70]\tvalid_0's ndcg@5: 0.898649\n",
      "[80]\tvalid_0's ndcg@5: 0.898985\n",
      "[90]\tvalid_0's ndcg@5: 0.899189\n",
      "[100]\tvalid_0's ndcg@5: 0.899594\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.899594\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.89529\n",
      "[20]\tvalid_0's ndcg@5: 0.896148\n",
      "[30]\tvalid_0's ndcg@5: 0.896799\n",
      "[40]\tvalid_0's ndcg@5: 0.897344\n",
      "[50]\tvalid_0's ndcg@5: 0.897612\n",
      "[60]\tvalid_0's ndcg@5: 0.898073\n",
      "[70]\tvalid_0's ndcg@5: 0.898458\n",
      "[80]\tvalid_0's ndcg@5: 0.898747\n",
      "[90]\tvalid_0's ndcg@5: 0.899043\n",
      "[100]\tvalid_0's ndcg@5: 0.8992\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.899248\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.895603\n",
      "[20]\tvalid_0's ndcg@5: 0.896393\n",
      "[30]\tvalid_0's ndcg@5: 0.897156\n",
      "[40]\tvalid_0's ndcg@5: 0.897817\n",
      "[50]\tvalid_0's ndcg@5: 0.898168\n",
      "[60]\tvalid_0's ndcg@5: 0.898602\n",
      "[70]\tvalid_0's ndcg@5: 0.898988\n",
      "[80]\tvalid_0's ndcg@5: 0.899305\n",
      "[90]\tvalid_0's ndcg@5: 0.899635\n",
      "[100]\tvalid_0's ndcg@5: 0.899936\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.899936\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9590800050447245, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9590800050447245\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0031900296156635905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0031900296156635905\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001035183265991094, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001035183265991094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.895032\n",
      "[20]\tvalid_0's ndcg@5: 0.896157\n",
      "[30]\tvalid_0's ndcg@5: 0.896743\n",
      "[40]\tvalid_0's ndcg@5: 0.897541\n",
      "[50]\tvalid_0's ndcg@5: 0.898064\n",
      "[60]\tvalid_0's ndcg@5: 0.898389\n",
      "[70]\tvalid_0's ndcg@5: 0.898606\n",
      "[80]\tvalid_0's ndcg@5: 0.898802\n",
      "[90]\tvalid_0's ndcg@5: 0.899179\n",
      "[100]\tvalid_0's ndcg@5: 0.899505\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.899505\n"
     ]
    }
   ],
   "source": [
    "# 最適なパラメータ\n",
    "best_params = study.best_params\n",
    "\n",
    "# KFoldでのモデル訓練\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = []\n",
    "\n",
    "for train_index, valid_index in kf.split(train_data):\n",
    "    train_fold_data = train_data.iloc[train_index]\n",
    "    valid_fold_data = train_data.iloc[valid_index]\n",
    "    \n",
    "    train_fold_group_sizes = train_fold_data.groupby('group').size().tolist()\n",
    "    valid_fold_group_sizes = valid_fold_data.groupby('group').size().tolist()\n",
    "    \n",
    "    ranker = lgb.LGBMRanker(**best_params)\n",
    "    ranker.fit(\n",
    "        train_fold_data[features], train_fold_data[target], \n",
    "        group=train_fold_group_sizes, \n",
    "        eval_set=[(valid_fold_data[features], valid_fold_data[target])], \n",
    "        eval_group=[valid_fold_group_sizes], \n",
    "        eval_at=5, early_stopping_rounds=20, verbose=10\n",
    "    )\n",
    "    models.append(ranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50618bb5-4841-4cb9-b4cb-cc029629358c",
   "metadata": {},
   "source": [
    "### テストデータで予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59de8ac9-a8f5-45fc-a650-2f6d47ffc9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>predicted_rank</th>\n",
       "      <th>kakutei_chakujun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1635601</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635602</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635603</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635604</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635605</th>\n",
       "      <td>2021-101-45-1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785604</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785605</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785606</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785607</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785608</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147974 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   group  predicted_rank  kakutei_chakujun\n",
       "1635601    2021-222-83-2             4.0                 6\n",
       "1635602    2021-222-83-2             1.0                 7\n",
       "1635603    2021-222-83-2            10.0                 1\n",
       "1635604    2021-222-83-2             2.0                 2\n",
       "1635605    2021-101-45-1             5.0                 1\n",
       "...                  ...             ...               ...\n",
       "1785604  2021-1231-54-11            11.0                 3\n",
       "1785605  2021-1231-54-11             4.0                 1\n",
       "1785606  2021-1231-54-11             5.0                 2\n",
       "1785607  2021-1231-54-11             8.0                11\n",
       "1785608  2021-1231-54-11             6.0                 5\n",
       "\n",
       "[147974 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2020年から2022年のテストデータを取得\n",
    "test_data_2020 = merged_df[merged_df['kaisai_nen'] == 2020].copy()\n",
    "test_data_2021 = merged_df[merged_df['kaisai_nen'] == 2021].copy()\n",
    "test_data_2022 = merged_df[merged_df['kaisai_nen'] == 2022].copy()\n",
    "\n",
    "# 2021年と2022年のデータに対して予測を行い、平均を取る\n",
    "test_data_2020.loc[:, 'y_pred'] = sum([model.predict(test_data_2020[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "test_data_2021.loc[:, 'y_pred'] = sum([model.predict(test_data_2021[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "test_data_2022.loc[:, 'y_pred'] = sum([model.predict(test_data_2022[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "\n",
    "# 予測されたランクをグループごとに計算\n",
    "test_data_2020.loc[:, 'predicted_rank'] = test_data_2020.groupby('group')['y_pred'].rank(method='min')\n",
    "test_data_2021.loc[:, 'predicted_rank'] = test_data_2021.groupby('group')['y_pred'].rank(method='min')\n",
    "test_data_2022.loc[:, 'predicted_rank'] = test_data_2022.groupby('group')['y_pred'].rank(method='min')\n",
    "\n",
    "# 結果を表示\n",
    "test_data_2021[['group', 'predicted_rank', 'kakutei_chakujun']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb294d-cc15-4ece-8051-7e9a1f1ca38c",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cea8327d-f038-4e3e-9b7e-15c0df8048b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datas = [test_data_2020, test_data_2021, test_data_2022]\n",
    "years = [2020, 2021, 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1c3a706-9b18-46da-86fc-4a3b9a5ab4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 2020: 3.717\n",
      "RMSE for 2021: 3.769\n",
      "RMSE for 2022: 3.748\n",
      "Mean RMSE: 3.745\n"
     ]
    }
   ],
   "source": [
    "rmse(test_datas, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "043525fe-1c05-4da8-ad68-145c682315d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 Precision@5: 38.013%\n",
      "2020 Recall@5: 63.355%\n",
      "2021 Precision@5: 37.801%\n",
      "2021 Recall@5: 63.001%\n",
      "2022 Precision@5: 37.738%\n",
      "2022 Recall@5: 62.897%\n"
     ]
    }
   ],
   "source": [
    "recall5(test_datas, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b531c07-7144-485f-863d-2d0e47211477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 Average Net Profit: -344.447 yen\n",
      "2021 Average Net Profit: -350.427 yen\n",
      "2022 Average Net Profit: -350.807 yen\n",
      "Mean Average Net Profit: -348.560 yen\n"
     ]
    }
   ],
   "source": [
    "profit(test_datas, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cba3cc9d-fed6-4870-8be0-6eafd8219f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# models[0]'s feature importances\n",
    "feature_importance = models[0].feature_importances_\n",
    "\n",
    "# Convert feature names and importances to DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b012e5-3fdb-4152-966e-1b76dd8e46bc",
   "metadata": {},
   "source": [
    "# モデル保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f56bbe4-89fe-4ca0-8ee4-3c4a6daed162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1172\n",
      "[LightGBM] [Info] Number of data points in the train set: 2017402, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 5.554518\n",
      "Model saved to ./bestmodels/lightgbmranker.pkl\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "full_train_dataset = lgb.Dataset(merged_df[features], label=merged_df[target])\n",
    "full_model = lgb.train(best_params, full_train_dataset, num_boost_round=500, verbose_eval=100)  # ここではvalid_setsやearly_stopping_roundsは使用しない\n",
    "\n",
    "# モデルを保存\n",
    "# model_save_path = '../app/models/model.pkl'\n",
    "model_save_path = './bestmodels/lightgbmranker.pkl'\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(full_model, f)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1347bfe-200d-49c4-8f13-6bf97f2b6f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_2020' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresults_2020\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_2020' is not defined"
     ]
    }
   ],
   "source": [
    "results_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb97dbd-e5c0-4f2b-8b9f-af80989301c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_groups_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19acc4-0eb7-4ac7-8e31-146549b97530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5343175-881e-4987-b4fa-5775077338b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
