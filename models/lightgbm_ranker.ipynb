{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ca9841-adc6-4612-b82f-f2f94a32faa7",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d127aa-802a-4ec0-9fc9-e2671c2dba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#モデル\n",
    "import lightgbm as lgb\n",
    "\n",
    "#パラメータ探索\n",
    "import optuna\n",
    "\n",
    "#クロスバリデーション\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "#エヴァリュエーション\n",
    "from sklearn.metrics import precision_score, recall_score, mean_squared_error\n",
    "\n",
    "#可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#保存\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11351b30-8083-423f-a50c-46bfde76436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを読み込む\n",
    "n_uma_race_df = pd.read_pickle('../datasets/traindata/n_uma_race.pkl')\n",
    "n_race_df = pd.read_pickle('../datasets/traindata/n_race.pkl')\n",
    "n_payout_df = pd.read_pickle('../datasets/traindata/n_payout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c664d196-e826-4eba-9cf8-851ea81544cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新しいグループを作成\n",
    "n_uma_race_df['group'] = n_uma_race_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_uma_race_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_uma_race_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_uma_race_df['race_bango'].astype(int).astype(str)\n",
    "n_race_df['group'] = n_race_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_race_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_race_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_race_df['race_bango'].astype(int).astype(str)\n",
    "n_payout_df['group'] = n_payout_df['kaisai_nen'].astype(int).astype(str) +\"-\"+ n_payout_df['kaisai_tsukihi'].astype(int).astype(str) +\"-\"+  n_payout_df['keibajo_code'].astype(int).astype(str) +\"-\"+  n_payout_df['race_bango'].astype(int).astype(str)\n",
    "\n",
    "n_race_df = n_race_df.drop(['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'kaisai_kai', 'kaisai_nichime', 'race_bango'],axis=1)\n",
    "n_payout_df = n_payout_df.drop(['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'kaisai_kai', 'kaisai_nichime', 'race_bango', 'toroku_tosu', 'shusso_tosu'],axis=1)\n",
    "\n",
    "merged_df = pd.merge(n_uma_race_df, n_race_df, on='group', how='left')\n",
    "merged_df = pd.merge(merged_df, n_payout_df, on='group', how='left')\n",
    "merged_df['group'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68c59eb-a4db-40af-b9cf-c5aba98aa72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-916-46-1', '2023-916-46-2', '2023-916-46-3', ...,\n",
       "       '2023-907-50-10', '2023-907-50-11', '2023-907-50-12'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ff2e39-36ac-42c3-b039-0a870a1dc2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kaisai_nen</th>\n",
       "      <th>kaisai_tsukihi</th>\n",
       "      <th>keibajo_code</th>\n",
       "      <th>kaisai_kai</th>\n",
       "      <th>kaisai_nichime</th>\n",
       "      <th>race_bango</th>\n",
       "      <th>wakuban</th>\n",
       "      <th>umaban</th>\n",
       "      <th>ketto_toroku_bango</th>\n",
       "      <th>bamei</th>\n",
       "      <th>umakigo_code</th>\n",
       "      <th>seibetsu_code</th>\n",
       "      <th>hinshu_code</th>\n",
       "      <th>moshoku_code</th>\n",
       "      <th>barei</th>\n",
       "      <th>tozai_shozoku_code</th>\n",
       "      <th>chokyoshi_code</th>\n",
       "      <th>banushi_code</th>\n",
       "      <th>banushimei</th>\n",
       "      <th>futan_juryo</th>\n",
       "      <th>blinker_shiyo_kubun</th>\n",
       "      <th>kishu_code</th>\n",
       "      <th>kishumei_ryakusho</th>\n",
       "      <th>kishu_minarai_code</th>\n",
       "      <th>bataiju</th>\n",
       "      <th>zogen_fugo</th>\n",
       "      <th>zogen_sa</th>\n",
       "      <th>ijo_kubun_code</th>\n",
       "      <th>nyusen_juni</th>\n",
       "      <th>kakutei_chakujun</th>\n",
       "      <th>dochaku_kubun</th>\n",
       "      <th>dochaku_tosu</th>\n",
       "      <th>soha_time</th>\n",
       "      <th>chakusa_code_1</th>\n",
       "      <th>chakusa_code_2</th>\n",
       "      <th>chakusa_code_3</th>\n",
       "      <th>corner_1</th>\n",
       "      <th>corner_2</th>\n",
       "      <th>corner_3</th>\n",
       "      <th>corner_4</th>\n",
       "      <th>tansho_odds</th>\n",
       "      <th>tansho_ninkijun</th>\n",
       "      <th>kakutoku_honshokin</th>\n",
       "      <th>kakutoku_fukashokin</th>\n",
       "      <th>kohan_4f</th>\n",
       "      <th>kohan_3f</th>\n",
       "      <th>aiteuma_joho_1</th>\n",
       "      <th>aiteuma_joho_2</th>\n",
       "      <th>aiteuma_joho_3</th>\n",
       "      <th>time_sa</th>\n",
       "      <th>record_koshin_kubun</th>\n",
       "      <th>kyakushitsu_hantei</th>\n",
       "      <th>group</th>\n",
       "      <th>yobi_code</th>\n",
       "      <th>jusho_kaiji</th>\n",
       "      <th>grade_code</th>\n",
       "      <th>kyoso_shubetsu_code</th>\n",
       "      <th>kyoso_kigo_code</th>\n",
       "      <th>juryo_shubetsu_code</th>\n",
       "      <th>kyoso_joken_code_2sai</th>\n",
       "      <th>kyoso_joken_code_3sai</th>\n",
       "      <th>kyoso_joken_code_4sai</th>\n",
       "      <th>kyoso_joken_code_5sai_ijo</th>\n",
       "      <th>kyoso_joken_code</th>\n",
       "      <th>kyori</th>\n",
       "      <th>track_code</th>\n",
       "      <th>course_kubun</th>\n",
       "      <th>honshokin</th>\n",
       "      <th>fukashokin</th>\n",
       "      <th>hasso_jikoku</th>\n",
       "      <th>toroku_tosu</th>\n",
       "      <th>shusso_tosu</th>\n",
       "      <th>nyusen_tosu</th>\n",
       "      <th>tenko_code</th>\n",
       "      <th>babajotai_code_shiba</th>\n",
       "      <th>babajotai_code_dirt</th>\n",
       "      <th>fuseiritsu_flag_sanrenpuku</th>\n",
       "      <th>tokubarai_flag_sanrenpuku</th>\n",
       "      <th>henkan_flag_sanrenpuku</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1a</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1b</th>\n",
       "      <th>haraimodoshi_sanrenpuku_1c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>916</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020106227</td>\n",
       "      <td>スターサファイア</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5319</td>\n",
       "      <td>875800</td>\n",
       "      <td>ＪＰＮ技研</td>\n",
       "      <td>560.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5300</td>\n",
       "      <td>葛山晃平</td>\n",
       "      <td>0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1359</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>899</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>2020105068フェイマスグローリ</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-916-46-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1400</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000e+51</td>\n",
       "      <td>0</td>\n",
       "      <td>1140</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60809.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kaisai_nen  kaisai_tsukihi keibajo_code  kaisai_kai  kaisai_nichime  \\\n",
       "0        2023             916           46          13               1   \n",
       "\n",
       "   race_bango  wakuban  umaban  ketto_toroku_bango               bamei  \\\n",
       "0           1        1       1          2020106227  スターサファイア　　　　　　　　　　   \n",
       "\n",
       "   umakigo_code  seibetsu_code  hinshu_code  moshoku_code  barei  \\\n",
       "0             0              1          1.0             3      3   \n",
       "\n",
       "   tozai_shozoku_code  chokyoshi_code  banushi_code  \\\n",
       "0                   3            5319        875800   \n",
       "\n",
       "                         banushimei  futan_juryo  blinker_shiyo_kubun  \\\n",
       "0  ＪＰＮ技研　　　　　　　　　　　　　　　　　　　　　　　　　　　        560.0                    0   \n",
       "\n",
       "   kishu_code kishumei_ryakusho  kishu_minarai_code  bataiju  zogen_fugo  \\\n",
       "0        5300              葛山晃平                   0    466.0           1   \n",
       "\n",
       "   zogen_sa  ijo_kubun_code  nyusen_juni  kakutei_chakujun  dochaku_kubun  \\\n",
       "0       0.0               0            8                 8              0   \n",
       "\n",
       "   dochaku_tosu  soha_time  chakusa_code_1  chakusa_code_2  chakusa_code_3  \\\n",
       "0             0       1359             4.0             NaN             NaN   \n",
       "\n",
       "   corner_1  corner_2  corner_3  corner_4  tansho_odds  tansho_ninkijun  \\\n",
       "0         9         9         9         9          899                8   \n",
       "\n",
       "   kakutoku_honshokin  kakutoku_fukashokin  kohan_4f  kohan_3f  \\\n",
       "0                   0                    0         0       411   \n",
       "\n",
       "                 aiteuma_joho_1                aiteuma_joho_2  \\\n",
       "0  2020105068フェイマスグローリ　　　　　　　　　  0000000000　　　　　　　　　　　　　　　　　　   \n",
       "\n",
       "                 aiteuma_joho_3  time_sa  record_koshin_kubun  \\\n",
       "0  0000000000　　　　　　　　　　　　　　　　　　       50                    0   \n",
       "\n",
       "   kyakushitsu_hantei          group  yobi_code  jusho_kaiji  grade_code  \\\n",
       "0                   0  2023-916-46-1          1            0           0   \n",
       "\n",
       "   kyoso_shubetsu_code  kyoso_kigo_code  juryo_shubetsu_code  \\\n",
       "0                   49                0                    4   \n",
       "\n",
       "   kyoso_joken_code_2sai  kyoso_joken_code_3sai  kyoso_joken_code_4sai  \\\n",
       "0                      0                      0                      0   \n",
       "\n",
       "   kyoso_joken_code_5sai_ijo  kyoso_joken_code  kyori  track_code  \\\n",
       "0                          0                 0   1400          24   \n",
       "\n",
       "   course_kubun     honshokin  fukashokin  hasso_jikoku  toroku_tosu  \\\n",
       "0             0  4.000000e+51           0          1140            9   \n",
       "\n",
       "   shusso_tosu  nyusen_tosu  tenko_code  babajotai_code_shiba  \\\n",
       "0            9            9           1                     0   \n",
       "\n",
       "   babajotai_code_dirt  fuseiritsu_flag_sanrenpuku  tokubarai_flag_sanrenpuku  \\\n",
       "0                  1.0                         0.0                        0.0   \n",
       "\n",
       "   henkan_flag_sanrenpuku  haraimodoshi_sanrenpuku_1a  \\\n",
       "0                     0.0                     60809.0   \n",
       "\n",
       "   haraimodoshi_sanrenpuku_1b  haraimodoshi_sanrenpuku_1c  \n",
       "0                       210.0                         1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1919f-72fe-444c-8021-dac0032d6065",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43e44e6-e588-4837-9eab-2c1542f91680",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['hutan_wariai'] = merged_df['futan_juryo'] / merged_df['bataiju']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3207c5cf-150b-40c0-9b0f-278f94680871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sign_and_diff(row):\n",
    "    if row['zogen_fugo'] == 2:\n",
    "        return row['zogen_sa']\n",
    "    elif row['zogen_fugo'] == 0:\n",
    "        return -row['zogen_sa']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "merged_df['zogen_ryou'] = merged_df.apply(combine_sign_and_diff, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9622447d-bc27-4919-8ec1-39e9a3d2bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['ijo_kubun_code'] == 0]\n",
    "# 1\t出走取消\t取消\tSCRATCHED\tS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2b6db6-3968-4a63-ab23-5b7a73ecc35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyori のデータ型: int32\n",
      "grade_code のデータ型: int32\n",
      "seibetsu_code のデータ型: int32\n",
      "moshoku_code のデータ型: int32\n",
      "barei のデータ型: int32\n",
      "chokyoshi_code のデータ型: int32\n",
      "banushi_code のデータ型: int32\n",
      "kishu_code のデータ型: int32\n",
      "kishu_minarai_code のデータ型: int32\n",
      "kyoso_shubetsu_code のデータ型: int32\n",
      "juryo_shubetsu_code のデータ型: int32\n",
      "shusso_tosu のデータ型: int32\n",
      "tenko_code のデータ型: int32\n",
      "babajotai_code_dirt のデータ型: int32\n",
      "hutan_wariai のデータ型: float64\n",
      "zogen_ryou のデータ型: int32\n",
      "track_code のデータ型: int32\n",
      "keibajo_code のデータ型: int32\n"
     ]
    }
   ],
   "source": [
    "columns_to_convert = [\n",
    "                    'kyori',\n",
    "                    'grade_code',\n",
    "                    'seibetsu_code',\n",
    "                    'moshoku_code',\n",
    "                    'barei',\n",
    "                    'chokyoshi_code',\n",
    "                    'banushi_code',\n",
    "                    'kishu_code',\n",
    "                    'kishu_minarai_code',\n",
    "                    'kyoso_shubetsu_code',\n",
    "                    'juryo_shubetsu_code',\n",
    "                    'shusso_tosu',\n",
    "                    'tenko_code',\n",
    "                    'babajotai_code_dirt',\n",
    "                    'hutan_wariai',\n",
    "                    'zogen_ryou',\n",
    "                    'track_code',\n",
    "                    'keibajo_code'\n",
    "                    ]\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    merged_df[column].fillna(0, inplace=True)\n",
    "    try:\n",
    "        if merged_df[column].astype(float).apply(lambda x: x.is_integer()).all():\n",
    "            merged_df[column] = merged_df[column].astype(int)\n",
    "        else:\n",
    "            merged_df[column] = merged_df[column].astype(float)\n",
    "    except ValueError:\n",
    "        merged_df[column] = merged_df[column].astype(float)\n",
    "\n",
    "    print(f\"{column} のデータ型: {merged_df[column].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a07b2c5-f805-499d-849d-d7cd88bf1f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 2017402 entries, 0 to 2042540\n",
      "Series name: keibajo_code\n",
      "Non-Null Count    Dtype\n",
      "--------------    -----\n",
      "2017402 non-null  int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 23.1 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df[column].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bf1c2-3a54-4a11-9309-d830239210d8",
   "metadata": {},
   "source": [
    "# lightgbm.LGBMRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "165fa6a7-87f9-4cf1-8cdc-3421e1c4a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010年から2019年のデータを学習データとして取得\n",
    "train_data = merged_df[merged_df['kaisai_nen'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9740ae80-b106-4159-a01e-765f87246544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ランキング学習のために必要な特徴量とターゲットを設定\n",
    "features = [\n",
    "            # 'kyori',\n",
    "            # 'grade_code',\n",
    "            'seibetsu_code',\n",
    "            'moshoku_code',\n",
    "            'barei',\n",
    "            'chokyoshi_code',\n",
    "            'banushi_code',\n",
    "            'kishu_code',\n",
    "            'kishu_minarai_code',\n",
    "            'kyoso_shubetsu_code',\n",
    "            'juryo_shubetsu_code',\n",
    "            # 'shusso_tosu',\n",
    "            # 'tenko_code',\n",
    "            # 'babajotai_code_dirt',\n",
    "            'hutan_wariai',\n",
    "            'zogen_ryou',\n",
    "            # 'track_code',\n",
    "            # 'keibajo_code'\n",
    "            ]\n",
    "\n",
    "target = 'kakutei_chakujun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "228abcf6-d782-4a41-8fa3-41594a5b3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_score(y_true, y_score, k=5):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    \n",
    "    gain = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "def mean_ndcg_score(y_true, y_score, groups, k=5):\n",
    "    ndcgs = []\n",
    "    idx_start = 0\n",
    "    for group in groups:\n",
    "        ndcgs.append(ndcg_score(y_true[idx_start:idx_start+group], y_score[idx_start:idx_start+group], k))\n",
    "        idx_start += group\n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4147b41-467a-4946-b929-6eb017229b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def lambda_rank_gradient(y_true, y_pred):\n",
    "#     pred_diff = y_pred[:, None] - y_pred[None, :]\n",
    "#     true_diff = y_true[:, None] - y_true[None, :]\n",
    "    \n",
    "#     S_ij = np.where(true_diff > 0, 1, np.where(true_diff < 0, -1, 0))\n",
    "    \n",
    "#     lambda_ij = 0.5 * (1 - S_ij) - sigmoid(-pred_diff)\n",
    "    \n",
    "#     grad = np.sum(lambda_ij, axis=1)\n",
    "    \n",
    "#     hess = np.sum(sigmoid(pred_diff) * (1 - sigmoid(pred_diff)), axis=1)\n",
    "#     return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eef21f3-910b-4c2c-a759-b3444fdd5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 損失関数\n",
    "# def custom_objective(y_true, y_pred):\n",
    "#     grad, hess = lambda_rank_gradient(y_true, y_pred)\n",
    "#     top5_indices = np.argsort(y_pred)[-5:]\n",
    "#     if np.sum(y_true[top5_indices] == 1) == 0:\n",
    "#         grad += penalty_grad\n",
    "#         hess += penalty_hess\n",
    "#     return grad, hess\n",
    "\n",
    "# # サンプリング\n",
    "# high_weight_value = 2.0\n",
    "# sample_weights = np.where(train_data[target] == 1, high_weight_value, 1)\n",
    "\n",
    "# # 再ランキング\n",
    "# def rerank_predictions(y_pred):\n",
    "#     top5_indices = np.argsort(y_pred)[-5:]\n",
    "#     if np.sum(y_true[top5_indices] == 1) < 3:\n",
    "#         one_indices = np.where(y_true == 1)[0]\n",
    "#         for idx in one_indices:\n",
    "#             if idx not in top5_indices:\n",
    "#                 top5_indices[-1] = idx\n",
    "#                 break\n",
    "#     return top5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "729dcb43-ba2d-426a-80f8-0e956074a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:36:32,940] A new study created in memory with name: no-name-44fceb7d-9d03-41e8-924f-234fabf2b6db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.536689\n",
      "[20]\tvalid_0's ndcg@5: 0.541784\n",
      "[30]\tvalid_0's ndcg@5: 0.543006\n",
      "[40]\tvalid_0's ndcg@5: 0.543816\n",
      "[50]\tvalid_0's ndcg@5: 0.544352\n",
      "[60]\tvalid_0's ndcg@5: 0.544866\n",
      "[70]\tvalid_0's ndcg@5: 0.54482\n",
      "[80]\tvalid_0's ndcg@5: 0.544214\n",
      "[90]\tvalid_0's ndcg@5: 0.545314\n",
      "[100]\tvalid_0's ndcg@5: 0.545938\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.546029\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.538368\n",
      "[20]\tvalid_0's ndcg@5: 0.541777\n",
      "[30]\tvalid_0's ndcg@5: 0.54273\n",
      "[40]\tvalid_0's ndcg@5: 0.54413\n",
      "[50]\tvalid_0's ndcg@5: 0.545072\n",
      "[60]\tvalid_0's ndcg@5: 0.545549\n",
      "[70]\tvalid_0's ndcg@5: 0.545308\n",
      "[80]\tvalid_0's ndcg@5: 0.545195\n",
      "[90]\tvalid_0's ndcg@5: 0.545917\n",
      "[100]\tvalid_0's ndcg@5: 0.546296\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.546431\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.53798\n",
      "[20]\tvalid_0's ndcg@5: 0.543259\n",
      "[30]\tvalid_0's ndcg@5: 0.544419\n",
      "[40]\tvalid_0's ndcg@5: 0.545823\n",
      "[50]\tvalid_0's ndcg@5: 0.545638\n",
      "[60]\tvalid_0's ndcg@5: 0.545996\n",
      "[70]\tvalid_0's ndcg@5: 0.546035\n",
      "[80]\tvalid_0's ndcg@5: 0.546483\n",
      "[90]\tvalid_0's ndcg@5: 0.547076\n",
      "[100]\tvalid_0's ndcg@5: 0.547409\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.547409\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.539871\n",
      "[20]\tvalid_0's ndcg@5: 0.544017\n",
      "[30]\tvalid_0's ndcg@5: 0.544493\n",
      "[40]\tvalid_0's ndcg@5: 0.54594\n",
      "[50]\tvalid_0's ndcg@5: 0.546308\n",
      "[60]\tvalid_0's ndcg@5: 0.546984\n",
      "[70]\tvalid_0's ndcg@5: 0.547401\n",
      "[80]\tvalid_0's ndcg@5: 0.547458\n",
      "[90]\tvalid_0's ndcg@5: 0.547917\n",
      "[100]\tvalid_0's ndcg@5: 0.548333\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.54858\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110614948240785, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110614948240785\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4350824683567366, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4350824683567366\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00012719260351976187, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012719260351976187\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.539125\n",
      "[20]\tvalid_0's ndcg@5: 0.54274\n",
      "[30]\tvalid_0's ndcg@5: 0.543669\n",
      "[40]\tvalid_0's ndcg@5: 0.544447\n",
      "[50]\tvalid_0's ndcg@5: 0.545429\n",
      "[60]\tvalid_0's ndcg@5: 0.545678\n",
      "[70]\tvalid_0's ndcg@5: 0.546145\n",
      "[80]\tvalid_0's ndcg@5: 0.546269\n",
      "[90]\tvalid_0's ndcg@5: 0.546911\n",
      "[100]\tvalid_0's ndcg@5: 0.547148\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.547243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:38:05,012] Trial 0 finished with value: 2150.499913594818 and parameters: {'num_leaves': 208, 'learning_rate': 0.00040338410105250237, 'feature_fraction': 0.5110614948240785, 'bagging_freq': 5, 'verbose': 0, 'lambda_l1': 0.4350824683567366, 'lambda_l2': 0.00012719260351976187}. Best is trial 0 with value: 2150.499913594818.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.51001\n",
      "[20]\tvalid_0's ndcg@5: 0.513843\n",
      "[30]\tvalid_0's ndcg@5: 0.517026\n",
      "[40]\tvalid_0's ndcg@5: 0.518433\n",
      "[50]\tvalid_0's ndcg@5: 0.520045\n",
      "[60]\tvalid_0's ndcg@5: 0.520382\n",
      "[70]\tvalid_0's ndcg@5: 0.521838\n",
      "[80]\tvalid_0's ndcg@5: 0.522266\n",
      "[90]\tvalid_0's ndcg@5: 0.523839\n",
      "[100]\tvalid_0's ndcg@5: 0.524375\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.524375\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.508424\n",
      "[20]\tvalid_0's ndcg@5: 0.511114\n",
      "[30]\tvalid_0's ndcg@5: 0.514559\n",
      "[40]\tvalid_0's ndcg@5: 0.515804\n",
      "[50]\tvalid_0's ndcg@5: 0.517733\n",
      "[60]\tvalid_0's ndcg@5: 0.51875\n",
      "[70]\tvalid_0's ndcg@5: 0.52043\n",
      "[80]\tvalid_0's ndcg@5: 0.521398\n",
      "[90]\tvalid_0's ndcg@5: 0.522388\n",
      "[100]\tvalid_0's ndcg@5: 0.522977\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.522977\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.509833\n",
      "[20]\tvalid_0's ndcg@5: 0.514543\n",
      "[30]\tvalid_0's ndcg@5: 0.516007\n",
      "[40]\tvalid_0's ndcg@5: 0.517266\n",
      "[50]\tvalid_0's ndcg@5: 0.519589\n",
      "[60]\tvalid_0's ndcg@5: 0.520732\n",
      "[70]\tvalid_0's ndcg@5: 0.52179\n",
      "[80]\tvalid_0's ndcg@5: 0.522622\n",
      "[90]\tvalid_0's ndcg@5: 0.524192\n",
      "[100]\tvalid_0's ndcg@5: 0.524675\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.524971\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.512725\n",
      "[20]\tvalid_0's ndcg@5: 0.51543\n",
      "[30]\tvalid_0's ndcg@5: 0.517872\n",
      "[40]\tvalid_0's ndcg@5: 0.518997\n",
      "[50]\tvalid_0's ndcg@5: 0.520965\n",
      "[60]\tvalid_0's ndcg@5: 0.522517\n",
      "[70]\tvalid_0's ndcg@5: 0.523988\n",
      "[80]\tvalid_0's ndcg@5: 0.524654\n",
      "[90]\tvalid_0's ndcg@5: 0.525877\n",
      "[100]\tvalid_0's ndcg@5: 0.52699\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.527067\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6200080469279441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6200080469279441\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0001631390128571795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001631390128571795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5186653519616857, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5186653519616857\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.509603\n",
      "[20]\tvalid_0's ndcg@5: 0.512141\n",
      "[30]\tvalid_0's ndcg@5: 0.514425\n",
      "[40]\tvalid_0's ndcg@5: 0.516354\n",
      "[50]\tvalid_0's ndcg@5: 0.518952\n",
      "[60]\tvalid_0's ndcg@5: 0.519763\n",
      "[70]\tvalid_0's ndcg@5: 0.520958\n",
      "[80]\tvalid_0's ndcg@5: 0.521444\n",
      "[90]\tvalid_0's ndcg@5: 0.522656\n",
      "[100]\tvalid_0's ndcg@5: 0.523227\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.523294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:39:04,160] Trial 1 finished with value: 2061.1288135935233 and parameters: {'num_leaves': 15, 'learning_rate': 0.021480188005177835, 'feature_fraction': 0.6200080469279441, 'bagging_freq': 6, 'verbose': 1, 'lambda_l1': 0.0001631390128571795, 'lambda_l2': 0.5186653519616857}. Best is trial 0 with value: 2150.499913594818.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.53585\n",
      "[20]\tvalid_0's ndcg@5: 0.543138\n",
      "[30]\tvalid_0's ndcg@5: 0.547079\n",
      "[40]\tvalid_0's ndcg@5: 0.551403\n",
      "[50]\tvalid_0's ndcg@5: 0.553802\n",
      "[60]\tvalid_0's ndcg@5: 0.556187\n",
      "[70]\tvalid_0's ndcg@5: 0.557664\n",
      "[80]\tvalid_0's ndcg@5: 0.559292\n",
      "[90]\tvalid_0's ndcg@5: 0.561247\n",
      "[100]\tvalid_0's ndcg@5: 0.56271\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.56271\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.535511\n",
      "[20]\tvalid_0's ndcg@5: 0.542232\n",
      "[30]\tvalid_0's ndcg@5: 0.548169\n",
      "[40]\tvalid_0's ndcg@5: 0.55071\n",
      "[50]\tvalid_0's ndcg@5: 0.55353\n",
      "[60]\tvalid_0's ndcg@5: 0.555924\n",
      "[70]\tvalid_0's ndcg@5: 0.558165\n",
      "[80]\tvalid_0's ndcg@5: 0.560049\n",
      "[90]\tvalid_0's ndcg@5: 0.562407\n",
      "[100]\tvalid_0's ndcg@5: 0.563383\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.563383\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.538122\n",
      "[20]\tvalid_0's ndcg@5: 0.544463\n",
      "[30]\tvalid_0's ndcg@5: 0.548544\n",
      "[40]\tvalid_0's ndcg@5: 0.551861\n",
      "[50]\tvalid_0's ndcg@5: 0.554225\n",
      "[60]\tvalid_0's ndcg@5: 0.557163\n",
      "[70]\tvalid_0's ndcg@5: 0.559518\n",
      "[80]\tvalid_0's ndcg@5: 0.560834\n",
      "[90]\tvalid_0's ndcg@5: 0.56312\n",
      "[100]\tvalid_0's ndcg@5: 0.564459\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.564459\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.538185\n",
      "[20]\tvalid_0's ndcg@5: 0.545313\n",
      "[30]\tvalid_0's ndcg@5: 0.549463\n",
      "[40]\tvalid_0's ndcg@5: 0.552526\n",
      "[50]\tvalid_0's ndcg@5: 0.555255\n",
      "[60]\tvalid_0's ndcg@5: 0.55702\n",
      "[70]\tvalid_0's ndcg@5: 0.559523\n",
      "[80]\tvalid_0's ndcg@5: 0.561255\n",
      "[90]\tvalid_0's ndcg@5: 0.56304\n",
      "[100]\tvalid_0's ndcg@5: 0.564338\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.564338\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.53715\n",
      "[20]\tvalid_0's ndcg@5: 0.544043\n",
      "[30]\tvalid_0's ndcg@5: 0.547443\n",
      "[40]\tvalid_0's ndcg@5: 0.551741\n",
      "[50]\tvalid_0's ndcg@5: 0.554523\n",
      "[60]\tvalid_0's ndcg@5: 0.556851\n",
      "[70]\tvalid_0's ndcg@5: 0.558991\n",
      "[80]\tvalid_0's ndcg@5: 0.560364\n",
      "[90]\tvalid_0's ndcg@5: 0.562066\n",
      "[100]\tvalid_0's ndcg@5: 0.563708\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.563708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:40:18,035] Trial 2 finished with value: 2245.6078296052387 and parameters: {'num_leaves': 101, 'learning_rate': 0.0658432777742698, 'feature_fraction': 0.7241844883625201, 'bagging_freq': 5, 'verbose': 1, 'lambda_l1': 0.42367192289815003, 'lambda_l2': 0.03048397774460981}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.516067\n",
      "[20]\tvalid_0's ndcg@5: 0.519402\n",
      "[30]\tvalid_0's ndcg@5: 0.520719\n",
      "[40]\tvalid_0's ndcg@5: 0.5217\n",
      "[50]\tvalid_0's ndcg@5: 0.521624\n",
      "[60]\tvalid_0's ndcg@5: 0.522405\n",
      "[70]\tvalid_0's ndcg@5: 0.522981\n",
      "[80]\tvalid_0's ndcg@5: 0.522725\n",
      "[90]\tvalid_0's ndcg@5: 0.522981\n",
      "[100]\tvalid_0's ndcg@5: 0.523845\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.523845\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.516986\n",
      "[20]\tvalid_0's ndcg@5: 0.518997\n",
      "[30]\tvalid_0's ndcg@5: 0.519569\n",
      "[40]\tvalid_0's ndcg@5: 0.521115\n",
      "[50]\tvalid_0's ndcg@5: 0.521207\n",
      "[60]\tvalid_0's ndcg@5: 0.521721\n",
      "[70]\tvalid_0's ndcg@5: 0.521612\n",
      "[80]\tvalid_0's ndcg@5: 0.521482\n",
      "[90]\tvalid_0's ndcg@5: 0.521867\n",
      "[100]\tvalid_0's ndcg@5: 0.523261\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.523261\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.516597\n",
      "[20]\tvalid_0's ndcg@5: 0.518337\n",
      "[30]\tvalid_0's ndcg@5: 0.520121\n",
      "[40]\tvalid_0's ndcg@5: 0.520003\n",
      "[50]\tvalid_0's ndcg@5: 0.52114\n",
      "[60]\tvalid_0's ndcg@5: 0.521128\n",
      "[70]\tvalid_0's ndcg@5: 0.521863\n",
      "[80]\tvalid_0's ndcg@5: 0.521733\n",
      "[90]\tvalid_0's ndcg@5: 0.522666\n",
      "[100]\tvalid_0's ndcg@5: 0.523122\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.523122\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.519658\n",
      "[20]\tvalid_0's ndcg@5: 0.521737\n",
      "[30]\tvalid_0's ndcg@5: 0.522919\n",
      "[40]\tvalid_0's ndcg@5: 0.523818\n",
      "[50]\tvalid_0's ndcg@5: 0.524549\n",
      "[60]\tvalid_0's ndcg@5: 0.52502\n",
      "[70]\tvalid_0's ndcg@5: 0.525492\n",
      "[80]\tvalid_0's ndcg@5: 0.525241\n",
      "[90]\tvalid_0's ndcg@5: 0.52576\n",
      "[100]\tvalid_0's ndcg@5: 0.526057\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.526057\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5062507239417015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5062507239417015\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017051078871069551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017051078871069551\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004924586474738295, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004924586474738295\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.516855\n",
      "[20]\tvalid_0's ndcg@5: 0.518857\n",
      "[30]\tvalid_0's ndcg@5: 0.52135\n",
      "[40]\tvalid_0's ndcg@5: 0.521415\n",
      "[50]\tvalid_0's ndcg@5: 0.522093\n",
      "[60]\tvalid_0's ndcg@5: 0.522443\n",
      "[70]\tvalid_0's ndcg@5: 0.523343\n",
      "[80]\tvalid_0's ndcg@5: 0.522894\n",
      "[90]\tvalid_0's ndcg@5: 0.523483\n",
      "[100]\tvalid_0's ndcg@5: 0.523935\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.523935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:41:22,609] Trial 3 finished with value: 2041.8282859494805 and parameters: {'num_leaves': 41, 'learning_rate': 0.00028144998473686954, 'feature_fraction': 0.5062507239417015, 'bagging_freq': 3, 'verbose': 0, 'lambda_l1': 0.0017051078871069551, 'lambda_l2': 0.0004924586474738295}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.5122\n",
      "[20]\tvalid_0's ndcg@5: 0.513618\n",
      "[30]\tvalid_0's ndcg@5: 0.514423\n",
      "[40]\tvalid_0's ndcg@5: 0.516013\n",
      "[50]\tvalid_0's ndcg@5: 0.516152\n",
      "[60]\tvalid_0's ndcg@5: 0.517697\n",
      "[70]\tvalid_0's ndcg@5: 0.518587\n",
      "[80]\tvalid_0's ndcg@5: 0.518925\n",
      "[90]\tvalid_0's ndcg@5: 0.519994\n",
      "[100]\tvalid_0's ndcg@5: 0.520489\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.520535\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.51079\n",
      "[20]\tvalid_0's ndcg@5: 0.512616\n",
      "[30]\tvalid_0's ndcg@5: 0.515578\n",
      "[40]\tvalid_0's ndcg@5: 0.516301\n",
      "[50]\tvalid_0's ndcg@5: 0.517712\n",
      "[60]\tvalid_0's ndcg@5: 0.518253\n",
      "[70]\tvalid_0's ndcg@5: 0.518632\n",
      "[80]\tvalid_0's ndcg@5: 0.518702\n",
      "[90]\tvalid_0's ndcg@5: 0.519639\n",
      "[100]\tvalid_0's ndcg@5: 0.520534\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.520534\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.513265\n",
      "[20]\tvalid_0's ndcg@5: 0.514623\n",
      "[30]\tvalid_0's ndcg@5: 0.515019\n",
      "[40]\tvalid_0's ndcg@5: 0.516177\n",
      "[50]\tvalid_0's ndcg@5: 0.516929\n",
      "[60]\tvalid_0's ndcg@5: 0.517753\n",
      "[70]\tvalid_0's ndcg@5: 0.518498\n",
      "[80]\tvalid_0's ndcg@5: 0.518551\n",
      "[90]\tvalid_0's ndcg@5: 0.519265\n",
      "[100]\tvalid_0's ndcg@5: 0.519539\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.519729\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.514807\n",
      "[20]\tvalid_0's ndcg@5: 0.516342\n",
      "[30]\tvalid_0's ndcg@5: 0.518579\n",
      "[40]\tvalid_0's ndcg@5: 0.519478\n",
      "[50]\tvalid_0's ndcg@5: 0.520882\n",
      "[60]\tvalid_0's ndcg@5: 0.521865\n",
      "[70]\tvalid_0's ndcg@5: 0.522534\n",
      "[80]\tvalid_0's ndcg@5: 0.522149\n",
      "[90]\tvalid_0's ndcg@5: 0.522816\n",
      "[100]\tvalid_0's ndcg@5: 0.523188\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.523194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5987891117944546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5987891117944546\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.5065994973799154e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.5065994973799154e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.1498356992641376e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.1498356992641376e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.512218\n",
      "[20]\tvalid_0's ndcg@5: 0.51419\n",
      "[30]\tvalid_0's ndcg@5: 0.514988\n",
      "[40]\tvalid_0's ndcg@5: 0.516427\n",
      "[50]\tvalid_0's ndcg@5: 0.518385\n",
      "[60]\tvalid_0's ndcg@5: 0.518692\n",
      "[70]\tvalid_0's ndcg@5: 0.5191\n",
      "[80]\tvalid_0's ndcg@5: 0.519356\n",
      "[90]\tvalid_0's ndcg@5: 0.519776\n",
      "[100]\tvalid_0's ndcg@5: 0.520215\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\tvalid_0's ndcg@5: 0.520474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:42:27,623] Trial 4 finished with value: 2041.04286508835 and parameters: {'num_leaves': 27, 'learning_rate': 0.00161926910119409, 'feature_fraction': 0.5987891117944546, 'bagging_freq': 5, 'verbose': 0, 'lambda_l1': 5.5065994973799154e-05, 'lambda_l2': 4.1498356992641376e-05}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.528719\n",
      "[20]\tvalid_0's ndcg@5: 0.531745\n",
      "[30]\tvalid_0's ndcg@5: 0.532512\n",
      "[40]\tvalid_0's ndcg@5: 0.534143\n",
      "[50]\tvalid_0's ndcg@5: 0.535158\n",
      "[60]\tvalid_0's ndcg@5: 0.535334\n",
      "[70]\tvalid_0's ndcg@5: 0.536214\n",
      "[80]\tvalid_0's ndcg@5: 0.53615\n",
      "[90]\tvalid_0's ndcg@5: 0.536492\n",
      "[100]\tvalid_0's ndcg@5: 0.536798\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's ndcg@5: 0.537045\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.529775\n",
      "[20]\tvalid_0's ndcg@5: 0.53252\n",
      "[30]\tvalid_0's ndcg@5: 0.532883\n",
      "[40]\tvalid_0's ndcg@5: 0.53432\n",
      "[50]\tvalid_0's ndcg@5: 0.535594\n",
      "[60]\tvalid_0's ndcg@5: 0.535977\n",
      "[70]\tvalid_0's ndcg@5: 0.53604\n",
      "[80]\tvalid_0's ndcg@5: 0.535694\n",
      "[90]\tvalid_0's ndcg@5: 0.536587\n",
      "[100]\tvalid_0's ndcg@5: 0.536998\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.537224\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.530989\n",
      "[20]\tvalid_0's ndcg@5: 0.532282\n",
      "[30]\tvalid_0's ndcg@5: 0.533577\n",
      "[40]\tvalid_0's ndcg@5: 0.535222\n",
      "[50]\tvalid_0's ndcg@5: 0.535784\n",
      "[60]\tvalid_0's ndcg@5: 0.536405\n",
      "[70]\tvalid_0's ndcg@5: 0.53652\n",
      "[80]\tvalid_0's ndcg@5: 0.536246\n",
      "[90]\tvalid_0's ndcg@5: 0.53718\n",
      "[100]\tvalid_0's ndcg@5: 0.537534\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@5: 0.537745\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.533429\n",
      "[20]\tvalid_0's ndcg@5: 0.534818\n",
      "[30]\tvalid_0's ndcg@5: 0.535675\n",
      "[40]\tvalid_0's ndcg@5: 0.536095\n",
      "[50]\tvalid_0's ndcg@5: 0.537632\n",
      "[60]\tvalid_0's ndcg@5: 0.538041\n",
      "[70]\tvalid_0's ndcg@5: 0.538886\n",
      "[80]\tvalid_0's ndcg@5: 0.538757\n",
      "[90]\tvalid_0's ndcg@5: 0.539245\n",
      "[100]\tvalid_0's ndcg@5: 0.539704\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.539927\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6728059338828258, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6728059338828258\n",
      "[LightGBM] [Warning] lambda_l1 is set=4.071248538751725, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.071248538751725\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6624928316149733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6624928316149733\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.530264\n",
      "[20]\tvalid_0's ndcg@5: 0.532588\n",
      "[30]\tvalid_0's ndcg@5: 0.533662\n",
      "[40]\tvalid_0's ndcg@5: 0.534527\n",
      "[50]\tvalid_0's ndcg@5: 0.535668\n",
      "[60]\tvalid_0's ndcg@5: 0.536256\n",
      "[70]\tvalid_0's ndcg@5: 0.536805\n",
      "[80]\tvalid_0's ndcg@5: 0.536494\n",
      "[90]\tvalid_0's ndcg@5: 0.537353\n",
      "[100]\tvalid_0's ndcg@5: 0.537573\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.537766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:43:46,756] Trial 5 finished with value: 2103.6608925449173 and parameters: {'num_leaves': 145, 'learning_rate': 0.00019005617485924828, 'feature_fraction': 0.6728059338828258, 'bagging_freq': 3, 'verbose': 1, 'lambda_l1': 4.071248538751725, 'lambda_l2': 0.6624928316149733}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.52159\n",
      "[20]\tvalid_0's ndcg@5: 0.523931\n",
      "[30]\tvalid_0's ndcg@5: 0.524871\n",
      "[40]\tvalid_0's ndcg@5: 0.526843\n",
      "[50]\tvalid_0's ndcg@5: 0.527311\n",
      "[60]\tvalid_0's ndcg@5: 0.528504\n",
      "[70]\tvalid_0's ndcg@5: 0.5292\n",
      "[80]\tvalid_0's ndcg@5: 0.529439\n",
      "[90]\tvalid_0's ndcg@5: 0.530017\n",
      "[100]\tvalid_0's ndcg@5: 0.530525\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.530525\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1173\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.520629\n",
      "[20]\tvalid_0's ndcg@5: 0.523718\n",
      "[30]\tvalid_0's ndcg@5: 0.524523\n",
      "[40]\tvalid_0's ndcg@5: 0.5256\n",
      "[50]\tvalid_0's ndcg@5: 0.526244\n",
      "[60]\tvalid_0's ndcg@5: 0.527292\n",
      "[70]\tvalid_0's ndcg@5: 0.528129\n",
      "[80]\tvalid_0's ndcg@5: 0.52838\n",
      "[90]\tvalid_0's ndcg@5: 0.529154\n",
      "[100]\tvalid_0's ndcg@5: 0.529341\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.529532\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.522424\n",
      "[20]\tvalid_0's ndcg@5: 0.525754\n",
      "[30]\tvalid_0's ndcg@5: 0.526446\n",
      "[40]\tvalid_0's ndcg@5: 0.527238\n",
      "[50]\tvalid_0's ndcg@5: 0.528627\n",
      "[60]\tvalid_0's ndcg@5: 0.529139\n",
      "[70]\tvalid_0's ndcg@5: 0.52988\n",
      "[80]\tvalid_0's ndcg@5: 0.530138\n",
      "[90]\tvalid_0's ndcg@5: 0.531102\n",
      "[100]\tvalid_0's ndcg@5: 0.531261\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.531519\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1169\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.523983\n",
      "[20]\tvalid_0's ndcg@5: 0.527633\n",
      "[30]\tvalid_0's ndcg@5: 0.528286\n",
      "[40]\tvalid_0's ndcg@5: 0.530432\n",
      "[50]\tvalid_0's ndcg@5: 0.531483\n",
      "[60]\tvalid_0's ndcg@5: 0.531408\n",
      "[70]\tvalid_0's ndcg@5: 0.532674\n",
      "[80]\tvalid_0's ndcg@5: 0.532984\n",
      "[90]\tvalid_0's ndcg@5: 0.533202\n",
      "[100]\tvalid_0's ndcg@5: 0.533795\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.533795\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168486, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7887191494320571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7887191494320571\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0583039683927263e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0583039683927263e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.23271014085304778, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23271014085304778\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.521822\n",
      "[20]\tvalid_0's ndcg@5: 0.52453\n",
      "[30]\tvalid_0's ndcg@5: 0.526339\n",
      "[40]\tvalid_0's ndcg@5: 0.527432\n",
      "[50]\tvalid_0's ndcg@5: 0.52791\n",
      "[60]\tvalid_0's ndcg@5: 0.529057\n",
      "[70]\tvalid_0's ndcg@5: 0.529542\n",
      "[80]\tvalid_0's ndcg@5: 0.529976\n",
      "[90]\tvalid_0's ndcg@5: 0.530515\n",
      "[100]\tvalid_0's ndcg@5: 0.530903\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.530903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:45:00,881] Trial 6 finished with value: 2088.521189855688 and parameters: {'num_leaves': 86, 'learning_rate': 0.0020973254152276744, 'feature_fraction': 0.7887191494320571, 'bagging_freq': 7, 'verbose': 1, 'lambda_l1': 1.0583039683927263e-05, 'lambda_l2': 0.23271014085304778}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.514608\n",
      "[20]\tvalid_0's ndcg@5: 0.526852\n",
      "[30]\tvalid_0's ndcg@5: 0.52863\n",
      "[40]\tvalid_0's ndcg@5: 0.528916\n",
      "[50]\tvalid_0's ndcg@5: 0.528747\n",
      "[60]\tvalid_0's ndcg@5: 0.529496\n",
      "[70]\tvalid_0's ndcg@5: 0.529071\n",
      "[80]\tvalid_0's ndcg@5: 0.528515\n",
      "[90]\tvalid_0's ndcg@5: 0.527639\n",
      "[100]\tvalid_0's ndcg@5: 0.528338\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[56]\tvalid_0's ndcg@5: 0.530156\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.515453\n",
      "[20]\tvalid_0's ndcg@5: 0.525444\n",
      "[30]\tvalid_0's ndcg@5: 0.527584\n",
      "[40]\tvalid_0's ndcg@5: 0.528219\n",
      "[50]\tvalid_0's ndcg@5: 0.527426\n",
      "[60]\tvalid_0's ndcg@5: 0.528537\n",
      "[70]\tvalid_0's ndcg@5: 0.528208\n",
      "[80]\tvalid_0's ndcg@5: 0.527666\n",
      "[90]\tvalid_0's ndcg@5: 0.527273\n",
      "[100]\tvalid_0's ndcg@5: 0.528103\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[57]\tvalid_0's ndcg@5: 0.529071\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.515634\n",
      "[20]\tvalid_0's ndcg@5: 0.526835\n",
      "[30]\tvalid_0's ndcg@5: 0.528348\n",
      "[40]\tvalid_0's ndcg@5: 0.529053\n",
      "[50]\tvalid_0's ndcg@5: 0.52775\n",
      "[60]\tvalid_0's ndcg@5: 0.528349\n",
      "[70]\tvalid_0's ndcg@5: 0.527478\n",
      "[80]\tvalid_0's ndcg@5: 0.527391\n",
      "[90]\tvalid_0's ndcg@5: 0.527367\n",
      "[100]\tvalid_0's ndcg@5: 0.528417\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[37]\tvalid_0's ndcg@5: 0.529561\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.517577\n",
      "[20]\tvalid_0's ndcg@5: 0.527697\n",
      "[30]\tvalid_0's ndcg@5: 0.530501\n",
      "[40]\tvalid_0's ndcg@5: 0.531435\n",
      "[50]\tvalid_0's ndcg@5: 0.530841\n",
      "[60]\tvalid_0's ndcg@5: 0.531713\n",
      "[70]\tvalid_0's ndcg@5: 0.531032\n",
      "[80]\tvalid_0's ndcg@5: 0.530908\n",
      "[90]\tvalid_0's ndcg@5: 0.530878\n",
      "[100]\tvalid_0's ndcg@5: 0.531817\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[56]\tvalid_0's ndcg@5: 0.532593\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4013332378919546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4013332378919546\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0648633121773343, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0648633121773343\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.5171939391380915, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.5171939391380915\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518606\n",
      "[20]\tvalid_0's ndcg@5: 0.527143\n",
      "[30]\tvalid_0's ndcg@5: 0.529452\n",
      "[40]\tvalid_0's ndcg@5: 0.529803\n",
      "[50]\tvalid_0's ndcg@5: 0.528956\n",
      "[60]\tvalid_0's ndcg@5: 0.529472\n",
      "[70]\tvalid_0's ndcg@5: 0.528948\n",
      "[80]\tvalid_0's ndcg@5: 0.528749\n",
      "[90]\tvalid_0's ndcg@5: 0.528588\n",
      "[100]\tvalid_0's ndcg@5: 0.529268\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[56]\tvalid_0's ndcg@5: 0.530561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:46:06,603] Trial 7 finished with value: 2079.1465843955775 and parameters: {'num_leaves': 63, 'learning_rate': 0.0017081253096541235, 'feature_fraction': 0.4013332378919546, 'bagging_freq': 8, 'verbose': 0, 'lambda_l1': 0.0648633121773343, 'lambda_l2': 3.5171939391380915}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.526911\n",
      "[20]\tvalid_0's ndcg@5: 0.531535\n",
      "[30]\tvalid_0's ndcg@5: 0.533006\n",
      "[40]\tvalid_0's ndcg@5: 0.533699\n",
      "[50]\tvalid_0's ndcg@5: 0.534826\n",
      "[60]\tvalid_0's ndcg@5: 0.535602\n",
      "[70]\tvalid_0's ndcg@5: 0.535927\n",
      "[80]\tvalid_0's ndcg@5: 0.535656\n",
      "[90]\tvalid_0's ndcg@5: 0.536876\n",
      "[100]\tvalid_0's ndcg@5: 0.537083\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.537311\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.529393\n",
      "[20]\tvalid_0's ndcg@5: 0.531202\n",
      "[30]\tvalid_0's ndcg@5: 0.531703\n",
      "[40]\tvalid_0's ndcg@5: 0.533485\n",
      "[50]\tvalid_0's ndcg@5: 0.533822\n",
      "[60]\tvalid_0's ndcg@5: 0.534781\n",
      "[70]\tvalid_0's ndcg@5: 0.535365\n",
      "[80]\tvalid_0's ndcg@5: 0.535339\n",
      "[90]\tvalid_0's ndcg@5: 0.536358\n",
      "[100]\tvalid_0's ndcg@5: 0.536555\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.536839\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.527126\n",
      "[20]\tvalid_0's ndcg@5: 0.531958\n",
      "[30]\tvalid_0's ndcg@5: 0.533487\n",
      "[40]\tvalid_0's ndcg@5: 0.534561\n",
      "[50]\tvalid_0's ndcg@5: 0.534826\n",
      "[60]\tvalid_0's ndcg@5: 0.535632\n",
      "[70]\tvalid_0's ndcg@5: 0.53621\n",
      "[80]\tvalid_0's ndcg@5: 0.536237\n",
      "[90]\tvalid_0's ndcg@5: 0.536835\n",
      "[100]\tvalid_0's ndcg@5: 0.53752\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.53759\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.531669\n",
      "[20]\tvalid_0's ndcg@5: 0.534015\n",
      "[30]\tvalid_0's ndcg@5: 0.535786\n",
      "[40]\tvalid_0's ndcg@5: 0.536981\n",
      "[50]\tvalid_0's ndcg@5: 0.537616\n",
      "[60]\tvalid_0's ndcg@5: 0.537972\n",
      "[70]\tvalid_0's ndcg@5: 0.538798\n",
      "[80]\tvalid_0's ndcg@5: 0.538821\n",
      "[90]\tvalid_0's ndcg@5: 0.539404\n",
      "[100]\tvalid_0's ndcg@5: 0.539871\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.540279\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5506723210542416, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5506723210542416\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1428013083754864, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1428013083754864\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0083817933586486, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0083817933586486\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.526285\n",
      "[20]\tvalid_0's ndcg@5: 0.530979\n",
      "[30]\tvalid_0's ndcg@5: 0.533375\n",
      "[40]\tvalid_0's ndcg@5: 0.533624\n",
      "[50]\tvalid_0's ndcg@5: 0.534825\n",
      "[60]\tvalid_0's ndcg@5: 0.535476\n",
      "[70]\tvalid_0's ndcg@5: 0.535869\n",
      "[80]\tvalid_0's ndcg@5: 0.536186\n",
      "[90]\tvalid_0's ndcg@5: 0.536082\n",
      "[100]\tvalid_0's ndcg@5: 0.536755\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.536755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:47:18,070] Trial 8 finished with value: 2110.0154239721637 and parameters: {'num_leaves': 88, 'learning_rate': 0.0008715335257268971, 'feature_fraction': 0.5506723210542416, 'bagging_freq': 6, 'verbose': 0, 'lambda_l1': 1.1428013083754864, 'lambda_l2': 0.0083817933586486}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518914\n",
      "[20]\tvalid_0's ndcg@5: 0.522245\n",
      "[30]\tvalid_0's ndcg@5: 0.523469\n",
      "[40]\tvalid_0's ndcg@5: 0.525296\n",
      "[50]\tvalid_0's ndcg@5: 0.527035\n",
      "[60]\tvalid_0's ndcg@5: 0.52783\n",
      "[70]\tvalid_0's ndcg@5: 0.528594\n",
      "[80]\tvalid_0's ndcg@5: 0.529644\n",
      "[90]\tvalid_0's ndcg@5: 0.530313\n",
      "[100]\tvalid_0's ndcg@5: 0.530859\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@5: 0.531127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.517286\n",
      "[20]\tvalid_0's ndcg@5: 0.520467\n",
      "[30]\tvalid_0's ndcg@5: 0.522218\n",
      "[40]\tvalid_0's ndcg@5: 0.524334\n",
      "[50]\tvalid_0's ndcg@5: 0.526653\n",
      "[60]\tvalid_0's ndcg@5: 0.526743\n",
      "[70]\tvalid_0's ndcg@5: 0.528405\n",
      "[80]\tvalid_0's ndcg@5: 0.528518\n",
      "[90]\tvalid_0's ndcg@5: 0.529805\n",
      "[100]\tvalid_0's ndcg@5: 0.530488\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.530488\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.518833\n",
      "[20]\tvalid_0's ndcg@5: 0.521339\n",
      "[30]\tvalid_0's ndcg@5: 0.523472\n",
      "[40]\tvalid_0's ndcg@5: 0.525395\n",
      "[50]\tvalid_0's ndcg@5: 0.527333\n",
      "[60]\tvalid_0's ndcg@5: 0.528396\n",
      "[70]\tvalid_0's ndcg@5: 0.529377\n",
      "[80]\tvalid_0's ndcg@5: 0.529457\n",
      "[90]\tvalid_0's ndcg@5: 0.530744\n",
      "[100]\tvalid_0's ndcg@5: 0.531385\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.531385\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.52107\n",
      "[20]\tvalid_0's ndcg@5: 0.524372\n",
      "[30]\tvalid_0's ndcg@5: 0.52745\n",
      "[40]\tvalid_0's ndcg@5: 0.529049\n",
      "[50]\tvalid_0's ndcg@5: 0.530789\n",
      "[60]\tvalid_0's ndcg@5: 0.5309\n",
      "[70]\tvalid_0's ndcg@5: 0.532245\n",
      "[80]\tvalid_0's ndcg@5: 0.532575\n",
      "[90]\tvalid_0's ndcg@5: 0.533146\n",
      "[100]\tvalid_0's ndcg@5: 0.533792\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@5: 0.533956\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6758922051429597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6758922051429597\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6766024076247694e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6766024076247694e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16572338950596058, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16572338950596058\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.519001\n",
      "[20]\tvalid_0's ndcg@5: 0.52027\n",
      "[30]\tvalid_0's ndcg@5: 0.522305\n",
      "[40]\tvalid_0's ndcg@5: 0.524429\n",
      "[50]\tvalid_0's ndcg@5: 0.526337\n",
      "[60]\tvalid_0's ndcg@5: 0.52726\n",
      "[70]\tvalid_0's ndcg@5: 0.528777\n",
      "[80]\tvalid_0's ndcg@5: 0.52871\n",
      "[90]\tvalid_0's ndcg@5: 0.529465\n",
      "[100]\tvalid_0's ndcg@5: 0.530343\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.530343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-11 19:48:22,602] Trial 9 finished with value: 2086.347224274828 and parameters: {'num_leaves': 37, 'learning_rate': 0.008663320674232632, 'feature_fraction': 0.6758922051429597, 'bagging_freq': 10, 'verbose': 0, 'lambda_l1': 1.6766024076247694e-05, 'lambda_l2': 0.16572338950596058}. Best is trial 2 with value: 2245.6078296052387.\n"
     ]
    }
   ],
   "source": [
    "group_sizes = train_data.groupby('group').size()\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna parameters\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_at': 5,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': trial.suggest_int('verbose', 0, 1),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-5, 10.0, log=True),  # L1正則化\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-5, 10.0, log=True)  # L2正則化\n",
    "    }\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    ndcgs = []\n",
    "    \n",
    "    for train_index, valid_index in gkf.split(train_data, groups=train_data['group']):\n",
    "        train_fold_data = train_data.iloc[train_index]\n",
    "        valid_fold_data = train_data.iloc[valid_index]\n",
    "\n",
    "        train_fold_group_sizes = train_fold_data.groupby('group').size().tolist()\n",
    "        valid_fold_group_sizes = valid_fold_data.groupby('group').size().tolist()\n",
    "\n",
    "        ranker = lgb.LGBMRanker(**params)\n",
    "        ranker.fit(train_fold_data[features], train_fold_data[target], \n",
    "           group=train_fold_group_sizes, \n",
    "           eval_set=[(valid_fold_data[features], valid_fold_data[target])], \n",
    "           eval_group=[valid_fold_group_sizes],\n",
    "        \n",
    "        y_pred = ranker.predict(valid_fold_data[features])\n",
    "        ndcg_value = mean_ndcg_score(valid_fold_data[target].values, y_pred, valid_fold_group_sizes)\n",
    "        ndcgs.append(ndcg_value)\n",
    "    \n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54620a51-010d-4544-ad64-6cb8c3033b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1172\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.897777\n",
      "[20]\tvalid_0's ndcg@5: 0.898658\n",
      "[30]\tvalid_0's ndcg@5: 0.899523\n",
      "[40]\tvalid_0's ndcg@5: 0.900109\n",
      "[50]\tvalid_0's ndcg@5: 0.900658\n",
      "[60]\tvalid_0's ndcg@5: 0.901099\n",
      "[70]\tvalid_0's ndcg@5: 0.901545\n",
      "[80]\tvalid_0's ndcg@5: 0.901841\n",
      "[90]\tvalid_0's ndcg@5: 0.902311\n",
      "[100]\tvalid_0's ndcg@5: 0.902587\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902587\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.896834\n",
      "[20]\tvalid_0's ndcg@5: 0.897916\n",
      "[30]\tvalid_0's ndcg@5: 0.899226\n",
      "[40]\tvalid_0's ndcg@5: 0.899902\n",
      "[50]\tvalid_0's ndcg@5: 0.900583\n",
      "[60]\tvalid_0's ndcg@5: 0.901011\n",
      "[70]\tvalid_0's ndcg@5: 0.901436\n",
      "[80]\tvalid_0's ndcg@5: 0.901669\n",
      "[90]\tvalid_0's ndcg@5: 0.902064\n",
      "[100]\tvalid_0's ndcg@5: 0.902235\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902235\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.896818\n",
      "[20]\tvalid_0's ndcg@5: 0.898489\n",
      "[30]\tvalid_0's ndcg@5: 0.899247\n",
      "[40]\tvalid_0's ndcg@5: 0.899774\n",
      "[50]\tvalid_0's ndcg@5: 0.900205\n",
      "[60]\tvalid_0's ndcg@5: 0.90056\n",
      "[70]\tvalid_0's ndcg@5: 0.900916\n",
      "[80]\tvalid_0's ndcg@5: 0.901191\n",
      "[90]\tvalid_0's ndcg@5: 0.901766\n",
      "[100]\tvalid_0's ndcg@5: 0.902054\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902054\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168487, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.897526\n",
      "[20]\tvalid_0's ndcg@5: 0.898858\n",
      "[30]\tvalid_0's ndcg@5: 0.899975\n",
      "[40]\tvalid_0's ndcg@5: 0.900197\n",
      "[50]\tvalid_0's ndcg@5: 0.90089\n",
      "[60]\tvalid_0's ndcg@5: 0.901267\n",
      "[70]\tvalid_0's ndcg@5: 0.901681\n",
      "[80]\tvalid_0's ndcg@5: 0.902005\n",
      "[90]\tvalid_0's ndcg@5: 0.902305\n",
      "[100]\tvalid_0's ndcg@5: 0.902694\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902694\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1170\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168488, number of used features: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7241844883625201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7241844883625201\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.42367192289815003, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.42367192289815003\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.03048397774460981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03048397774460981\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\tvalid_0's ndcg@5: 0.897343\n",
      "[20]\tvalid_0's ndcg@5: 0.898325\n",
      "[30]\tvalid_0's ndcg@5: 0.899273\n",
      "[40]\tvalid_0's ndcg@5: 0.899979\n",
      "[50]\tvalid_0's ndcg@5: 0.900436\n",
      "[60]\tvalid_0's ndcg@5: 0.901116\n",
      "[70]\tvalid_0's ndcg@5: 0.901325\n",
      "[80]\tvalid_0's ndcg@5: 0.901642\n",
      "[90]\tvalid_0's ndcg@5: 0.901999\n",
      "[100]\tvalid_0's ndcg@5: 0.902158\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@5: 0.902158\n"
     ]
    }
   ],
   "source": [
    "# 最適なパラメータ\n",
    "best_params = study.best_params\n",
    "\n",
    "# KFoldでのモデル訓練\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = []\n",
    "\n",
    "for train_index, valid_index in kf.split(train_data):\n",
    "    train_fold_data = train_data.iloc[train_index]\n",
    "    valid_fold_data = train_data.iloc[valid_index]\n",
    "    \n",
    "    train_fold_group_sizes = train_fold_data.groupby('group').size().tolist()\n",
    "    valid_fold_group_sizes = valid_fold_data.groupby('group').size().tolist()\n",
    "    \n",
    "    ranker = lgb.LGBMRanker(**best_params)\n",
    "    ranker.fit(\n",
    "        train_fold_data[features], train_fold_data[target], \n",
    "        group=train_fold_group_sizes, \n",
    "        eval_set=[(valid_fold_data[features], valid_fold_data[target])], \n",
    "        eval_group=[valid_fold_group_sizes], \n",
    "        eval_at=5, early_stopping_rounds=20, verbose=10\n",
    "    )\n",
    "    models.append(ranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50618bb5-4841-4cb9-b4cb-cc029629358c",
   "metadata": {},
   "source": [
    "### テストデータで予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59de8ac9-a8f5-45fc-a650-2f6d47ffc9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>predicted_rank</th>\n",
       "      <th>kakutei_chakujun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1635601</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635602</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635603</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635604</th>\n",
       "      <td>2021-222-83-2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635605</th>\n",
       "      <td>2021-101-45-1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785604</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785605</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785606</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785607</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785608</th>\n",
       "      <td>2021-1231-54-11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147974 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   group  predicted_rank  kakutei_chakujun\n",
       "1635601    2021-222-83-2             7.0                 6\n",
       "1635602    2021-222-83-2             2.0                 7\n",
       "1635603    2021-222-83-2            10.0                 1\n",
       "1635604    2021-222-83-2             1.0                 2\n",
       "1635605    2021-101-45-1             4.0                 1\n",
       "...                  ...             ...               ...\n",
       "1785604  2021-1231-54-11            11.0                 3\n",
       "1785605  2021-1231-54-11             5.0                 1\n",
       "1785606  2021-1231-54-11            10.0                 2\n",
       "1785607  2021-1231-54-11             4.0                11\n",
       "1785608  2021-1231-54-11             7.0                 5\n",
       "\n",
       "[147974 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2020年から2022年のテストデータを取得\n",
    "test_data_2020 = merged_df[merged_df['kaisai_nen'] == 2020].copy()\n",
    "test_data_2021 = merged_df[merged_df['kaisai_nen'] == 2021].copy()\n",
    "test_data_2022 = merged_df[merged_df['kaisai_nen'] == 2022].copy()\n",
    "\n",
    "# 2021年と2022年のデータに対して予測を行い、平均を取る\n",
    "test_data_2020.loc[:, 'y_pred'] = sum([model.predict(test_data_2020[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "test_data_2021.loc[:, 'y_pred'] = sum([model.predict(test_data_2021[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "test_data_2022.loc[:, 'y_pred'] = sum([model.predict(test_data_2022[features], num_iteration=model.best_iteration_) for model in models]) / len(models)\n",
    "\n",
    "# 予測されたランクをグループごとに計算\n",
    "test_data_2020.loc[:, 'predicted_rank'] = test_data_2020.groupby('group')['y_pred'].rank(method='min')\n",
    "test_data_2021.loc[:, 'predicted_rank'] = test_data_2021.groupby('group')['y_pred'].rank(method='min')\n",
    "test_data_2022.loc[:, 'predicted_rank'] = test_data_2022.groupby('group')['y_pred'].rank(method='min')\n",
    "\n",
    "# 結果を表示\n",
    "test_data_2021[['group', 'predicted_rank', 'kakutei_chakujun']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb294d-cc15-4ece-8051-7e9a1f1ca38c",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b3c7ea0-effe-4b02-a1db-9f38b16e8542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 RMSE: 365.760%\n",
      "2021 RMSE: 371.648%\n",
      "2022 RMSE: 370.456%\n",
      "Mean RMSE: 369.288%\n"
     ]
    }
   ],
   "source": [
    "# RMSEを計算\n",
    "rmse_2020 = np.sqrt(mean_squared_error(test_data_2020['predicted_rank'], test_data_2020['kakutei_chakujun']))\n",
    "print(f\"2021 RMSE: {rmse_2020:.3%}\")\n",
    "rmse_2021 = np.sqrt(mean_squared_error(test_data_2021['predicted_rank'], test_data_2021['kakutei_chakujun']))\n",
    "print(f\"2021 RMSE: {rmse_2021:.3%}\")\n",
    "rmse_2022 = np.sqrt(mean_squared_error(test_data_2022['predicted_rank'], test_data_2022['kakutei_chakujun']))\n",
    "print(f\"2022 RMSE: {rmse_2022:.3%}\")\n",
    "rmse_mean = np.mean([rmse_2020, rmse_2021, rmse_2022])\n",
    "print(f\"Mean RMSE: {rmse_mean:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d558ef-cfb1-42ed-aebf-8a7d6e851def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_5(predictions, actual):\n",
    "    top_5_predictions = predictions.argsort()[-5:][::-1]  # 予測の上位5頭のインデックス\n",
    "    top_3_actual = actual.argsort()[-3:][::-1]  # 実際の上位3頭のインデックス\n",
    "    common_elements = np.intersect1d(top_5_predictions, top_3_actual)  # 共通の要素を抽出\n",
    "    precision = len(common_elements) / 5\n",
    "    return precision\n",
    "\n",
    "def recall_at_5(predictions, actual):\n",
    "    top_5_predictions = predictions.argsort()[-5:][::-1]  # 予測の上位5頭のインデックス\n",
    "    top_3_actual = actual.argsort()[-3:][::-1]  # 実際の上位3頭のインデックス\n",
    "    common_elements = np.intersect1d(top_5_predictions, top_3_actual)  # 共通の要素を抽出\n",
    "    recall = len(common_elements) / 3\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19fcf8c0-d484-4492-9ca0-2e4b65bb7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_top3_in_top5_predictions(group):\n",
    "    predicted_top5 = group.nsmallest(5, 'y_pred').index.tolist()\n",
    "    actual_top3 = group.nsmallest(3, 'kakutei_chakujun').index.tolist()\n",
    "    return all([horse in predicted_top5 for horse in actual_top3])\n",
    "\n",
    "def calculate_group_profit(group):\n",
    "    if check_top3_in_top5_predictions(group):\n",
    "        payout_value = group['haraimodoshi_sanrenpuku_1b'].iloc[0]\n",
    "        return payout_value - 1000\n",
    "    else:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b30d6ff-8e24-4188-9d8f-f569b483b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test_data in [test_data_2020, test_data_2021, test_data_2022]:\n",
    "#     merged_data = test_data\n",
    "#     profits = merged_data.groupby('group').apply(calculate_group_profit).reset_index()\n",
    "#     profits.columns = ['group', 'profit']\n",
    "#     test_data = pd.merge(test_data, profits, on='group', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c260fed-21fe-4db0-9bbb-e378b2443d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 Precision@5: 38.689%\n",
      "2020 Recall@5: 64.481%\n",
      "2021 Precision@5: 38.213%\n",
      "2021 Recall@5: 63.689%\n",
      "2022 Precision@5: 38.198%\n",
      "2022 Recall@5: 63.664%\n",
      "Mean Precision@5: 38.367%\n",
      "Mean Recall@5: 63.945%\n",
      "2020 Average Net Profit: 583.169 yen\n",
      "2021 Average Net Profit: 527.891 yen\n",
      "2022 Average Net Profit: 581.717 yen\n",
      "Mean Average Net Profit: 564.259 yen\n"
     ]
    }
   ],
   "source": [
    "group_ids_2020 = test_data_2020['group'].unique()\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for group_id in group_ids_2020:\n",
    "    test_data = test_data_2020[test_data_2020['group'] == group_id]\n",
    "    p = precision_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    r = recall_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "\n",
    "precision_5_2020 = np.mean(precisions)\n",
    "recall_5_2020 = np.mean(recalls)\n",
    "\n",
    "print(f\"2020 Precision@5: {precision_5_2020:.3%}\")\n",
    "print(f\"2020 Recall@5: {recall_5_2020:.3%}\")\n",
    "\n",
    "group_ids_2021 = test_data_2021['group'].unique()\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for group_id in group_ids_2021:\n",
    "    test_data = test_data_2021[test_data_2021['group'] == group_id]\n",
    "    p = precision_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    r = recall_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "\n",
    "precision_5_2021 = np.mean(precisions)\n",
    "recall_5_2021 = np.mean(recalls)\n",
    "\n",
    "print(f\"2021 Precision@5: {precision_5_2021:.3%}\")\n",
    "print(f\"2021 Recall@5: {recall_5_2021:.3%}\")\n",
    "\n",
    "\n",
    "roup_ids_2022 = test_data_2022['group'].unique()\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for group_id in roup_ids_2022:\n",
    "    test_data = test_data_2022[test_data_2022['group'] == group_id]\n",
    "    p = precision_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    r = recall_at_5(test_data['predicted_rank'].values, test_data['kakutei_chakujun'].values)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "\n",
    "precision_5_2022 = np.mean(precisions)\n",
    "recall_5_2022 = np.mean(recalls)\n",
    "\n",
    "print(f\"2022 Precision@5: {precision_5_2022:.3%}\")\n",
    "print(f\"2022 Recall@5: {recall_5_2022:.3%}\")\n",
    "\n",
    "\n",
    "precision_5_mean = np.mean([precision_5_2020, precision_5_2021, precision_5_2022])\n",
    "recall_5_mean = np.mean([recall_5_2020, recall_5_2021, recall_5_2022])\n",
    "\n",
    "print(f\"Mean Precision@5: {precision_5_mean:.3%}\")\n",
    "print(f\"Mean Recall@5: {recall_5_mean:.3%}\")\n",
    "\n",
    "# 各レースで上記の関数を適用\n",
    "results_2020 = test_data_2020.groupby('group').apply(check_top3_in_top5_predictions)\n",
    "results_2021 = test_data_2021.groupby('group').apply(check_top3_in_top5_predictions)\n",
    "results_2022 = test_data_2022.groupby('group').apply(check_top3_in_top5_predictions)\n",
    "\n",
    "# 予測モデルが成功したレースのインデックスを取得する\n",
    "successful_groups_2020 = results_2020[results_2020].index\n",
    "successful_groups_2021 = results_2021[results_2021].index\n",
    "successful_groups_2022 = results_2022[results_2022].index\n",
    "\n",
    "# そのインデックスを使用して、harai_dfから対応する支払倍率を取得する\n",
    "successful_payout_2020 = merged_df[merged_df['group'].isin(successful_groups_2020)]\n",
    "successful_payout_2021 = merged_df[merged_df['group'].isin(successful_groups_2021)]\n",
    "successful_payout_2022 = merged_df[merged_df['group'].isin(successful_groups_2022)]\n",
    "\n",
    "payouts_2020 = successful_payout_2020['haraimodoshi_sanrenpuku_1b']\n",
    "payouts_2021 = successful_payout_2021['haraimodoshi_sanrenpuku_1b']\n",
    "payouts_2022 = successful_payout_2022['haraimodoshi_sanrenpuku_1b']\n",
    "\n",
    "# すべての成功したレースに対して、100円をかけた場合の支払いを計算する\n",
    "total_payout_2020 = (payouts_2020).sum()\n",
    "total_payout_2021 = (payouts_2021).sum()\n",
    "total_payout_2022 = (payouts_2022).sum()\n",
    "\n",
    "# 合計の支払いから、すべてのレースにかけた合計金額を引くことで、純利益を計算する\n",
    "total_investment_2020 = test_data_2020['group'].nunique() * 1000\n",
    "total_investment_2021 = test_data_2021['group'].nunique() * 1000\n",
    "total_investment_2022 = test_data_2021['group'].nunique() * 1000\n",
    "\n",
    "net_profit_2020 = total_payout_2020 - total_investment_2020\n",
    "net_profit_2021 = total_payout_2021 - total_investment_2021\n",
    "net_profit_2022 = total_payout_2022 - total_investment_2022\n",
    "\n",
    "average_net_profit_2020 = net_profit_2020/len(test_data_2020)\n",
    "average_net_profit_2021 = net_profit_2021/len(test_data_2021)\n",
    "average_net_profit_2022 = net_profit_2020/len(test_data_2022)\n",
    "\n",
    "print(f\"2020 Average Net Profit: {average_net_profit_2020:.3f} yen\")\n",
    "print(f\"2021 Average Net Profit: {average_net_profit_2021:.3f} yen\")\n",
    "print(f\"2022 Average Net Profit: {average_net_profit_2022:.3f} yen\")\n",
    "\n",
    "mean_average_net_profit = np.mean([average_net_profit_2020, average_net_profit_2021, average_net_profit_2022])\n",
    "print(f\"Mean Average Net Profit: {mean_average_net_profit:.3f} yen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cba3cc9d-fed6-4870-8be0-6eafd8219f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# models[0]'s feature importances\n",
    "feature_importance = models[0].feature_importances_\n",
    "\n",
    "# Convert feature names and importances to DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b012e5-3fdb-4152-966e-1b76dd8e46bc",
   "metadata": {},
   "source": [
    "# モデル保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f56bbe4-89fe-4ca0-8ee4-3c4a6daed162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1172\n",
      "[LightGBM] [Info] Number of data points in the train set: 2017402, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 5.554518\n",
      "Model saved to ./bestmodels/lightgbmranker.pkl\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "full_train_dataset = lgb.Dataset(merged_df[features], label=merged_df[target])\n",
    "full_model = lgb.train(best_params, full_train_dataset, num_boost_round=500, verbose_eval=100)  # ここではvalid_setsやearly_stopping_roundsは使用しない\n",
    "\n",
    "# モデルを保存\n",
    "# model_save_path = '../app/models/model.pkl'\n",
    "model_save_path = './bestmodels/lightgbmranker.pkl'\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(full_model, f)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1347bfe-200d-49c4-8f13-6bf97f2b6f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "2020-1001-30-1     False\n",
       "2020-1001-30-10    False\n",
       "2020-1001-30-11    False\n",
       "2020-1001-30-12    False\n",
       "2020-1001-30-2      True\n",
       "                   ...  \n",
       "2020-930-50-5       True\n",
       "2020-930-50-6      False\n",
       "2020-930-50-7      False\n",
       "2020-930-50-8       True\n",
       "2020-930-50-9       True\n",
       "Length: 15067, dtype: bool"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fb97dbd-e5c0-4f2b-8b9f-af80989301c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2020-1001-30-2', '2020-1001-30-4', '2020-1001-30-7', '2020-1001-30-9',\n",
       "       '2020-1001-43-5', '2020-1001-48-10', '2020-1001-48-3', '2020-1001-48-4',\n",
       "       '2020-1001-48-5', '2020-1001-48-7',\n",
       "       ...\n",
       "       '2020-930-43-4', '2020-930-43-7', '2020-930-43-8', '2020-930-48-1',\n",
       "       '2020-930-48-11', '2020-930-48-12', '2020-930-50-4', '2020-930-50-5',\n",
       "       '2020-930-50-8', '2020-930-50-9'],\n",
       "      dtype='object', name='group', length=4076)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successful_groups_2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
